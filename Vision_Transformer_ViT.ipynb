{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beginning-apartment",
   "metadata": {
    "id": "beginning-apartment",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "Prepared by Comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-louis",
   "metadata": {
    "id": "focal-louis"
   },
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complete-missouri",
   "metadata": {
    "id": "complete-missouri",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "KKuX81tUbwLM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKuX81tUbwLM",
    "outputId": "5cf86782-a467-47c4-ea83-8c66394b5d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Gb4PC5uYdWN8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gb4PC5uYdWN8",
    "outputId": "a3ac0b47-6849-4c22-df0d-cfc77866e1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/comp411_assignment3/assignment3\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/comp411_assignment3/assignment3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forbidden-yellow",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "forbidden-yellow",
    "outputId": "f0029b34-c5a8-4467-894c-16ee72826a6a",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-portal",
   "metadata": {
    "id": "hollywood-portal",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "needed-charleston",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "needed-charleston",
    "outputId": "a28690a9-edfa-41a7-ba8b-03e311a1a92f",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-remove",
   "metadata": {
    "id": "greenhouse-remove"
   },
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "liquid-default",
   "metadata": {
    "id": "liquid-default"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        \n",
    "        self.W_O = nn.Linear(self.proj_dims,self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        #print(self.input_dims)\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality --> self.head_dims\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "        attn_out = None\n",
    "        \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is implemented as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "       \n",
    "        #print(\"attn_logits ----------------------\")\n",
    "        k_T = k.transpose(2, 3)\n",
    "        attn_logits = torch.matmul(q, k_T)\n",
    "        #scale\n",
    "        attn_logits = attn_logits / (self.head_dims**(1/2))\n",
    "        #print(attn_logits.size())\n",
    "\n",
    "        ## Compute attention Weights. Recall that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "        \n",
    "        softmax = nn.Softmax(dim=-1)\n",
    "        attn_weights = softmax(attn_logits)\n",
    "        #print(\"attn_weights ----------------------\")\n",
    "        #print(attn_weights.size())\n",
    "        #print(v.size())\n",
    "\n",
    "        ## Compute attention output values. Bear in mind that this operation is applied as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H] ||| D//H -> self.head_dims\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        ##       (or any other equivalent torch operations)\n",
    "        ################################################################################################\n",
    "        #print(\"attn_applied ----------------------\")\n",
    "\n",
    "        attn_applied = torch.matmul(attn_weights, v) \n",
    "        attn_applied = attn_applied.permute(0, 2, 1, 3)\n",
    "        attn_applied = torch.reshape(attn_applied, (b, n, self.head_dims * self.num_heads))\n",
    "        #print(attn_applied.size())\n",
    "\n",
    "        ################################################################################################\n",
    "        ## Compute output feature map. This operation is just passing the concatenated attention \n",
    "        ## output that we have just obtained through a final projection layer W_O.\n",
    "        ## Both the input and the output should be of size [B, N, D]\n",
    "        \n",
    "        #print(\"attn_out ----------------------\")\n",
    "        attn_out = self.W_O(attn_applied)      \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-desert",
   "metadata": {
    "id": "civic-desert"
   },
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dressed-valve",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dressed-valve",
    "outputId": "d353ae67-306a-48f3-99e1-74ed92e74a2a",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-bermuda",
   "metadata": {
    "id": "embedded-bermuda"
   },
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "finished-level",
   "metadata": {
    "id": "finished-level"
   },
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.gelu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fp6CHYzEtQKo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fp6CHYzEtQKo",
    "outputId": "da342726-e3ea-4247-b386-91e69aa67655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "## testing mlp output size\n",
    "def test_mlp():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  \n",
    "    layer = MLP(32,32,32)\n",
    "    out = layer(x)\n",
    "    print(out.size())  \n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "immediate-february",
   "metadata": {
    "id": "immediate-february"
   },
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##        --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "###############################################################\n",
    "# TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "###############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.norm1 = nn.LayerNorm(hidden_dims)\n",
    "        self.self_attention = SelfAttention(hidden_dims, head_dims = hidden_dims // num_heads, num_heads=num_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dims)\n",
    "        self.mlp = MLP(hidden_dims, hidden_dims, hidden_dims)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "##############################################################\n",
    "# TODO: Complete the forward of TransformerBlock module      #\n",
    "##############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        skip = x\n",
    "        out = self.norm1(x)\n",
    "        out = self.self_attention(out)\n",
    "        out += skip\n",
    "\n",
    "        skip = out\n",
    "        out = self.norm2(out)\n",
    "        out = self.mlp(out)\n",
    "        out += skip\n",
    "        # [b,n,d]\n",
    "        return out\n",
    " # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-tomato",
   "metadata": {
    "id": "literary-tomato"
   },
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "closed-cabin",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "closed-cabin",
    "outputId": "0231d4ee-6c22-4e77-e06e-160c545a3e43",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,128]\n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-mainland",
   "metadata": {
    "id": "middle-mainland"
   },
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "protected-philippines",
   "metadata": {
    "id": "protected-philippines"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-banner",
   "metadata": {
    "id": "comparable-banner"
   },
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unable-daisy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unable-daisy",
    "outputId": "7593e5a7-74a4-4938-c623-d8bfa54d6871",
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size())  # you should see [64,10]\n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-firewall",
   "metadata": {
    "id": "sixth-firewall"
   },
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-parker",
   "metadata": {
    "id": "caring-parker"
   },
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "smart-button",
   "metadata": {
    "id": "smart-button",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, np.float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-structure",
   "metadata": {
    "id": "popular-structure"
   },
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "coordinated-strategy",
   "metadata": {
    "id": "coordinated-strategy",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-crown",
   "metadata": {
    "id": "resident-crown"
   },
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "broadband-national",
   "metadata": {
    "id": "broadband-national"
   },
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-religious",
   "metadata": {
    "id": "protecting-religious"
   },
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "military-buying",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "military-buying",
    "outputId": "e29ac6cc-cbed-4db5-95ea-21ea90769a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "absolute-disorder",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "absolute-disorder",
    "outputId": "9eed746d-6aa9-4809-d303-ed50815a0875",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.512 | Acc: 12.000% (3/25)\n",
      "Loss: 3.912 | Acc: 12.000% (6/50)\n",
      "Loss: 3.749 | Acc: 13.333% (10/75)\n",
      "Loss: 3.537 | Acc: 14.000% (14/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 14.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.453 | Acc: 12.000% (3/25)\n",
      "Loss: 3.265 | Acc: 16.000% (8/50)\n",
      "Loss: 2.872 | Acc: 20.000% (15/75)\n",
      "Loss: 3.129 | Acc: 17.000% (17/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 17.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 2.619 | Acc: 20.000% (5/25)\n",
      "Loss: 2.731 | Acc: 16.000% (8/50)\n",
      "Loss: 2.929 | Acc: 14.667% (11/75)\n",
      "Loss: 2.955 | Acc: 14.000% (14/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 14.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.562 | Acc: 4.000% (1/25)\n",
      "Loss: 2.549 | Acc: 10.000% (5/50)\n",
      "Loss: 2.534 | Acc: 16.000% (12/75)\n",
      "Loss: 2.622 | Acc: 14.000% (14/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 14.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.697 | Acc: 8.000% (2/25)\n",
      "Loss: 2.385 | Acc: 22.000% (11/50)\n",
      "Loss: 2.297 | Acc: 25.333% (19/75)\n",
      "Loss: 2.271 | Acc: 23.000% (23/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 23.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.414 | Acc: 12.000% (3/25)\n",
      "Loss: 2.290 | Acc: 22.000% (11/50)\n",
      "Loss: 2.362 | Acc: 18.667% (14/75)\n",
      "Loss: 2.421 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 1.579 | Acc: 56.000% (14/25)\n",
      "Loss: 1.874 | Acc: 38.000% (19/50)\n",
      "Loss: 1.937 | Acc: 34.667% (26/75)\n",
      "Loss: 1.928 | Acc: 37.000% (37/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 37.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.696 | Acc: 16.000% (4/25)\n",
      "Loss: 2.464 | Acc: 22.000% (11/50)\n",
      "Loss: 2.431 | Acc: 20.000% (15/75)\n",
      "Loss: 2.558 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.542 | Acc: 48.000% (12/25)\n",
      "Loss: 1.713 | Acc: 46.000% (23/50)\n",
      "Loss: 1.619 | Acc: 48.000% (36/75)\n",
      "Loss: 1.615 | Acc: 46.000% (46/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 46.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.154 | Acc: 28.000% (7/25)\n",
      "Loss: 2.158 | Acc: 24.000% (12/50)\n",
      "Loss: 2.175 | Acc: 22.667% (17/75)\n",
      "Loss: 2.202 | Acc: 21.000% (21/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 21.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.504 | Acc: 56.000% (14/25)\n",
      "Loss: 1.523 | Acc: 46.000% (23/50)\n",
      "Loss: 1.437 | Acc: 49.333% (37/75)\n",
      "Loss: 1.411 | Acc: 51.000% (51/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 51.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.273 | Acc: 20.000% (5/25)\n",
      "Loss: 2.190 | Acc: 18.000% (9/50)\n",
      "Loss: 2.223 | Acc: 20.000% (15/75)\n",
      "Loss: 2.281 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.144 | Acc: 68.000% (17/25)\n",
      "Loss: 0.992 | Acc: 74.000% (37/50)\n",
      "Loss: 1.138 | Acc: 68.000% (51/75)\n",
      "Loss: 1.173 | Acc: 67.000% (67/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 67.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.743 | Acc: 12.000% (3/25)\n",
      "Loss: 2.482 | Acc: 18.000% (9/50)\n",
      "Loss: 2.448 | Acc: 21.333% (16/75)\n",
      "Loss: 2.548 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 0.821 | Acc: 72.000% (18/25)\n",
      "Loss: 0.821 | Acc: 78.000% (39/50)\n",
      "Loss: 0.798 | Acc: 78.667% (59/75)\n",
      "Loss: 0.851 | Acc: 73.000% (73/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 73.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.868 | Acc: 16.000% (4/25)\n",
      "Loss: 2.698 | Acc: 18.000% (9/50)\n",
      "Loss: 2.646 | Acc: 17.333% (13/75)\n",
      "Loss: 2.737 | Acc: 17.000% (17/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 17.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 0.661 | Acc: 80.000% (20/25)\n",
      "Loss: 0.628 | Acc: 82.000% (41/50)\n",
      "Loss: 0.660 | Acc: 81.333% (61/75)\n",
      "Loss: 0.656 | Acc: 83.000% (83/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 83.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.251 | Acc: 16.000% (4/25)\n",
      "Loss: 2.878 | Acc: 18.000% (9/50)\n",
      "Loss: 2.865 | Acc: 17.333% (13/75)\n",
      "Loss: 2.882 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 0.486 | Acc: 96.000% (24/25)\n",
      "Loss: 0.419 | Acc: 92.000% (46/50)\n",
      "Loss: 0.457 | Acc: 89.333% (67/75)\n",
      "Loss: 0.460 | Acc: 89.000% (89/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 89.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.227 | Acc: 16.000% (4/25)\n",
      "Loss: 2.854 | Acc: 24.000% (12/50)\n",
      "Loss: 2.786 | Acc: 25.333% (19/75)\n",
      "Loss: 2.874 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 0.277 | Acc: 100.000% (25/25)\n",
      "Loss: 0.369 | Acc: 96.000% (48/50)\n",
      "Loss: 0.354 | Acc: 93.333% (70/75)\n",
      "Loss: 0.338 | Acc: 95.000% (95/100)\n",
      "Epoch 10 of training is completed, Training accuracy for this epoch is 95.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.310 | Acc: 12.000% (3/25)\n",
      "Loss: 2.992 | Acc: 20.000% (10/50)\n",
      "Loss: 2.956 | Acc: 21.333% (16/75)\n",
      "Loss: 3.040 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 0.214 | Acc: 100.000% (25/25)\n",
      "Loss: 0.189 | Acc: 98.000% (49/50)\n",
      "Loss: 0.167 | Acc: 98.667% (74/75)\n",
      "Loss: 0.164 | Acc: 99.000% (99/100)\n",
      "Epoch 11 of training is completed, Training accuracy for this epoch is 99.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.721 | Acc: 16.000% (4/25)\n",
      "Loss: 3.349 | Acc: 18.000% (9/50)\n",
      "Loss: 3.334 | Acc: 18.667% (14/75)\n",
      "Loss: 3.330 | Acc: 18.000% (18/100)\n",
      "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 18.0\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 0.103 | Acc: 100.000% (25/25)\n",
      "Loss: 0.101 | Acc: 100.000% (50/50)\n",
      "Loss: 0.101 | Acc: 100.000% (75/75)\n",
      "Loss: 0.103 | Acc: 100.000% (100/100)\n",
      "Epoch 12 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.780 | Acc: 16.000% (4/25)\n",
      "Loss: 3.275 | Acc: 18.000% (9/50)\n",
      "Loss: 3.351 | Acc: 18.667% (14/75)\n",
      "Loss: 3.371 | Acc: 17.000% (17/100)\n",
      "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 17.0\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 0.079 | Acc: 100.000% (25/25)\n",
      "Loss: 0.072 | Acc: 100.000% (50/50)\n",
      "Loss: 0.067 | Acc: 100.000% (75/75)\n",
      "Loss: 0.065 | Acc: 100.000% (100/100)\n",
      "Epoch 13 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.744 | Acc: 8.000% (2/25)\n",
      "Loss: 3.200 | Acc: 18.000% (9/50)\n",
      "Loss: 3.278 | Acc: 16.000% (12/75)\n",
      "Loss: 3.369 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 0.048 | Acc: 100.000% (25/25)\n",
      "Loss: 0.043 | Acc: 100.000% (50/50)\n",
      "Loss: 0.042 | Acc: 100.000% (75/75)\n",
      "Loss: 0.039 | Acc: 100.000% (100/100)\n",
      "Epoch 14 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.808 | Acc: 12.000% (3/25)\n",
      "Loss: 3.366 | Acc: 22.000% (11/50)\n",
      "Loss: 3.402 | Acc: 21.333% (16/75)\n",
      "Loss: 3.521 | Acc: 21.000% (21/100)\n",
      "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 21.0\n",
      "\n",
      "Final train set accuracy is 100.0\n",
      "Final val set accuracy is 21.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims=hidden_dims, input_dims=input_dims, output_dims=output_dims,\n",
    "              num_trans_layers=num_trans_layers, num_heads=num_heads, image_k=image_k,\n",
    "              patch_k=patch_k)\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(15):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-auditor",
   "metadata": {
    "id": "representative-auditor"
   },
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "liberal-dispatch",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liberal-dispatch",
    "outputId": "1b2b8d94-147e-4050-9156-9484c3c4b027",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.801 | Acc: 12.500% (8/64)\n",
      "Loss: 3.762 | Acc: 14.844% (19/128)\n",
      "Loss: 3.856 | Acc: 13.021% (25/192)\n",
      "Loss: 3.804 | Acc: 14.062% (36/256)\n",
      "Loss: 3.758 | Acc: 14.688% (47/320)\n",
      "Loss: 3.564 | Acc: 14.323% (55/384)\n",
      "Loss: 3.435 | Acc: 14.509% (65/448)\n",
      "Loss: 3.325 | Acc: 15.625% (80/512)\n",
      "Loss: 3.217 | Acc: 16.319% (94/576)\n",
      "Loss: 3.129 | Acc: 16.875% (108/640)\n",
      "Loss: 3.055 | Acc: 16.903% (119/704)\n",
      "Loss: 2.984 | Acc: 16.927% (130/768)\n",
      "Loss: 2.927 | Acc: 16.827% (140/832)\n",
      "Loss: 2.867 | Acc: 17.746% (159/896)\n",
      "Loss: 2.829 | Acc: 18.021% (173/960)\n",
      "Loss: 2.785 | Acc: 17.871% (183/1024)\n",
      "Loss: 2.756 | Acc: 17.647% (192/1088)\n",
      "Loss: 2.728 | Acc: 17.795% (205/1152)\n",
      "Loss: 2.701 | Acc: 17.845% (217/1216)\n",
      "Loss: 2.676 | Acc: 17.891% (229/1280)\n",
      "Loss: 2.648 | Acc: 18.006% (242/1344)\n",
      "Loss: 2.623 | Acc: 18.466% (260/1408)\n",
      "Loss: 2.600 | Acc: 18.682% (275/1472)\n",
      "Loss: 2.574 | Acc: 19.206% (295/1536)\n",
      "Loss: 2.555 | Acc: 19.438% (311/1600)\n",
      "Loss: 2.539 | Acc: 19.651% (327/1664)\n",
      "Loss: 2.526 | Acc: 19.850% (343/1728)\n",
      "Loss: 2.510 | Acc: 19.866% (356/1792)\n",
      "Loss: 2.500 | Acc: 20.043% (372/1856)\n",
      "Loss: 2.482 | Acc: 20.469% (393/1920)\n",
      "Loss: 2.472 | Acc: 20.716% (411/1984)\n",
      "Loss: 2.465 | Acc: 20.557% (421/2048)\n",
      "Loss: 2.455 | Acc: 20.644% (436/2112)\n",
      "Loss: 2.442 | Acc: 20.726% (451/2176)\n",
      "Loss: 2.429 | Acc: 20.804% (466/2240)\n",
      "Loss: 2.417 | Acc: 20.877% (481/2304)\n",
      "Loss: 2.403 | Acc: 20.946% (496/2368)\n",
      "Loss: 2.391 | Acc: 21.217% (516/2432)\n",
      "Loss: 2.382 | Acc: 21.514% (537/2496)\n",
      "Loss: 2.367 | Acc: 21.641% (554/2560)\n",
      "Loss: 2.359 | Acc: 21.951% (576/2624)\n",
      "Loss: 2.350 | Acc: 22.061% (593/2688)\n",
      "Loss: 2.338 | Acc: 22.238% (612/2752)\n",
      "Loss: 2.336 | Acc: 22.230% (626/2816)\n",
      "Loss: 2.333 | Acc: 22.257% (641/2880)\n",
      "Loss: 2.324 | Acc: 22.317% (657/2944)\n",
      "Loss: 2.313 | Acc: 22.374% (673/3008)\n",
      "Loss: 2.306 | Acc: 22.656% (696/3072)\n",
      "Loss: 2.306 | Acc: 22.608% (709/3136)\n",
      "Loss: 2.302 | Acc: 22.781% (729/3200)\n",
      "Loss: 2.296 | Acc: 22.947% (749/3264)\n",
      "Loss: 2.292 | Acc: 22.776% (758/3328)\n",
      "Loss: 2.289 | Acc: 22.789% (773/3392)\n",
      "Loss: 2.283 | Acc: 22.946% (793/3456)\n",
      "Loss: 2.278 | Acc: 22.955% (808/3520)\n",
      "Loss: 2.270 | Acc: 23.019% (825/3584)\n",
      "Loss: 2.266 | Acc: 22.999% (839/3648)\n",
      "Loss: 2.264 | Acc: 22.899% (850/3712)\n",
      "Loss: 2.258 | Acc: 22.987% (868/3776)\n",
      "Loss: 2.252 | Acc: 23.099% (887/3840)\n",
      "Loss: 2.250 | Acc: 23.079% (901/3904)\n",
      "Loss: 2.243 | Acc: 23.160% (919/3968)\n",
      "Loss: 2.239 | Acc: 23.264% (938/4032)\n",
      "Loss: 2.237 | Acc: 23.315% (955/4096)\n",
      "Loss: 2.232 | Acc: 23.341% (971/4160)\n",
      "Loss: 2.228 | Acc: 23.248% (982/4224)\n",
      "Loss: 2.223 | Acc: 23.368% (1002/4288)\n",
      "Loss: 2.221 | Acc: 23.529% (1024/4352)\n",
      "Loss: 2.214 | Acc: 23.800% (1051/4416)\n",
      "Loss: 2.208 | Acc: 23.884% (1070/4480)\n",
      "Loss: 2.207 | Acc: 23.900% (1086/4544)\n",
      "Loss: 2.203 | Acc: 23.915% (1102/4608)\n",
      "Loss: 2.200 | Acc: 23.908% (1117/4672)\n",
      "Loss: 2.198 | Acc: 23.860% (1130/4736)\n",
      "Loss: 2.195 | Acc: 23.958% (1150/4800)\n",
      "Loss: 2.191 | Acc: 24.116% (1173/4864)\n",
      "Loss: 2.189 | Acc: 24.168% (1191/4928)\n",
      "Loss: 2.181 | Acc: 24.459% (1221/4992)\n",
      "Loss: 2.176 | Acc: 24.525% (1240/5056)\n",
      "Loss: 2.174 | Acc: 24.512% (1255/5120)\n",
      "Loss: 2.172 | Acc: 24.556% (1273/5184)\n",
      "Loss: 2.166 | Acc: 24.809% (1302/5248)\n",
      "Loss: 2.161 | Acc: 25.000% (1328/5312)\n",
      "Loss: 2.158 | Acc: 25.019% (1345/5376)\n",
      "Loss: 2.154 | Acc: 25.110% (1366/5440)\n",
      "Loss: 2.152 | Acc: 25.145% (1384/5504)\n",
      "Loss: 2.151 | Acc: 25.144% (1400/5568)\n",
      "Loss: 2.148 | Acc: 25.213% (1420/5632)\n",
      "Loss: 2.145 | Acc: 25.351% (1444/5696)\n",
      "Loss: 2.143 | Acc: 25.278% (1456/5760)\n",
      "Loss: 2.142 | Acc: 25.223% (1469/5824)\n",
      "Loss: 2.138 | Acc: 25.391% (1495/5888)\n",
      "Loss: 2.135 | Acc: 25.470% (1516/5952)\n",
      "Loss: 2.133 | Acc: 25.532% (1536/6016)\n",
      "Loss: 2.133 | Acc: 25.493% (1550/6080)\n",
      "Loss: 2.130 | Acc: 25.553% (1570/6144)\n",
      "Loss: 2.128 | Acc: 25.596% (1589/6208)\n",
      "Loss: 2.126 | Acc: 25.622% (1607/6272)\n",
      "Loss: 2.123 | Acc: 25.679% (1627/6336)\n",
      "Loss: 2.120 | Acc: 25.812% (1652/6400)\n",
      "Loss: 2.119 | Acc: 25.789% (1667/6464)\n",
      "Loss: 2.117 | Acc: 25.827% (1686/6528)\n",
      "Loss: 2.115 | Acc: 25.819% (1702/6592)\n",
      "Loss: 2.113 | Acc: 25.856% (1721/6656)\n",
      "Loss: 2.112 | Acc: 25.908% (1741/6720)\n",
      "Loss: 2.109 | Acc: 25.929% (1759/6784)\n",
      "Loss: 2.108 | Acc: 25.993% (1780/6848)\n",
      "Loss: 2.104 | Acc: 26.027% (1799/6912)\n",
      "Loss: 2.102 | Acc: 25.989% (1813/6976)\n",
      "Loss: 2.098 | Acc: 26.108% (1838/7040)\n",
      "Loss: 2.095 | Acc: 26.197% (1861/7104)\n",
      "Loss: 2.093 | Acc: 26.270% (1883/7168)\n",
      "Loss: 2.092 | Acc: 26.272% (1900/7232)\n",
      "Loss: 2.092 | Acc: 26.288% (1918/7296)\n",
      "Loss: 2.091 | Acc: 26.196% (1928/7360)\n",
      "Loss: 2.091 | Acc: 26.185% (1944/7424)\n",
      "Loss: 2.088 | Acc: 26.282% (1968/7488)\n",
      "Loss: 2.084 | Acc: 26.443% (1997/7552)\n",
      "Loss: 2.081 | Acc: 26.615% (2027/7616)\n",
      "Loss: 2.081 | Acc: 26.615% (2044/7680)\n",
      "Loss: 2.081 | Acc: 26.627% (2062/7744)\n",
      "Loss: 2.079 | Acc: 26.767% (2090/7808)\n",
      "Loss: 2.077 | Acc: 26.817% (2111/7872)\n",
      "Loss: 2.073 | Acc: 26.890% (2134/7936)\n",
      "Loss: 2.072 | Acc: 26.950% (2156/8000)\n",
      "Loss: 2.071 | Acc: 27.009% (2178/8064)\n",
      "Loss: 2.067 | Acc: 27.116% (2204/8128)\n",
      "Loss: 2.065 | Acc: 27.185% (2227/8192)\n",
      "Loss: 2.064 | Acc: 27.156% (2242/8256)\n",
      "Loss: 2.061 | Acc: 27.175% (2261/8320)\n",
      "Loss: 2.058 | Acc: 27.266% (2286/8384)\n",
      "Loss: 2.056 | Acc: 27.344% (2310/8448)\n",
      "Loss: 2.055 | Acc: 27.467% (2338/8512)\n",
      "Loss: 2.052 | Acc: 27.495% (2358/8576)\n",
      "Loss: 2.050 | Acc: 27.569% (2382/8640)\n",
      "Loss: 2.048 | Acc: 27.665% (2408/8704)\n",
      "Loss: 2.046 | Acc: 27.760% (2434/8768)\n",
      "Loss: 2.043 | Acc: 27.808% (2456/8832)\n",
      "Loss: 2.041 | Acc: 27.810% (2474/8896)\n",
      "Loss: 2.039 | Acc: 27.924% (2502/8960)\n",
      "Loss: 2.036 | Acc: 28.036% (2530/9024)\n",
      "Loss: 2.034 | Acc: 28.081% (2552/9088)\n",
      "Loss: 2.032 | Acc: 28.147% (2576/9152)\n",
      "Loss: 2.030 | Acc: 28.158% (2595/9216)\n",
      "Loss: 2.029 | Acc: 28.233% (2620/9280)\n",
      "Loss: 2.027 | Acc: 28.318% (2646/9344)\n",
      "Loss: 2.026 | Acc: 28.369% (2669/9408)\n",
      "Loss: 2.024 | Acc: 28.378% (2688/9472)\n",
      "Loss: 2.022 | Acc: 28.440% (2712/9536)\n",
      "Loss: 2.019 | Acc: 28.479% (2734/9600)\n",
      "Loss: 2.017 | Acc: 28.570% (2761/9664)\n",
      "Loss: 2.015 | Acc: 28.629% (2785/9728)\n",
      "Loss: 2.012 | Acc: 28.799% (2820/9792)\n",
      "Loss: 2.010 | Acc: 28.866% (2845/9856)\n",
      "Loss: 2.008 | Acc: 28.962% (2873/9920)\n",
      "Loss: 2.006 | Acc: 29.036% (2899/9984)\n",
      "Loss: 2.004 | Acc: 29.120% (2926/10048)\n",
      "Loss: 2.002 | Acc: 29.134% (2946/10112)\n",
      "Loss: 2.000 | Acc: 29.206% (2972/10176)\n",
      "Loss: 1.998 | Acc: 29.229% (2993/10240)\n",
      "Loss: 1.996 | Acc: 29.270% (3016/10304)\n",
      "Loss: 1.996 | Acc: 29.263% (3034/10368)\n",
      "Loss: 1.994 | Acc: 29.285% (3055/10432)\n",
      "Loss: 1.992 | Acc: 29.325% (3078/10496)\n",
      "Loss: 1.990 | Acc: 29.375% (3102/10560)\n",
      "Loss: 1.989 | Acc: 29.330% (3116/10624)\n",
      "Loss: 1.988 | Acc: 29.323% (3134/10688)\n",
      "Loss: 1.987 | Acc: 29.306% (3151/10752)\n",
      "Loss: 1.986 | Acc: 29.336% (3173/10816)\n",
      "Loss: 1.985 | Acc: 29.357% (3194/10880)\n",
      "Loss: 1.983 | Acc: 29.441% (3222/10944)\n",
      "Loss: 1.981 | Acc: 29.497% (3247/11008)\n",
      "Loss: 1.980 | Acc: 29.543% (3271/11072)\n",
      "Loss: 1.978 | Acc: 29.643% (3301/11136)\n",
      "Loss: 1.975 | Acc: 29.705% (3327/11200)\n",
      "Loss: 1.975 | Acc: 29.696% (3345/11264)\n",
      "Loss: 1.974 | Acc: 29.732% (3368/11328)\n",
      "Loss: 1.973 | Acc: 29.767% (3391/11392)\n",
      "Loss: 1.972 | Acc: 29.749% (3408/11456)\n",
      "Loss: 1.971 | Acc: 29.783% (3431/11520)\n",
      "Loss: 1.969 | Acc: 29.808% (3453/11584)\n",
      "Loss: 1.967 | Acc: 29.885% (3481/11648)\n",
      "Loss: 1.965 | Acc: 29.927% (3505/11712)\n",
      "Loss: 1.962 | Acc: 29.993% (3532/11776)\n",
      "Loss: 1.962 | Acc: 30.017% (3554/11840)\n",
      "Loss: 1.960 | Acc: 30.108% (3584/11904)\n",
      "Loss: 1.958 | Acc: 30.172% (3611/11968)\n",
      "Loss: 1.956 | Acc: 30.261% (3641/12032)\n",
      "Loss: 1.953 | Acc: 30.365% (3673/12096)\n",
      "Loss: 1.953 | Acc: 30.387% (3695/12160)\n",
      "Loss: 1.951 | Acc: 30.432% (3720/12224)\n",
      "Loss: 1.949 | Acc: 30.501% (3748/12288)\n",
      "Loss: 1.947 | Acc: 30.562% (3775/12352)\n",
      "Loss: 1.945 | Acc: 30.622% (3802/12416)\n",
      "Loss: 1.944 | Acc: 30.649% (3825/12480)\n",
      "Loss: 1.942 | Acc: 30.684% (3849/12544)\n",
      "Loss: 1.941 | Acc: 30.703% (3871/12608)\n",
      "Loss: 1.940 | Acc: 30.745% (3896/12672)\n",
      "Loss: 1.939 | Acc: 30.755% (3917/12736)\n",
      "Loss: 1.938 | Acc: 30.773% (3939/12800)\n",
      "Loss: 1.938 | Acc: 30.776% (3959/12864)\n",
      "Loss: 1.936 | Acc: 30.786% (3980/12928)\n",
      "Loss: 1.934 | Acc: 30.842% (4007/12992)\n",
      "Loss: 1.932 | Acc: 30.890% (4033/13056)\n",
      "Loss: 1.931 | Acc: 30.915% (4056/13120)\n",
      "Loss: 1.930 | Acc: 30.894% (4073/13184)\n",
      "Loss: 1.930 | Acc: 30.895% (4093/13248)\n",
      "Loss: 1.928 | Acc: 30.927% (4117/13312)\n",
      "Loss: 1.927 | Acc: 30.981% (4144/13376)\n",
      "Loss: 1.925 | Acc: 31.042% (4172/13440)\n",
      "Loss: 1.924 | Acc: 31.057% (4194/13504)\n",
      "Loss: 1.923 | Acc: 31.110% (4221/13568)\n",
      "Loss: 1.922 | Acc: 31.140% (4245/13632)\n",
      "Loss: 1.921 | Acc: 31.177% (4270/13696)\n",
      "Loss: 1.919 | Acc: 31.199% (4293/13760)\n",
      "Loss: 1.918 | Acc: 31.250% (4320/13824)\n",
      "Loss: 1.917 | Acc: 31.286% (4345/13888)\n",
      "Loss: 1.916 | Acc: 31.358% (4375/13952)\n",
      "Loss: 1.914 | Acc: 31.443% (4407/14016)\n",
      "Loss: 1.912 | Acc: 31.491% (4434/14080)\n",
      "Loss: 1.912 | Acc: 31.497% (4455/14144)\n",
      "Loss: 1.911 | Acc: 31.503% (4476/14208)\n",
      "Loss: 1.909 | Acc: 31.537% (4501/14272)\n",
      "Loss: 1.908 | Acc: 31.571% (4526/14336)\n",
      "Loss: 1.907 | Acc: 31.590% (4549/14400)\n",
      "Loss: 1.906 | Acc: 31.651% (4578/14464)\n",
      "Loss: 1.905 | Acc: 31.711% (4607/14528)\n",
      "Loss: 1.904 | Acc: 31.716% (4628/14592)\n",
      "Loss: 1.902 | Acc: 31.762% (4655/14656)\n",
      "Loss: 1.901 | Acc: 31.760% (4675/14720)\n",
      "Loss: 1.900 | Acc: 31.832% (4706/14784)\n",
      "Loss: 1.900 | Acc: 31.863% (4731/14848)\n",
      "Loss: 1.898 | Acc: 31.941% (4763/14912)\n",
      "Loss: 1.898 | Acc: 31.944% (4784/14976)\n",
      "Loss: 1.897 | Acc: 31.981% (4810/15040)\n",
      "Loss: 1.896 | Acc: 32.011% (4835/15104)\n",
      "Loss: 1.894 | Acc: 32.068% (4864/15168)\n",
      "Loss: 1.892 | Acc: 32.103% (4890/15232)\n",
      "Loss: 1.891 | Acc: 32.120% (4913/15296)\n",
      "Loss: 1.890 | Acc: 32.174% (4942/15360)\n",
      "Loss: 1.889 | Acc: 32.216% (4969/15424)\n",
      "Loss: 1.888 | Acc: 32.238% (4993/15488)\n",
      "Loss: 1.888 | Acc: 32.247% (5015/15552)\n",
      "Loss: 1.887 | Acc: 32.243% (5035/15616)\n",
      "Loss: 1.887 | Acc: 32.251% (5057/15680)\n",
      "Loss: 1.887 | Acc: 32.254% (5078/15744)\n",
      "Loss: 1.886 | Acc: 32.294% (5105/15808)\n",
      "Loss: 1.885 | Acc: 32.264% (5121/15872)\n",
      "Loss: 1.884 | Acc: 32.292% (5146/15936)\n",
      "Loss: 1.883 | Acc: 32.319% (5171/16000)\n",
      "Loss: 1.882 | Acc: 32.352% (5197/16064)\n",
      "Loss: 1.882 | Acc: 32.372% (5221/16128)\n",
      "Loss: 1.881 | Acc: 32.411% (5248/16192)\n",
      "Loss: 1.880 | Acc: 32.450% (5275/16256)\n",
      "Loss: 1.880 | Acc: 32.451% (5296/16320)\n",
      "Loss: 1.879 | Acc: 32.489% (5323/16384)\n",
      "Loss: 1.878 | Acc: 32.502% (5346/16448)\n",
      "Loss: 1.877 | Acc: 32.516% (5369/16512)\n",
      "Loss: 1.876 | Acc: 32.541% (5394/16576)\n",
      "Loss: 1.876 | Acc: 32.572% (5420/16640)\n",
      "Loss: 1.875 | Acc: 32.549% (5437/16704)\n",
      "Loss: 1.874 | Acc: 32.586% (5464/16768)\n",
      "Loss: 1.873 | Acc: 32.599% (5487/16832)\n",
      "Loss: 1.872 | Acc: 32.599% (5508/16896)\n",
      "Loss: 1.871 | Acc: 32.647% (5537/16960)\n",
      "Loss: 1.871 | Acc: 32.636% (5556/17024)\n",
      "Loss: 1.872 | Acc: 32.631% (5576/17088)\n",
      "Loss: 1.870 | Acc: 32.649% (5600/17152)\n",
      "Loss: 1.869 | Acc: 32.714% (5632/17216)\n",
      "Loss: 1.869 | Acc: 32.737% (5657/17280)\n",
      "Loss: 1.868 | Acc: 32.784% (5686/17344)\n",
      "Loss: 1.867 | Acc: 32.790% (5708/17408)\n",
      "Loss: 1.866 | Acc: 32.835% (5737/17472)\n",
      "Loss: 1.864 | Acc: 32.881% (5766/17536)\n",
      "Loss: 1.865 | Acc: 32.864% (5784/17600)\n",
      "Loss: 1.864 | Acc: 32.897% (5811/17664)\n",
      "Loss: 1.863 | Acc: 32.903% (5833/17728)\n",
      "Loss: 1.862 | Acc: 32.914% (5856/17792)\n",
      "Loss: 1.861 | Acc: 32.930% (5880/17856)\n",
      "Loss: 1.860 | Acc: 32.963% (5907/17920)\n",
      "Loss: 1.859 | Acc: 32.996% (5934/17984)\n",
      "Loss: 1.858 | Acc: 33.029% (5961/18048)\n",
      "Loss: 1.857 | Acc: 33.072% (5990/18112)\n",
      "Loss: 1.856 | Acc: 33.099% (6016/18176)\n",
      "Loss: 1.855 | Acc: 33.125% (6042/18240)\n",
      "Loss: 1.854 | Acc: 33.179% (6073/18304)\n",
      "Loss: 1.853 | Acc: 33.177% (6094/18368)\n",
      "Loss: 1.853 | Acc: 33.214% (6122/18432)\n",
      "Loss: 1.852 | Acc: 33.245% (6149/18496)\n",
      "Loss: 1.851 | Acc: 33.276% (6176/18560)\n",
      "Loss: 1.849 | Acc: 33.328% (6207/18624)\n",
      "Loss: 1.848 | Acc: 33.358% (6234/18688)\n",
      "Loss: 1.847 | Acc: 33.404% (6264/18752)\n",
      "Loss: 1.847 | Acc: 33.418% (6288/18816)\n",
      "Loss: 1.846 | Acc: 33.432% (6312/18880)\n",
      "Loss: 1.844 | Acc: 33.488% (6344/18944)\n",
      "Loss: 1.843 | Acc: 33.560% (6379/19008)\n",
      "Loss: 1.842 | Acc: 33.594% (6407/19072)\n",
      "Loss: 1.841 | Acc: 33.643% (6438/19136)\n",
      "Loss: 1.841 | Acc: 33.630% (6457/19200)\n",
      "Loss: 1.841 | Acc: 33.638% (6480/19264)\n",
      "Loss: 1.840 | Acc: 33.687% (6511/19328)\n",
      "Loss: 1.839 | Acc: 33.751% (6545/19392)\n",
      "Loss: 1.838 | Acc: 33.794% (6575/19456)\n",
      "Loss: 1.838 | Acc: 33.806% (6599/19520)\n",
      "Loss: 1.837 | Acc: 33.839% (6627/19584)\n",
      "Loss: 1.836 | Acc: 33.841% (6649/19648)\n",
      "Loss: 1.835 | Acc: 33.873% (6677/19712)\n",
      "Loss: 1.835 | Acc: 33.895% (6703/19776)\n",
      "Loss: 1.834 | Acc: 33.947% (6735/19840)\n",
      "Loss: 1.832 | Acc: 34.003% (6768/19904)\n",
      "Loss: 1.831 | Acc: 34.059% (6801/19968)\n",
      "Loss: 1.831 | Acc: 34.046% (6820/20032)\n",
      "Loss: 1.830 | Acc: 34.101% (6853/20096)\n",
      "Loss: 1.829 | Acc: 34.162% (6887/20160)\n",
      "Loss: 1.828 | Acc: 34.192% (6915/20224)\n",
      "Loss: 1.827 | Acc: 34.262% (6951/20288)\n",
      "Loss: 1.826 | Acc: 34.311% (6983/20352)\n",
      "Loss: 1.825 | Acc: 34.306% (7004/20416)\n",
      "Loss: 1.824 | Acc: 34.336% (7032/20480)\n",
      "Loss: 1.824 | Acc: 34.331% (7053/20544)\n",
      "Loss: 1.824 | Acc: 34.326% (7074/20608)\n",
      "Loss: 1.823 | Acc: 34.360% (7103/20672)\n",
      "Loss: 1.824 | Acc: 34.341% (7121/20736)\n",
      "Loss: 1.823 | Acc: 34.380% (7151/20800)\n",
      "Loss: 1.822 | Acc: 34.428% (7183/20864)\n",
      "Loss: 1.821 | Acc: 34.418% (7203/20928)\n",
      "Loss: 1.821 | Acc: 34.446% (7231/20992)\n",
      "Loss: 1.819 | Acc: 34.479% (7260/21056)\n",
      "Loss: 1.819 | Acc: 34.465% (7279/21120)\n",
      "Loss: 1.818 | Acc: 34.521% (7313/21184)\n",
      "Loss: 1.817 | Acc: 34.540% (7339/21248)\n",
      "Loss: 1.816 | Acc: 34.596% (7373/21312)\n",
      "Loss: 1.815 | Acc: 34.614% (7399/21376)\n",
      "Loss: 1.814 | Acc: 34.627% (7424/21440)\n",
      "Loss: 1.814 | Acc: 34.668% (7455/21504)\n",
      "Loss: 1.813 | Acc: 34.704% (7485/21568)\n",
      "Loss: 1.812 | Acc: 34.754% (7518/21632)\n",
      "Loss: 1.811 | Acc: 34.804% (7551/21696)\n",
      "Loss: 1.810 | Acc: 34.839% (7581/21760)\n",
      "Loss: 1.810 | Acc: 34.852% (7606/21824)\n",
      "Loss: 1.808 | Acc: 34.873% (7633/21888)\n",
      "Loss: 1.808 | Acc: 34.872% (7655/21952)\n",
      "Loss: 1.807 | Acc: 34.888% (7681/22016)\n",
      "Loss: 1.806 | Acc: 34.900% (7706/22080)\n",
      "Loss: 1.805 | Acc: 34.930% (7735/22144)\n",
      "Loss: 1.804 | Acc: 34.969% (7766/22208)\n",
      "Loss: 1.804 | Acc: 34.981% (7791/22272)\n",
      "Loss: 1.804 | Acc: 34.993% (7816/22336)\n",
      "Loss: 1.803 | Acc: 35.004% (7841/22400)\n",
      "Loss: 1.803 | Acc: 35.016% (7866/22464)\n",
      "Loss: 1.801 | Acc: 35.059% (7898/22528)\n",
      "Loss: 1.801 | Acc: 35.061% (7921/22592)\n",
      "Loss: 1.801 | Acc: 35.099% (7952/22656)\n",
      "Loss: 1.800 | Acc: 35.145% (7985/22720)\n",
      "Loss: 1.800 | Acc: 35.156% (8010/22784)\n",
      "Loss: 1.799 | Acc: 35.180% (8038/22848)\n",
      "Loss: 1.799 | Acc: 35.200% (8065/22912)\n",
      "Loss: 1.798 | Acc: 35.206% (8089/22976)\n",
      "Loss: 1.797 | Acc: 35.239% (8119/23040)\n",
      "Loss: 1.797 | Acc: 35.280% (8151/23104)\n",
      "Loss: 1.797 | Acc: 35.286% (8175/23168)\n",
      "Loss: 1.796 | Acc: 35.296% (8200/23232)\n",
      "Loss: 1.796 | Acc: 35.294% (8222/23296)\n",
      "Loss: 1.795 | Acc: 35.308% (8248/23360)\n",
      "Loss: 1.795 | Acc: 35.327% (8275/23424)\n",
      "Loss: 1.794 | Acc: 35.337% (8300/23488)\n",
      "Loss: 1.794 | Acc: 35.352% (8326/23552)\n",
      "Loss: 1.794 | Acc: 35.370% (8353/23616)\n",
      "Loss: 1.792 | Acc: 35.410% (8385/23680)\n",
      "Loss: 1.792 | Acc: 35.432% (8413/23744)\n",
      "Loss: 1.792 | Acc: 35.459% (8442/23808)\n",
      "Loss: 1.793 | Acc: 35.431% (8458/23872)\n",
      "Loss: 1.791 | Acc: 35.470% (8490/23936)\n",
      "Loss: 1.791 | Acc: 35.454% (8509/24000)\n",
      "Loss: 1.790 | Acc: 35.509% (8545/24064)\n",
      "Loss: 1.790 | Acc: 35.511% (8568/24128)\n",
      "Loss: 1.790 | Acc: 35.508% (8590/24192)\n",
      "Loss: 1.789 | Acc: 35.533% (8619/24256)\n",
      "Loss: 1.788 | Acc: 35.555% (8647/24320)\n",
      "Loss: 1.787 | Acc: 35.585% (8677/24384)\n",
      "Loss: 1.786 | Acc: 35.623% (8709/24448)\n",
      "Loss: 1.786 | Acc: 35.627% (8733/24512)\n",
      "Loss: 1.786 | Acc: 35.657% (8763/24576)\n",
      "Loss: 1.785 | Acc: 35.670% (8789/24640)\n",
      "Loss: 1.785 | Acc: 35.678% (8814/24704)\n",
      "Loss: 1.784 | Acc: 35.695% (8841/24768)\n",
      "Loss: 1.783 | Acc: 35.724% (8871/24832)\n",
      "Loss: 1.783 | Acc: 35.745% (8899/24896)\n",
      "Loss: 1.782 | Acc: 35.777% (8930/24960)\n",
      "Loss: 1.782 | Acc: 35.774% (8952/25024)\n",
      "Loss: 1.781 | Acc: 35.814% (8985/25088)\n",
      "Loss: 1.781 | Acc: 35.834% (9013/25152)\n",
      "Loss: 1.780 | Acc: 35.866% (9044/25216)\n",
      "Loss: 1.779 | Acc: 35.882% (9071/25280)\n",
      "Loss: 1.778 | Acc: 35.934% (9107/25344)\n",
      "Loss: 1.777 | Acc: 35.938% (9131/25408)\n",
      "Loss: 1.776 | Acc: 35.985% (9166/25472)\n",
      "Loss: 1.776 | Acc: 35.992% (9191/25536)\n",
      "Loss: 1.775 | Acc: 36.020% (9221/25600)\n",
      "Loss: 1.774 | Acc: 36.066% (9256/25664)\n",
      "Loss: 1.774 | Acc: 36.066% (9279/25728)\n",
      "Loss: 1.773 | Acc: 36.108% (9313/25792)\n",
      "Loss: 1.772 | Acc: 36.142% (9345/25856)\n",
      "Loss: 1.772 | Acc: 36.154% (9371/25920)\n",
      "Loss: 1.771 | Acc: 36.172% (9399/25984)\n",
      "Loss: 1.770 | Acc: 36.241% (9440/26048)\n",
      "Loss: 1.769 | Acc: 36.278% (9473/26112)\n",
      "Loss: 1.769 | Acc: 36.285% (9498/26176)\n",
      "Loss: 1.769 | Acc: 36.303% (9526/26240)\n",
      "Loss: 1.768 | Acc: 36.325% (9555/26304)\n",
      "Loss: 1.767 | Acc: 36.343% (9583/26368)\n",
      "Loss: 1.767 | Acc: 36.346% (9607/26432)\n",
      "Loss: 1.766 | Acc: 36.364% (9635/26496)\n",
      "Loss: 1.766 | Acc: 36.382% (9663/26560)\n",
      "Loss: 1.766 | Acc: 36.396% (9690/26624)\n",
      "Loss: 1.765 | Acc: 36.436% (9724/26688)\n",
      "Loss: 1.764 | Acc: 36.465% (9755/26752)\n",
      "Loss: 1.765 | Acc: 36.441% (9772/26816)\n",
      "Loss: 1.764 | Acc: 36.492% (9809/26880)\n",
      "Loss: 1.764 | Acc: 36.498% (9834/26944)\n",
      "Loss: 1.764 | Acc: 36.504% (9859/27008)\n",
      "Loss: 1.763 | Acc: 36.540% (9892/27072)\n",
      "Loss: 1.763 | Acc: 36.568% (9923/27136)\n",
      "Loss: 1.761 | Acc: 36.607% (9957/27200)\n",
      "Loss: 1.761 | Acc: 36.638% (9989/27264)\n",
      "Loss: 1.760 | Acc: 36.666% (10020/27328)\n",
      "Loss: 1.759 | Acc: 36.682% (10048/27392)\n",
      "Loss: 1.759 | Acc: 36.688% (10073/27456)\n",
      "Loss: 1.759 | Acc: 36.693% (10098/27520)\n",
      "Loss: 1.759 | Acc: 36.706% (10125/27584)\n",
      "Loss: 1.758 | Acc: 36.715% (10151/27648)\n",
      "Loss: 1.759 | Acc: 36.706% (10172/27712)\n",
      "Loss: 1.758 | Acc: 36.726% (10201/27776)\n",
      "Loss: 1.757 | Acc: 36.767% (10236/27840)\n",
      "Loss: 1.758 | Acc: 36.758% (10257/27904)\n",
      "Loss: 1.758 | Acc: 36.756% (10280/27968)\n",
      "Loss: 1.758 | Acc: 36.747% (10301/28032)\n",
      "Loss: 1.757 | Acc: 36.753% (10326/28096)\n",
      "Loss: 1.757 | Acc: 36.772% (10355/28160)\n",
      "Loss: 1.757 | Acc: 36.781% (10381/28224)\n",
      "Loss: 1.756 | Acc: 36.779% (10404/28288)\n",
      "Loss: 1.756 | Acc: 36.795% (10432/28352)\n",
      "Loss: 1.755 | Acc: 36.821% (10463/28416)\n",
      "Loss: 1.755 | Acc: 36.822% (10487/28480)\n",
      "Loss: 1.755 | Acc: 36.852% (10519/28544)\n",
      "Loss: 1.754 | Acc: 36.860% (10545/28608)\n",
      "Loss: 1.754 | Acc: 36.862% (10569/28672)\n",
      "Loss: 1.754 | Acc: 36.874% (10596/28736)\n",
      "Loss: 1.753 | Acc: 36.892% (10625/28800)\n",
      "Loss: 1.753 | Acc: 36.921% (10657/28864)\n",
      "Loss: 1.752 | Acc: 36.950% (10689/28928)\n",
      "Loss: 1.752 | Acc: 36.965% (10717/28992)\n",
      "Loss: 1.751 | Acc: 36.994% (10749/29056)\n",
      "Loss: 1.750 | Acc: 37.009% (10777/29120)\n",
      "Loss: 1.750 | Acc: 37.027% (10806/29184)\n",
      "Loss: 1.749 | Acc: 37.028% (10830/29248)\n",
      "Loss: 1.750 | Acc: 37.033% (10855/29312)\n",
      "Loss: 1.749 | Acc: 37.047% (10883/29376)\n",
      "Loss: 1.748 | Acc: 37.065% (10912/29440)\n",
      "Loss: 1.747 | Acc: 37.076% (10939/29504)\n",
      "Loss: 1.747 | Acc: 37.081% (10964/29568)\n",
      "Loss: 1.746 | Acc: 37.102% (10994/29632)\n",
      "Loss: 1.746 | Acc: 37.126% (11025/29696)\n",
      "Loss: 1.745 | Acc: 37.137% (11052/29760)\n",
      "Loss: 1.745 | Acc: 37.124% (11072/29824)\n",
      "Loss: 1.744 | Acc: 37.162% (11107/29888)\n",
      "Loss: 1.744 | Acc: 37.186% (11138/29952)\n",
      "Loss: 1.743 | Acc: 37.210% (11169/30016)\n",
      "Loss: 1.742 | Acc: 37.241% (11202/30080)\n",
      "Loss: 1.741 | Acc: 37.291% (11241/30144)\n",
      "Loss: 1.740 | Acc: 37.321% (11274/30208)\n",
      "Loss: 1.739 | Acc: 37.345% (11305/30272)\n",
      "Loss: 1.739 | Acc: 37.358% (11333/30336)\n",
      "Loss: 1.738 | Acc: 37.378% (11363/30400)\n",
      "Loss: 1.737 | Acc: 37.411% (11397/30464)\n",
      "Loss: 1.737 | Acc: 37.431% (11427/30528)\n",
      "Loss: 1.736 | Acc: 37.458% (11459/30592)\n",
      "Loss: 1.736 | Acc: 37.464% (11485/30656)\n",
      "Loss: 1.735 | Acc: 37.471% (11511/30720)\n",
      "Loss: 1.735 | Acc: 37.487% (11540/30784)\n",
      "Loss: 1.734 | Acc: 37.503% (11569/30848)\n",
      "Loss: 1.733 | Acc: 37.529% (11601/30912)\n",
      "Loss: 1.733 | Acc: 37.555% (11633/30976)\n",
      "Loss: 1.733 | Acc: 37.548% (11655/31040)\n",
      "Loss: 1.733 | Acc: 37.561% (11683/31104)\n",
      "Loss: 1.732 | Acc: 37.580% (11713/31168)\n",
      "Loss: 1.732 | Acc: 37.606% (11745/31232)\n",
      "Loss: 1.731 | Acc: 37.625% (11775/31296)\n",
      "Loss: 1.731 | Acc: 37.628% (11800/31360)\n",
      "Loss: 1.731 | Acc: 37.640% (11828/31424)\n",
      "Loss: 1.731 | Acc: 37.640% (11852/31488)\n",
      "Loss: 1.730 | Acc: 37.639% (11876/31552)\n",
      "Loss: 1.730 | Acc: 37.661% (11907/31616)\n",
      "Loss: 1.730 | Acc: 37.652% (11928/31680)\n",
      "Loss: 1.731 | Acc: 37.645% (11950/31744)\n",
      "Loss: 1.730 | Acc: 37.667% (11981/31808)\n",
      "Loss: 1.731 | Acc: 37.657% (12002/31872)\n",
      "Loss: 1.731 | Acc: 37.653% (12025/31936)\n",
      "Loss: 1.730 | Acc: 37.666% (12053/32000)\n",
      "Loss: 1.730 | Acc: 37.681% (12082/32064)\n",
      "Loss: 1.729 | Acc: 37.699% (12112/32128)\n",
      "Loss: 1.729 | Acc: 37.708% (12139/32192)\n",
      "Loss: 1.729 | Acc: 37.698% (12160/32256)\n",
      "Loss: 1.729 | Acc: 37.717% (12190/32320)\n",
      "Loss: 1.728 | Acc: 37.738% (12221/32384)\n",
      "Loss: 1.728 | Acc: 37.753% (12250/32448)\n",
      "Loss: 1.728 | Acc: 37.752% (12274/32512)\n",
      "Loss: 1.727 | Acc: 37.755% (12299/32576)\n",
      "Loss: 1.727 | Acc: 37.760% (12325/32640)\n",
      "Loss: 1.727 | Acc: 37.760% (12349/32704)\n",
      "Loss: 1.726 | Acc: 37.796% (12385/32768)\n",
      "Loss: 1.725 | Acc: 37.808% (12413/32832)\n",
      "Loss: 1.724 | Acc: 37.853% (12452/32896)\n",
      "Loss: 1.723 | Acc: 37.882% (12486/32960)\n",
      "Loss: 1.723 | Acc: 37.900% (12516/33024)\n",
      "Loss: 1.722 | Acc: 37.923% (12548/33088)\n",
      "Loss: 1.722 | Acc: 37.940% (12578/33152)\n",
      "Loss: 1.721 | Acc: 37.970% (12612/33216)\n",
      "Loss: 1.721 | Acc: 37.999% (12646/33280)\n",
      "Loss: 1.720 | Acc: 38.013% (12675/33344)\n",
      "Loss: 1.720 | Acc: 38.024% (12703/33408)\n",
      "Loss: 1.720 | Acc: 38.038% (12732/33472)\n",
      "Loss: 1.719 | Acc: 38.052% (12761/33536)\n",
      "Loss: 1.719 | Acc: 38.065% (12790/33600)\n",
      "Loss: 1.719 | Acc: 38.053% (12810/33664)\n",
      "Loss: 1.719 | Acc: 38.043% (12831/33728)\n",
      "Loss: 1.718 | Acc: 38.062% (12862/33792)\n",
      "Loss: 1.719 | Acc: 38.038% (12878/33856)\n",
      "Loss: 1.718 | Acc: 38.066% (12912/33920)\n",
      "Loss: 1.717 | Acc: 38.091% (12945/33984)\n",
      "Loss: 1.717 | Acc: 38.111% (12976/34048)\n",
      "Loss: 1.716 | Acc: 38.116% (13002/34112)\n",
      "Loss: 1.716 | Acc: 38.147% (13037/34176)\n",
      "Loss: 1.716 | Acc: 38.160% (13066/34240)\n",
      "Loss: 1.715 | Acc: 38.165% (13092/34304)\n",
      "Loss: 1.715 | Acc: 38.198% (13128/34368)\n",
      "Loss: 1.714 | Acc: 38.209% (13156/34432)\n",
      "Loss: 1.714 | Acc: 38.222% (13185/34496)\n",
      "Loss: 1.713 | Acc: 38.244% (13217/34560)\n",
      "Loss: 1.713 | Acc: 38.251% (13244/34624)\n",
      "Loss: 1.712 | Acc: 38.281% (13279/34688)\n",
      "Loss: 1.712 | Acc: 38.306% (13312/34752)\n",
      "Loss: 1.712 | Acc: 38.298% (13334/34816)\n",
      "Loss: 1.712 | Acc: 38.308% (13362/34880)\n",
      "Loss: 1.711 | Acc: 38.321% (13391/34944)\n",
      "Loss: 1.711 | Acc: 38.340% (13422/35008)\n",
      "Loss: 1.711 | Acc: 38.341% (13447/35072)\n",
      "Loss: 1.710 | Acc: 38.362% (13479/35136)\n",
      "Loss: 1.710 | Acc: 38.369% (13506/35200)\n",
      "Loss: 1.710 | Acc: 38.402% (13542/35264)\n",
      "Loss: 1.709 | Acc: 38.420% (13573/35328)\n",
      "Loss: 1.709 | Acc: 38.413% (13595/35392)\n",
      "Loss: 1.709 | Acc: 38.414% (13620/35456)\n",
      "Loss: 1.708 | Acc: 38.415% (13645/35520)\n",
      "Loss: 1.708 | Acc: 38.422% (13672/35584)\n",
      "Loss: 1.708 | Acc: 38.440% (13703/35648)\n",
      "Loss: 1.707 | Acc: 38.477% (13741/35712)\n",
      "Loss: 1.706 | Acc: 38.498% (13773/35776)\n",
      "Loss: 1.705 | Acc: 38.524% (13807/35840)\n",
      "Loss: 1.705 | Acc: 38.542% (13838/35904)\n",
      "Loss: 1.705 | Acc: 38.551% (13866/35968)\n",
      "Loss: 1.705 | Acc: 38.549% (13890/36032)\n",
      "Loss: 1.704 | Acc: 38.550% (13915/36096)\n",
      "Loss: 1.704 | Acc: 38.543% (13937/36160)\n",
      "Loss: 1.704 | Acc: 38.566% (13970/36224)\n",
      "Loss: 1.703 | Acc: 38.594% (14005/36288)\n",
      "Loss: 1.702 | Acc: 38.625% (14041/36352)\n",
      "Loss: 1.702 | Acc: 38.629% (14067/36416)\n",
      "Loss: 1.701 | Acc: 38.640% (14096/36480)\n",
      "Loss: 1.701 | Acc: 38.655% (14126/36544)\n",
      "Loss: 1.700 | Acc: 38.680% (14160/36608)\n",
      "Loss: 1.700 | Acc: 38.703% (14193/36672)\n",
      "Loss: 1.699 | Acc: 38.725% (14226/36736)\n",
      "Loss: 1.699 | Acc: 38.734% (14254/36800)\n",
      "Loss: 1.699 | Acc: 38.737% (14280/36864)\n",
      "Loss: 1.698 | Acc: 38.754% (14311/36928)\n",
      "Loss: 1.698 | Acc: 38.771% (14342/36992)\n",
      "Loss: 1.698 | Acc: 38.782% (14371/37056)\n",
      "Loss: 1.697 | Acc: 38.796% (14401/37120)\n",
      "Loss: 1.697 | Acc: 38.823% (14436/37184)\n",
      "Loss: 1.696 | Acc: 38.837% (14466/37248)\n",
      "Loss: 1.696 | Acc: 38.840% (14492/37312)\n",
      "Loss: 1.696 | Acc: 38.846% (14519/37376)\n",
      "Loss: 1.696 | Acc: 38.860% (14549/37440)\n",
      "Loss: 1.695 | Acc: 38.876% (14580/37504)\n",
      "Loss: 1.694 | Acc: 38.900% (14614/37568)\n",
      "Loss: 1.694 | Acc: 38.924% (14648/37632)\n",
      "Loss: 1.694 | Acc: 38.930% (14675/37696)\n",
      "Loss: 1.693 | Acc: 38.943% (14705/37760)\n",
      "Loss: 1.693 | Acc: 38.944% (14730/37824)\n",
      "Loss: 1.693 | Acc: 38.949% (14757/37888)\n",
      "Loss: 1.692 | Acc: 38.981% (14794/37952)\n",
      "Loss: 1.692 | Acc: 39.007% (14829/38016)\n",
      "Loss: 1.691 | Acc: 39.026% (14861/38080)\n",
      "Loss: 1.691 | Acc: 39.023% (14885/38144)\n",
      "Loss: 1.690 | Acc: 39.047% (14919/38208)\n",
      "Loss: 1.690 | Acc: 39.055% (14947/38272)\n",
      "Loss: 1.690 | Acc: 39.049% (14970/38336)\n",
      "Loss: 1.690 | Acc: 39.076% (15005/38400)\n",
      "Loss: 1.690 | Acc: 39.062% (15025/38464)\n",
      "Loss: 1.690 | Acc: 39.057% (15048/38528)\n",
      "Loss: 1.689 | Acc: 39.083% (15083/38592)\n",
      "Loss: 1.689 | Acc: 39.101% (15115/38656)\n",
      "Loss: 1.689 | Acc: 39.112% (15144/38720)\n",
      "Loss: 1.688 | Acc: 39.135% (15178/38784)\n",
      "Loss: 1.688 | Acc: 39.158% (15212/38848)\n",
      "Loss: 1.687 | Acc: 39.165% (15240/38912)\n",
      "Loss: 1.687 | Acc: 39.178% (15270/38976)\n",
      "Loss: 1.687 | Acc: 39.193% (15301/39040)\n",
      "Loss: 1.686 | Acc: 39.211% (15333/39104)\n",
      "Loss: 1.686 | Acc: 39.244% (15371/39168)\n",
      "Loss: 1.685 | Acc: 39.243% (15396/39232)\n",
      "Loss: 1.685 | Acc: 39.253% (15425/39296)\n",
      "Loss: 1.685 | Acc: 39.268% (15456/39360)\n",
      "Loss: 1.685 | Acc: 39.281% (15486/39424)\n",
      "Loss: 1.685 | Acc: 39.280% (15511/39488)\n",
      "Loss: 1.685 | Acc: 39.290% (15540/39552)\n",
      "Loss: 1.684 | Acc: 39.305% (15571/39616)\n",
      "Loss: 1.683 | Acc: 39.335% (15608/39680)\n",
      "Loss: 1.683 | Acc: 39.354% (15641/39744)\n",
      "Loss: 1.682 | Acc: 39.346% (15663/39808)\n",
      "Loss: 1.682 | Acc: 39.368% (15697/39872)\n",
      "Loss: 1.682 | Acc: 39.373% (15724/39936)\n",
      "Loss: 1.681 | Acc: 39.383% (15753/40000)\n",
      "Loss: 1.681 | Acc: 39.412% (15790/40064)\n",
      "Loss: 1.680 | Acc: 39.436% (15825/40128)\n",
      "Loss: 1.679 | Acc: 39.456% (15858/40192)\n",
      "Loss: 1.679 | Acc: 39.462% (15886/40256)\n",
      "Loss: 1.678 | Acc: 39.482% (15919/40320)\n",
      "Loss: 1.678 | Acc: 39.486% (15946/40384)\n",
      "Loss: 1.678 | Acc: 39.500% (15977/40448)\n",
      "Loss: 1.677 | Acc: 39.519% (16010/40512)\n",
      "Loss: 1.677 | Acc: 39.514% (16033/40576)\n",
      "Loss: 1.677 | Acc: 39.515% (16059/40640)\n",
      "Loss: 1.677 | Acc: 39.532% (16091/40704)\n",
      "Loss: 1.676 | Acc: 39.556% (16126/40768)\n",
      "Loss: 1.676 | Acc: 39.567% (16156/40832)\n",
      "Loss: 1.676 | Acc: 39.566% (16181/40896)\n",
      "Loss: 1.675 | Acc: 39.578% (16211/40960)\n",
      "Loss: 1.675 | Acc: 39.601% (16246/41024)\n",
      "Loss: 1.674 | Acc: 39.639% (16287/41088)\n",
      "Loss: 1.674 | Acc: 39.648% (16316/41152)\n",
      "Loss: 1.673 | Acc: 39.650% (16342/41216)\n",
      "Loss: 1.673 | Acc: 39.666% (16374/41280)\n",
      "Loss: 1.673 | Acc: 39.677% (16404/41344)\n",
      "Loss: 1.673 | Acc: 39.688% (16434/41408)\n",
      "Loss: 1.673 | Acc: 39.706% (16467/41472)\n",
      "Loss: 1.672 | Acc: 39.732% (16503/41536)\n",
      "Loss: 1.672 | Acc: 39.745% (16534/41600)\n",
      "Loss: 1.671 | Acc: 39.766% (16568/41664)\n",
      "Loss: 1.671 | Acc: 39.767% (16594/41728)\n",
      "Loss: 1.671 | Acc: 39.785% (16627/41792)\n",
      "Loss: 1.670 | Acc: 39.789% (16654/41856)\n",
      "Loss: 1.670 | Acc: 39.790% (16680/41920)\n",
      "Loss: 1.670 | Acc: 39.813% (16715/41984)\n",
      "Loss: 1.669 | Acc: 39.843% (16753/42048)\n",
      "Loss: 1.668 | Acc: 39.858% (16785/42112)\n",
      "Loss: 1.668 | Acc: 39.881% (16820/42176)\n",
      "Loss: 1.667 | Acc: 39.898% (16853/42240)\n",
      "Loss: 1.668 | Acc: 39.897% (16878/42304)\n",
      "Loss: 1.667 | Acc: 39.919% (16913/42368)\n",
      "Loss: 1.666 | Acc: 39.956% (16954/42432)\n",
      "Loss: 1.666 | Acc: 39.947% (16976/42496)\n",
      "Loss: 1.665 | Acc: 39.974% (17013/42560)\n",
      "Loss: 1.665 | Acc: 39.996% (17048/42624)\n",
      "Loss: 1.665 | Acc: 40.002% (17076/42688)\n",
      "Loss: 1.665 | Acc: 39.998% (17100/42752)\n",
      "Loss: 1.665 | Acc: 40.011% (17131/42816)\n",
      "Loss: 1.664 | Acc: 40.019% (17160/42880)\n",
      "Loss: 1.664 | Acc: 40.027% (17189/42944)\n",
      "Loss: 1.664 | Acc: 40.037% (17219/43008)\n",
      "Loss: 1.664 | Acc: 40.042% (17247/43072)\n",
      "Loss: 1.663 | Acc: 40.066% (17283/43136)\n",
      "Loss: 1.663 | Acc: 40.088% (17318/43200)\n",
      "Loss: 1.662 | Acc: 40.103% (17350/43264)\n",
      "Loss: 1.662 | Acc: 40.103% (17376/43328)\n",
      "Loss: 1.662 | Acc: 40.120% (17409/43392)\n",
      "Loss: 1.662 | Acc: 40.119% (17434/43456)\n",
      "Loss: 1.661 | Acc: 40.129% (17464/43520)\n",
      "Loss: 1.661 | Acc: 40.120% (17486/43584)\n",
      "Loss: 1.661 | Acc: 40.135% (17518/43648)\n",
      "Loss: 1.661 | Acc: 40.154% (17552/43712)\n",
      "Loss: 1.660 | Acc: 40.152% (17577/43776)\n",
      "Loss: 1.660 | Acc: 40.167% (17609/43840)\n",
      "Loss: 1.660 | Acc: 40.169% (17636/43904)\n",
      "Loss: 1.660 | Acc: 40.168% (17661/43968)\n",
      "Loss: 1.659 | Acc: 40.171% (17688/44032)\n",
      "Loss: 1.659 | Acc: 40.192% (17723/44096)\n",
      "Loss: 1.659 | Acc: 40.208% (17756/44160)\n",
      "Loss: 1.659 | Acc: 40.207% (17781/44224)\n",
      "Loss: 1.658 | Acc: 40.210% (17808/44288)\n",
      "Loss: 1.658 | Acc: 40.224% (17840/44352)\n",
      "Loss: 1.657 | Acc: 40.245% (17875/44416)\n",
      "Loss: 1.657 | Acc: 40.259% (17907/44480)\n",
      "Loss: 1.656 | Acc: 40.266% (17936/44544)\n",
      "Loss: 1.656 | Acc: 40.280% (17968/44608)\n",
      "Loss: 1.656 | Acc: 40.303% (18004/44672)\n",
      "Loss: 1.656 | Acc: 40.312% (18034/44736)\n",
      "Loss: 1.656 | Acc: 40.326% (18066/44800)\n",
      "Loss: 1.655 | Acc: 40.331% (18094/44864)\n",
      "Loss: 1.655 | Acc: 40.327% (18118/44928)\n",
      "Loss: 1.655 | Acc: 40.334% (18147/44992)\n",
      "Loss: 1.655 | Acc: 40.370% (18189/45056)\n",
      "Loss: 1.654 | Acc: 40.377% (18218/45120)\n",
      "Loss: 1.654 | Acc: 40.388% (18249/45184)\n",
      "Loss: 1.654 | Acc: 40.395% (18278/45248)\n",
      "Loss: 1.653 | Acc: 40.422% (18316/45312)\n",
      "Loss: 1.653 | Acc: 40.435% (18348/45376)\n",
      "Loss: 1.653 | Acc: 40.442% (18377/45440)\n",
      "Loss: 1.652 | Acc: 40.458% (18410/45504)\n",
      "Loss: 1.652 | Acc: 40.476% (18444/45568)\n",
      "Loss: 1.651 | Acc: 40.478% (18471/45632)\n",
      "Loss: 1.651 | Acc: 40.470% (18493/45696)\n",
      "Loss: 1.651 | Acc: 40.481% (18524/45760)\n",
      "Loss: 1.651 | Acc: 40.490% (18554/45824)\n",
      "Loss: 1.650 | Acc: 40.490% (18580/45888)\n",
      "Loss: 1.650 | Acc: 40.510% (18615/45952)\n",
      "Loss: 1.650 | Acc: 40.519% (18645/46016)\n",
      "Loss: 1.649 | Acc: 40.540% (18681/46080)\n",
      "Loss: 1.649 | Acc: 40.556% (18714/46144)\n",
      "Loss: 1.648 | Acc: 40.575% (18749/46208)\n",
      "Loss: 1.648 | Acc: 40.582% (18778/46272)\n",
      "Loss: 1.648 | Acc: 40.588% (18807/46336)\n",
      "Loss: 1.648 | Acc: 40.614% (18845/46400)\n",
      "Loss: 1.647 | Acc: 40.623% (18875/46464)\n",
      "Loss: 1.647 | Acc: 40.636% (18907/46528)\n",
      "Loss: 1.647 | Acc: 40.661% (18945/46592)\n",
      "Loss: 1.646 | Acc: 40.679% (18979/46656)\n",
      "Loss: 1.646 | Acc: 40.681% (19006/46720)\n",
      "Loss: 1.646 | Acc: 40.674% (19029/46784)\n",
      "Loss: 1.647 | Acc: 40.670% (19053/46848)\n",
      "Loss: 1.647 | Acc: 40.674% (19081/46912)\n",
      "Loss: 1.646 | Acc: 40.700% (19119/46976)\n",
      "Loss: 1.646 | Acc: 40.716% (19153/47040)\n",
      "Loss: 1.645 | Acc: 40.738% (19189/47104)\n",
      "Loss: 1.645 | Acc: 40.752% (19222/47168)\n",
      "Loss: 1.645 | Acc: 40.756% (19250/47232)\n",
      "Loss: 1.645 | Acc: 40.746% (19271/47296)\n",
      "Loss: 1.645 | Acc: 40.758% (19303/47360)\n",
      "Loss: 1.644 | Acc: 40.766% (19333/47424)\n",
      "Loss: 1.644 | Acc: 40.781% (19366/47488)\n",
      "Loss: 1.643 | Acc: 40.802% (19402/47552)\n",
      "Loss: 1.643 | Acc: 40.816% (19435/47616)\n",
      "Loss: 1.643 | Acc: 40.822% (19464/47680)\n",
      "Loss: 1.642 | Acc: 40.830% (19494/47744)\n",
      "Loss: 1.642 | Acc: 40.840% (19525/47808)\n",
      "Loss: 1.642 | Acc: 40.853% (19557/47872)\n",
      "Loss: 1.641 | Acc: 40.873% (19593/47936)\n",
      "Loss: 1.641 | Acc: 40.890% (19627/48000)\n",
      "Loss: 1.641 | Acc: 40.902% (19659/48064)\n",
      "Loss: 1.640 | Acc: 40.910% (19689/48128)\n",
      "Loss: 1.640 | Acc: 40.920% (19720/48192)\n",
      "Loss: 1.640 | Acc: 40.925% (19749/48256)\n",
      "Loss: 1.640 | Acc: 40.927% (19776/48320)\n",
      "Loss: 1.640 | Acc: 40.954% (19815/48384)\n",
      "Loss: 1.640 | Acc: 40.964% (19846/48448)\n",
      "Loss: 1.639 | Acc: 40.971% (19876/48512)\n",
      "Loss: 1.639 | Acc: 40.983% (19908/48576)\n",
      "Loss: 1.638 | Acc: 41.012% (19948/48640)\n",
      "Loss: 1.638 | Acc: 41.023% (19980/48704)\n",
      "Loss: 1.638 | Acc: 41.041% (20015/48768)\n",
      "Loss: 1.637 | Acc: 41.055% (20048/48832)\n",
      "Loss: 1.637 | Acc: 41.071% (20082/48896)\n",
      "Loss: 1.637 | Acc: 41.087% (20116/48960)\n",
      "Loss: 1.636 | Acc: 41.090% (20134/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 41.089795918367344\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.357 | Acc: 53.125% (34/64)\n",
      "Loss: 1.327 | Acc: 53.906% (69/128)\n",
      "Loss: 1.333 | Acc: 51.042% (98/192)\n",
      "Loss: 1.422 | Acc: 47.656% (122/256)\n",
      "Loss: 1.395 | Acc: 48.750% (156/320)\n",
      "Loss: 1.421 | Acc: 47.656% (183/384)\n",
      "Loss: 1.465 | Acc: 46.429% (208/448)\n",
      "Loss: 1.458 | Acc: 46.484% (238/512)\n",
      "Loss: 1.433 | Acc: 47.569% (274/576)\n",
      "Loss: 1.409 | Acc: 48.750% (312/640)\n",
      "Loss: 1.418 | Acc: 48.153% (339/704)\n",
      "Loss: 1.420 | Acc: 48.307% (371/768)\n",
      "Loss: 1.423 | Acc: 48.317% (402/832)\n",
      "Loss: 1.412 | Acc: 48.884% (438/896)\n",
      "Loss: 1.407 | Acc: 48.646% (467/960)\n",
      "Loss: 1.386 | Acc: 49.512% (507/1024)\n",
      "Loss: 1.383 | Acc: 49.449% (538/1088)\n",
      "Loss: 1.385 | Acc: 49.566% (571/1152)\n",
      "Loss: 1.385 | Acc: 50.082% (609/1216)\n",
      "Loss: 1.395 | Acc: 49.688% (636/1280)\n",
      "Loss: 1.399 | Acc: 49.107% (660/1344)\n",
      "Loss: 1.398 | Acc: 48.935% (689/1408)\n",
      "Loss: 1.388 | Acc: 49.321% (726/1472)\n",
      "Loss: 1.390 | Acc: 49.740% (764/1536)\n",
      "Loss: 1.389 | Acc: 49.750% (796/1600)\n",
      "Loss: 1.393 | Acc: 49.700% (827/1664)\n",
      "Loss: 1.395 | Acc: 49.826% (861/1728)\n",
      "Loss: 1.398 | Acc: 49.721% (891/1792)\n",
      "Loss: 1.401 | Acc: 49.300% (915/1856)\n",
      "Loss: 1.402 | Acc: 49.635% (953/1920)\n",
      "Loss: 1.402 | Acc: 49.798% (988/1984)\n",
      "Loss: 1.406 | Acc: 49.658% (1017/2048)\n",
      "Loss: 1.397 | Acc: 49.763% (1051/2112)\n",
      "Loss: 1.402 | Acc: 49.540% (1078/2176)\n",
      "Loss: 1.401 | Acc: 49.598% (1111/2240)\n",
      "Loss: 1.400 | Acc: 49.609% (1143/2304)\n",
      "Loss: 1.400 | Acc: 49.578% (1174/2368)\n",
      "Loss: 1.403 | Acc: 49.548% (1205/2432)\n",
      "Loss: 1.401 | Acc: 49.559% (1237/2496)\n",
      "Loss: 1.408 | Acc: 49.414% (1265/2560)\n",
      "Loss: 1.408 | Acc: 49.314% (1294/2624)\n",
      "Loss: 1.403 | Acc: 49.479% (1330/2688)\n",
      "Loss: 1.407 | Acc: 49.164% (1353/2752)\n",
      "Loss: 1.407 | Acc: 49.183% (1385/2816)\n",
      "Loss: 1.404 | Acc: 49.271% (1419/2880)\n",
      "Loss: 1.400 | Acc: 49.592% (1460/2944)\n",
      "Loss: 1.398 | Acc: 49.668% (1494/3008)\n",
      "Loss: 1.400 | Acc: 49.642% (1525/3072)\n",
      "Loss: 1.398 | Acc: 49.617% (1556/3136)\n",
      "Loss: 1.396 | Acc: 49.688% (1590/3200)\n",
      "Loss: 1.398 | Acc: 49.632% (1620/3264)\n",
      "Loss: 1.400 | Acc: 49.549% (1649/3328)\n",
      "Loss: 1.399 | Acc: 49.587% (1682/3392)\n",
      "Loss: 1.399 | Acc: 49.566% (1713/3456)\n",
      "Loss: 1.400 | Acc: 49.517% (1743/3520)\n",
      "Loss: 1.397 | Acc: 49.554% (1776/3584)\n",
      "Loss: 1.399 | Acc: 49.479% (1805/3648)\n",
      "Loss: 1.397 | Acc: 49.488% (1837/3712)\n",
      "Loss: 1.398 | Acc: 49.523% (1870/3776)\n",
      "Loss: 1.395 | Acc: 49.583% (1904/3840)\n",
      "Loss: 1.395 | Acc: 49.641% (1938/3904)\n",
      "Loss: 1.397 | Acc: 49.546% (1966/3968)\n",
      "Loss: 1.400 | Acc: 49.430% (1993/4032)\n",
      "Loss: 1.403 | Acc: 49.365% (2022/4096)\n",
      "Loss: 1.407 | Acc: 49.303% (2051/4160)\n",
      "Loss: 1.404 | Acc: 49.455% (2089/4224)\n",
      "Loss: 1.405 | Acc: 49.417% (2119/4288)\n",
      "Loss: 1.404 | Acc: 49.472% (2153/4352)\n",
      "Loss: 1.399 | Acc: 49.751% (2197/4416)\n",
      "Loss: 1.396 | Acc: 49.799% (2231/4480)\n",
      "Loss: 1.396 | Acc: 49.824% (2264/4544)\n",
      "Loss: 1.397 | Acc: 49.674% (2289/4608)\n",
      "Loss: 1.393 | Acc: 49.807% (2327/4672)\n",
      "Loss: 1.392 | Acc: 49.747% (2356/4736)\n",
      "Loss: 1.394 | Acc: 49.667% (2384/4800)\n",
      "Loss: 1.391 | Acc: 49.733% (2419/4864)\n",
      "Loss: 1.389 | Acc: 49.756% (2452/4928)\n",
      "Loss: 1.388 | Acc: 49.760% (2484/4992)\n",
      "Loss: 1.387 | Acc: 49.862% (2521/5056)\n",
      "Loss: 1.388 | Acc: 49.766% (2548/5120)\n",
      "Loss: 1.387 | Acc: 49.846% (2584/5184)\n",
      "Loss: 1.387 | Acc: 49.867% (2617/5248)\n",
      "Loss: 1.385 | Acc: 49.944% (2653/5312)\n",
      "Loss: 1.388 | Acc: 49.888% (2682/5376)\n",
      "Loss: 1.387 | Acc: 49.871% (2713/5440)\n",
      "Loss: 1.387 | Acc: 49.891% (2746/5504)\n",
      "Loss: 1.389 | Acc: 49.767% (2771/5568)\n",
      "Loss: 1.391 | Acc: 49.751% (2802/5632)\n",
      "Loss: 1.392 | Acc: 49.789% (2836/5696)\n",
      "Loss: 1.389 | Acc: 49.757% (2866/5760)\n",
      "Loss: 1.387 | Acc: 49.828% (2902/5824)\n",
      "Loss: 1.389 | Acc: 49.762% (2930/5888)\n",
      "Loss: 1.390 | Acc: 49.748% (2961/5952)\n",
      "Loss: 1.390 | Acc: 49.784% (2995/6016)\n",
      "Loss: 1.391 | Acc: 49.737% (3024/6080)\n",
      "Loss: 1.391 | Acc: 49.658% (3051/6144)\n",
      "Loss: 1.390 | Acc: 49.630% (3081/6208)\n",
      "Loss: 1.392 | Acc: 49.554% (3108/6272)\n",
      "Loss: 1.391 | Acc: 49.605% (3143/6336)\n",
      "Loss: 1.392 | Acc: 49.500% (3168/6400)\n",
      "Loss: 1.393 | Acc: 49.459% (3197/6464)\n",
      "Loss: 1.393 | Acc: 49.494% (3231/6528)\n",
      "Loss: 1.395 | Acc: 49.378% (3255/6592)\n",
      "Loss: 1.395 | Acc: 49.384% (3287/6656)\n",
      "Loss: 1.395 | Acc: 49.330% (3315/6720)\n",
      "Loss: 1.394 | Acc: 49.366% (3349/6784)\n",
      "Loss: 1.391 | Acc: 49.489% (3389/6848)\n",
      "Loss: 1.394 | Acc: 49.421% (3416/6912)\n",
      "Loss: 1.395 | Acc: 49.412% (3447/6976)\n",
      "Loss: 1.396 | Acc: 49.290% (3470/7040)\n",
      "Loss: 1.395 | Acc: 49.352% (3506/7104)\n",
      "Loss: 1.394 | Acc: 49.386% (3540/7168)\n",
      "Loss: 1.395 | Acc: 49.295% (3565/7232)\n",
      "Loss: 1.394 | Acc: 49.287% (3596/7296)\n",
      "Loss: 1.392 | Acc: 49.348% (3632/7360)\n",
      "Loss: 1.392 | Acc: 49.340% (3663/7424)\n",
      "Loss: 1.389 | Acc: 49.399% (3699/7488)\n",
      "Loss: 1.388 | Acc: 49.444% (3734/7552)\n",
      "Loss: 1.389 | Acc: 49.422% (3764/7616)\n",
      "Loss: 1.389 | Acc: 49.440% (3797/7680)\n",
      "Loss: 1.386 | Acc: 49.535% (3836/7744)\n",
      "Loss: 1.385 | Acc: 49.629% (3875/7808)\n",
      "Loss: 1.386 | Acc: 49.568% (3902/7872)\n",
      "Loss: 1.385 | Acc: 49.597% (3936/7936)\n",
      "Loss: 1.386 | Acc: 49.625% (3970/8000)\n",
      "Loss: 1.385 | Acc: 49.640% (4003/8064)\n",
      "Loss: 1.386 | Acc: 49.692% (4039/8128)\n",
      "Loss: 1.384 | Acc: 49.695% (4071/8192)\n",
      "Loss: 1.385 | Acc: 49.600% (4095/8256)\n",
      "Loss: 1.387 | Acc: 49.459% (4115/8320)\n",
      "Loss: 1.387 | Acc: 49.463% (4147/8384)\n",
      "Loss: 1.387 | Acc: 49.420% (4175/8448)\n",
      "Loss: 1.388 | Acc: 49.401% (4205/8512)\n",
      "Loss: 1.388 | Acc: 49.382% (4235/8576)\n",
      "Loss: 1.388 | Acc: 49.375% (4266/8640)\n",
      "Loss: 1.387 | Acc: 49.368% (4297/8704)\n",
      "Loss: 1.387 | Acc: 49.361% (4328/8768)\n",
      "Loss: 1.386 | Acc: 49.457% (4368/8832)\n",
      "Loss: 1.385 | Acc: 49.460% (4400/8896)\n",
      "Loss: 1.384 | Acc: 49.498% (4435/8960)\n",
      "Loss: 1.384 | Acc: 49.490% (4466/9024)\n",
      "Loss: 1.385 | Acc: 49.417% (4491/9088)\n",
      "Loss: 1.385 | Acc: 49.421% (4523/9152)\n",
      "Loss: 1.384 | Acc: 49.490% (4561/9216)\n",
      "Loss: 1.383 | Acc: 49.515% (4595/9280)\n",
      "Loss: 1.384 | Acc: 49.454% (4621/9344)\n",
      "Loss: 1.384 | Acc: 49.458% (4653/9408)\n",
      "Loss: 1.385 | Acc: 49.440% (4683/9472)\n",
      "Loss: 1.383 | Acc: 49.486% (4719/9536)\n",
      "Loss: 1.382 | Acc: 49.552% (4757/9600)\n",
      "Loss: 1.383 | Acc: 49.514% (4785/9664)\n",
      "Loss: 1.382 | Acc: 49.507% (4816/9728)\n",
      "Loss: 1.383 | Acc: 49.500% (4847/9792)\n",
      "Loss: 1.383 | Acc: 49.513% (4880/9856)\n",
      "Loss: 1.384 | Acc: 49.466% (4907/9920)\n",
      "Loss: 1.385 | Acc: 49.419% (4934/9984)\n",
      "Loss: 1.385 | Acc: 49.410% (4941/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 49.41\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.115 | Acc: 50.000% (32/64)\n",
      "Loss: 1.181 | Acc: 54.688% (70/128)\n",
      "Loss: 1.213 | Acc: 53.646% (103/192)\n",
      "Loss: 1.196 | Acc: 53.906% (138/256)\n",
      "Loss: 1.208 | Acc: 52.500% (168/320)\n",
      "Loss: 1.256 | Acc: 51.302% (197/384)\n",
      "Loss: 1.248 | Acc: 51.339% (230/448)\n",
      "Loss: 1.267 | Acc: 50.977% (261/512)\n",
      "Loss: 1.278 | Acc: 50.694% (292/576)\n",
      "Loss: 1.257 | Acc: 52.031% (333/640)\n",
      "Loss: 1.266 | Acc: 51.420% (362/704)\n",
      "Loss: 1.258 | Acc: 52.083% (400/768)\n",
      "Loss: 1.282 | Acc: 51.442% (428/832)\n",
      "Loss: 1.291 | Acc: 51.339% (460/896)\n",
      "Loss: 1.298 | Acc: 51.250% (492/960)\n",
      "Loss: 1.300 | Acc: 51.270% (525/1024)\n",
      "Loss: 1.305 | Acc: 51.195% (557/1088)\n",
      "Loss: 1.292 | Acc: 51.823% (597/1152)\n",
      "Loss: 1.292 | Acc: 52.056% (633/1216)\n",
      "Loss: 1.292 | Acc: 52.109% (667/1280)\n",
      "Loss: 1.294 | Acc: 51.786% (696/1344)\n",
      "Loss: 1.294 | Acc: 51.918% (731/1408)\n",
      "Loss: 1.284 | Acc: 52.310% (770/1472)\n",
      "Loss: 1.279 | Acc: 52.474% (806/1536)\n",
      "Loss: 1.300 | Acc: 51.500% (824/1600)\n",
      "Loss: 1.290 | Acc: 51.803% (862/1664)\n",
      "Loss: 1.297 | Acc: 51.620% (892/1728)\n",
      "Loss: 1.306 | Acc: 51.228% (918/1792)\n",
      "Loss: 1.309 | Acc: 51.131% (949/1856)\n",
      "Loss: 1.309 | Acc: 51.146% (982/1920)\n",
      "Loss: 1.310 | Acc: 50.958% (1011/1984)\n",
      "Loss: 1.313 | Acc: 51.172% (1048/2048)\n",
      "Loss: 1.312 | Acc: 51.326% (1084/2112)\n",
      "Loss: 1.314 | Acc: 51.287% (1116/2176)\n",
      "Loss: 1.312 | Acc: 51.250% (1148/2240)\n",
      "Loss: 1.314 | Acc: 51.302% (1182/2304)\n",
      "Loss: 1.312 | Acc: 51.267% (1214/2368)\n",
      "Loss: 1.313 | Acc: 51.398% (1250/2432)\n",
      "Loss: 1.315 | Acc: 51.242% (1279/2496)\n",
      "Loss: 1.314 | Acc: 51.289% (1313/2560)\n",
      "Loss: 1.313 | Acc: 51.334% (1347/2624)\n",
      "Loss: 1.319 | Acc: 51.116% (1374/2688)\n",
      "Loss: 1.322 | Acc: 50.872% (1400/2752)\n",
      "Loss: 1.323 | Acc: 50.888% (1433/2816)\n",
      "Loss: 1.325 | Acc: 50.833% (1464/2880)\n",
      "Loss: 1.326 | Acc: 50.713% (1493/2944)\n",
      "Loss: 1.326 | Acc: 50.798% (1528/3008)\n",
      "Loss: 1.327 | Acc: 50.716% (1558/3072)\n",
      "Loss: 1.328 | Acc: 50.702% (1590/3136)\n",
      "Loss: 1.329 | Acc: 50.656% (1621/3200)\n",
      "Loss: 1.331 | Acc: 50.674% (1654/3264)\n",
      "Loss: 1.331 | Acc: 50.781% (1690/3328)\n",
      "Loss: 1.332 | Acc: 50.737% (1721/3392)\n",
      "Loss: 1.332 | Acc: 50.752% (1754/3456)\n",
      "Loss: 1.337 | Acc: 50.511% (1778/3520)\n",
      "Loss: 1.334 | Acc: 50.670% (1816/3584)\n",
      "Loss: 1.335 | Acc: 50.603% (1846/3648)\n",
      "Loss: 1.334 | Acc: 50.673% (1881/3712)\n",
      "Loss: 1.332 | Acc: 50.821% (1919/3776)\n",
      "Loss: 1.334 | Acc: 50.859% (1953/3840)\n",
      "Loss: 1.337 | Acc: 50.820% (1984/3904)\n",
      "Loss: 1.338 | Acc: 50.832% (2017/3968)\n",
      "Loss: 1.340 | Acc: 50.843% (2050/4032)\n",
      "Loss: 1.342 | Acc: 50.781% (2080/4096)\n",
      "Loss: 1.342 | Acc: 50.817% (2114/4160)\n",
      "Loss: 1.340 | Acc: 50.805% (2146/4224)\n",
      "Loss: 1.338 | Acc: 50.910% (2183/4288)\n",
      "Loss: 1.338 | Acc: 50.873% (2214/4352)\n",
      "Loss: 1.339 | Acc: 50.928% (2249/4416)\n",
      "Loss: 1.341 | Acc: 50.960% (2283/4480)\n",
      "Loss: 1.339 | Acc: 51.012% (2318/4544)\n",
      "Loss: 1.339 | Acc: 50.977% (2349/4608)\n",
      "Loss: 1.340 | Acc: 51.027% (2384/4672)\n",
      "Loss: 1.343 | Acc: 50.971% (2414/4736)\n",
      "Loss: 1.344 | Acc: 50.917% (2444/4800)\n",
      "Loss: 1.345 | Acc: 50.905% (2476/4864)\n",
      "Loss: 1.344 | Acc: 50.954% (2511/4928)\n",
      "Loss: 1.345 | Acc: 50.962% (2544/4992)\n",
      "Loss: 1.341 | Acc: 51.028% (2580/5056)\n",
      "Loss: 1.340 | Acc: 51.172% (2620/5120)\n",
      "Loss: 1.340 | Acc: 51.254% (2657/5184)\n",
      "Loss: 1.339 | Acc: 51.353% (2695/5248)\n",
      "Loss: 1.338 | Acc: 51.374% (2729/5312)\n",
      "Loss: 1.339 | Acc: 51.358% (2761/5376)\n",
      "Loss: 1.339 | Acc: 51.324% (2792/5440)\n",
      "Loss: 1.339 | Acc: 51.308% (2824/5504)\n",
      "Loss: 1.340 | Acc: 51.347% (2859/5568)\n",
      "Loss: 1.337 | Acc: 51.385% (2894/5632)\n",
      "Loss: 1.339 | Acc: 51.404% (2928/5696)\n",
      "Loss: 1.342 | Acc: 51.319% (2956/5760)\n",
      "Loss: 1.344 | Acc: 51.236% (2984/5824)\n",
      "Loss: 1.342 | Acc: 51.376% (3025/5888)\n",
      "Loss: 1.344 | Acc: 51.310% (3054/5952)\n",
      "Loss: 1.344 | Acc: 51.297% (3086/6016)\n",
      "Loss: 1.345 | Acc: 51.283% (3118/6080)\n",
      "Loss: 1.346 | Acc: 51.204% (3146/6144)\n",
      "Loss: 1.349 | Acc: 51.208% (3179/6208)\n",
      "Loss: 1.349 | Acc: 51.180% (3210/6272)\n",
      "Loss: 1.350 | Acc: 51.105% (3238/6336)\n",
      "Loss: 1.349 | Acc: 51.062% (3268/6400)\n",
      "Loss: 1.348 | Acc: 51.067% (3301/6464)\n",
      "Loss: 1.348 | Acc: 51.072% (3334/6528)\n",
      "Loss: 1.350 | Acc: 50.986% (3361/6592)\n",
      "Loss: 1.353 | Acc: 50.931% (3390/6656)\n",
      "Loss: 1.354 | Acc: 50.848% (3417/6720)\n",
      "Loss: 1.355 | Acc: 50.840% (3449/6784)\n",
      "Loss: 1.355 | Acc: 50.789% (3478/6848)\n",
      "Loss: 1.355 | Acc: 50.767% (3509/6912)\n",
      "Loss: 1.358 | Acc: 50.659% (3534/6976)\n",
      "Loss: 1.358 | Acc: 50.625% (3564/7040)\n",
      "Loss: 1.357 | Acc: 50.676% (3600/7104)\n",
      "Loss: 1.359 | Acc: 50.642% (3630/7168)\n",
      "Loss: 1.358 | Acc: 50.622% (3661/7232)\n",
      "Loss: 1.358 | Acc: 50.589% (3691/7296)\n",
      "Loss: 1.360 | Acc: 50.503% (3717/7360)\n",
      "Loss: 1.359 | Acc: 50.552% (3753/7424)\n",
      "Loss: 1.360 | Acc: 50.534% (3784/7488)\n",
      "Loss: 1.359 | Acc: 50.516% (3815/7552)\n",
      "Loss: 1.361 | Acc: 50.460% (3843/7616)\n",
      "Loss: 1.361 | Acc: 50.456% (3875/7680)\n",
      "Loss: 1.361 | Acc: 50.439% (3906/7744)\n",
      "Loss: 1.361 | Acc: 50.487% (3942/7808)\n",
      "Loss: 1.360 | Acc: 50.534% (3978/7872)\n",
      "Loss: 1.361 | Acc: 50.479% (4006/7936)\n",
      "Loss: 1.362 | Acc: 50.475% (4038/8000)\n",
      "Loss: 1.362 | Acc: 50.459% (4069/8064)\n",
      "Loss: 1.363 | Acc: 50.480% (4103/8128)\n",
      "Loss: 1.364 | Acc: 50.488% (4136/8192)\n",
      "Loss: 1.365 | Acc: 50.472% (4167/8256)\n",
      "Loss: 1.366 | Acc: 50.481% (4200/8320)\n",
      "Loss: 1.365 | Acc: 50.549% (4238/8384)\n",
      "Loss: 1.367 | Acc: 50.521% (4268/8448)\n",
      "Loss: 1.367 | Acc: 50.552% (4303/8512)\n",
      "Loss: 1.368 | Acc: 50.583% (4338/8576)\n",
      "Loss: 1.369 | Acc: 50.567% (4369/8640)\n",
      "Loss: 1.369 | Acc: 50.528% (4398/8704)\n",
      "Loss: 1.369 | Acc: 50.513% (4429/8768)\n",
      "Loss: 1.371 | Acc: 50.453% (4456/8832)\n",
      "Loss: 1.371 | Acc: 50.472% (4490/8896)\n",
      "Loss: 1.370 | Acc: 50.446% (4520/8960)\n",
      "Loss: 1.369 | Acc: 50.454% (4553/9024)\n",
      "Loss: 1.368 | Acc: 50.473% (4587/9088)\n",
      "Loss: 1.368 | Acc: 50.470% (4619/9152)\n",
      "Loss: 1.367 | Acc: 50.532% (4657/9216)\n",
      "Loss: 1.368 | Acc: 50.517% (4688/9280)\n",
      "Loss: 1.368 | Acc: 50.503% (4719/9344)\n",
      "Loss: 1.368 | Acc: 50.489% (4750/9408)\n",
      "Loss: 1.367 | Acc: 50.507% (4784/9472)\n",
      "Loss: 1.369 | Acc: 50.419% (4808/9536)\n",
      "Loss: 1.369 | Acc: 50.458% (4844/9600)\n",
      "Loss: 1.369 | Acc: 50.497% (4880/9664)\n",
      "Loss: 1.370 | Acc: 50.473% (4910/9728)\n",
      "Loss: 1.370 | Acc: 50.500% (4945/9792)\n",
      "Loss: 1.370 | Acc: 50.528% (4980/9856)\n",
      "Loss: 1.371 | Acc: 50.464% (5006/9920)\n",
      "Loss: 1.371 | Acc: 50.501% (5042/9984)\n",
      "Loss: 1.370 | Acc: 50.488% (5073/10048)\n",
      "Loss: 1.370 | Acc: 50.494% (5106/10112)\n",
      "Loss: 1.369 | Acc: 50.560% (5145/10176)\n",
      "Loss: 1.369 | Acc: 50.596% (5181/10240)\n",
      "Loss: 1.368 | Acc: 50.621% (5216/10304)\n",
      "Loss: 1.368 | Acc: 50.656% (5252/10368)\n",
      "Loss: 1.368 | Acc: 50.652% (5284/10432)\n",
      "Loss: 1.368 | Acc: 50.657% (5317/10496)\n",
      "Loss: 1.370 | Acc: 50.616% (5345/10560)\n",
      "Loss: 1.370 | Acc: 50.612% (5377/10624)\n",
      "Loss: 1.370 | Acc: 50.608% (5409/10688)\n",
      "Loss: 1.369 | Acc: 50.623% (5443/10752)\n",
      "Loss: 1.370 | Acc: 50.601% (5473/10816)\n",
      "Loss: 1.370 | Acc: 50.570% (5502/10880)\n",
      "Loss: 1.370 | Acc: 50.576% (5535/10944)\n",
      "Loss: 1.369 | Acc: 50.581% (5568/11008)\n",
      "Loss: 1.370 | Acc: 50.533% (5595/11072)\n",
      "Loss: 1.369 | Acc: 50.593% (5634/11136)\n",
      "Loss: 1.368 | Acc: 50.616% (5669/11200)\n",
      "Loss: 1.368 | Acc: 50.613% (5701/11264)\n",
      "Loss: 1.369 | Acc: 50.583% (5730/11328)\n",
      "Loss: 1.368 | Acc: 50.597% (5764/11392)\n",
      "Loss: 1.369 | Acc: 50.576% (5794/11456)\n",
      "Loss: 1.368 | Acc: 50.573% (5826/11520)\n",
      "Loss: 1.369 | Acc: 50.587% (5860/11584)\n",
      "Loss: 1.368 | Acc: 50.610% (5895/11648)\n",
      "Loss: 1.369 | Acc: 50.572% (5923/11712)\n",
      "Loss: 1.368 | Acc: 50.611% (5960/11776)\n",
      "Loss: 1.368 | Acc: 50.608% (5992/11840)\n",
      "Loss: 1.368 | Acc: 50.588% (6022/11904)\n",
      "Loss: 1.368 | Acc: 50.577% (6053/11968)\n",
      "Loss: 1.369 | Acc: 50.532% (6080/12032)\n",
      "Loss: 1.371 | Acc: 50.496% (6108/12096)\n",
      "Loss: 1.369 | Acc: 50.559% (6148/12160)\n",
      "Loss: 1.369 | Acc: 50.548% (6179/12224)\n",
      "Loss: 1.368 | Acc: 50.553% (6212/12288)\n",
      "Loss: 1.368 | Acc: 50.559% (6245/12352)\n",
      "Loss: 1.368 | Acc: 50.604% (6283/12416)\n",
      "Loss: 1.367 | Acc: 50.641% (6320/12480)\n",
      "Loss: 1.368 | Acc: 50.598% (6347/12544)\n",
      "Loss: 1.369 | Acc: 50.571% (6376/12608)\n",
      "Loss: 1.369 | Acc: 50.568% (6408/12672)\n",
      "Loss: 1.370 | Acc: 50.550% (6438/12736)\n",
      "Loss: 1.369 | Acc: 50.547% (6470/12800)\n",
      "Loss: 1.369 | Acc: 50.513% (6498/12864)\n",
      "Loss: 1.369 | Acc: 50.526% (6532/12928)\n",
      "Loss: 1.368 | Acc: 50.508% (6562/12992)\n",
      "Loss: 1.367 | Acc: 50.544% (6599/13056)\n",
      "Loss: 1.367 | Acc: 50.587% (6637/13120)\n",
      "Loss: 1.366 | Acc: 50.584% (6669/13184)\n",
      "Loss: 1.366 | Acc: 50.589% (6702/13248)\n",
      "Loss: 1.366 | Acc: 50.571% (6732/13312)\n",
      "Loss: 1.366 | Acc: 50.598% (6768/13376)\n",
      "Loss: 1.366 | Acc: 50.543% (6793/13440)\n",
      "Loss: 1.366 | Acc: 50.585% (6831/13504)\n",
      "Loss: 1.365 | Acc: 50.582% (6863/13568)\n",
      "Loss: 1.365 | Acc: 50.565% (6893/13632)\n",
      "Loss: 1.365 | Acc: 50.584% (6928/13696)\n",
      "Loss: 1.363 | Acc: 50.632% (6967/13760)\n",
      "Loss: 1.364 | Acc: 50.593% (6994/13824)\n",
      "Loss: 1.363 | Acc: 50.612% (7029/13888)\n",
      "Loss: 1.363 | Acc: 50.595% (7059/13952)\n",
      "Loss: 1.362 | Acc: 50.592% (7091/14016)\n",
      "Loss: 1.361 | Acc: 50.625% (7128/14080)\n",
      "Loss: 1.361 | Acc: 50.650% (7164/14144)\n",
      "Loss: 1.361 | Acc: 50.683% (7201/14208)\n",
      "Loss: 1.360 | Acc: 50.708% (7237/14272)\n",
      "Loss: 1.359 | Acc: 50.739% (7274/14336)\n",
      "Loss: 1.359 | Acc: 50.715% (7303/14400)\n",
      "Loss: 1.359 | Acc: 50.740% (7339/14464)\n",
      "Loss: 1.359 | Acc: 50.702% (7366/14528)\n",
      "Loss: 1.358 | Acc: 50.726% (7402/14592)\n",
      "Loss: 1.358 | Acc: 50.730% (7435/14656)\n",
      "Loss: 1.359 | Acc: 50.713% (7465/14720)\n",
      "Loss: 1.358 | Acc: 50.697% (7495/14784)\n",
      "Loss: 1.358 | Acc: 50.714% (7530/14848)\n",
      "Loss: 1.358 | Acc: 50.751% (7568/14912)\n",
      "Loss: 1.357 | Acc: 50.775% (7604/14976)\n",
      "Loss: 1.357 | Acc: 50.791% (7639/15040)\n",
      "Loss: 1.357 | Acc: 50.788% (7671/15104)\n",
      "Loss: 1.356 | Acc: 50.804% (7706/15168)\n",
      "Loss: 1.357 | Acc: 50.821% (7741/15232)\n",
      "Loss: 1.356 | Acc: 50.856% (7779/15296)\n",
      "Loss: 1.357 | Acc: 50.853% (7811/15360)\n",
      "Loss: 1.356 | Acc: 50.869% (7846/15424)\n",
      "Loss: 1.356 | Acc: 50.878% (7880/15488)\n",
      "Loss: 1.357 | Acc: 50.862% (7910/15552)\n",
      "Loss: 1.357 | Acc: 50.852% (7941/15616)\n",
      "Loss: 1.356 | Acc: 50.861% (7975/15680)\n",
      "Loss: 1.356 | Acc: 50.851% (8006/15744)\n",
      "Loss: 1.355 | Acc: 50.860% (8040/15808)\n",
      "Loss: 1.356 | Acc: 50.838% (8069/15872)\n",
      "Loss: 1.355 | Acc: 50.835% (8101/15936)\n",
      "Loss: 1.355 | Acc: 50.825% (8132/16000)\n",
      "Loss: 1.356 | Acc: 50.809% (8162/16064)\n",
      "Loss: 1.356 | Acc: 50.812% (8195/16128)\n",
      "Loss: 1.356 | Acc: 50.797% (8225/16192)\n",
      "Loss: 1.357 | Acc: 50.775% (8254/16256)\n",
      "Loss: 1.356 | Acc: 50.821% (8294/16320)\n",
      "Loss: 1.355 | Acc: 50.848% (8331/16384)\n",
      "Loss: 1.355 | Acc: 50.839% (8362/16448)\n",
      "Loss: 1.355 | Acc: 50.842% (8395/16512)\n",
      "Loss: 1.355 | Acc: 50.857% (8430/16576)\n",
      "Loss: 1.356 | Acc: 50.823% (8457/16640)\n",
      "Loss: 1.357 | Acc: 50.814% (8488/16704)\n",
      "Loss: 1.357 | Acc: 50.787% (8516/16768)\n",
      "Loss: 1.358 | Acc: 50.796% (8550/16832)\n",
      "Loss: 1.357 | Acc: 50.835% (8589/16896)\n",
      "Loss: 1.357 | Acc: 50.855% (8625/16960)\n",
      "Loss: 1.357 | Acc: 50.875% (8661/17024)\n",
      "Loss: 1.358 | Acc: 50.878% (8694/17088)\n",
      "Loss: 1.359 | Acc: 50.828% (8718/17152)\n",
      "Loss: 1.359 | Acc: 50.825% (8750/17216)\n",
      "Loss: 1.359 | Acc: 50.822% (8782/17280)\n",
      "Loss: 1.359 | Acc: 50.773% (8806/17344)\n",
      "Loss: 1.358 | Acc: 50.798% (8843/17408)\n",
      "Loss: 1.359 | Acc: 50.801% (8876/17472)\n",
      "Loss: 1.360 | Acc: 50.776% (8904/17536)\n",
      "Loss: 1.359 | Acc: 50.807% (8942/17600)\n",
      "Loss: 1.359 | Acc: 50.787% (8971/17664)\n",
      "Loss: 1.359 | Acc: 50.790% (9004/17728)\n",
      "Loss: 1.359 | Acc: 50.798% (9038/17792)\n",
      "Loss: 1.359 | Acc: 50.795% (9070/17856)\n",
      "Loss: 1.359 | Acc: 50.792% (9102/17920)\n",
      "Loss: 1.360 | Acc: 50.756% (9128/17984)\n",
      "Loss: 1.361 | Acc: 50.737% (9157/18048)\n",
      "Loss: 1.360 | Acc: 50.707% (9184/18112)\n",
      "Loss: 1.360 | Acc: 50.743% (9223/18176)\n",
      "Loss: 1.361 | Acc: 50.718% (9251/18240)\n",
      "Loss: 1.360 | Acc: 50.738% (9287/18304)\n",
      "Loss: 1.359 | Acc: 50.762% (9324/18368)\n",
      "Loss: 1.359 | Acc: 50.754% (9355/18432)\n",
      "Loss: 1.360 | Acc: 50.730% (9383/18496)\n",
      "Loss: 1.360 | Acc: 50.722% (9414/18560)\n",
      "Loss: 1.361 | Acc: 50.698% (9442/18624)\n",
      "Loss: 1.360 | Acc: 50.706% (9476/18688)\n",
      "Loss: 1.360 | Acc: 50.747% (9516/18752)\n",
      "Loss: 1.360 | Acc: 50.755% (9550/18816)\n",
      "Loss: 1.360 | Acc: 50.768% (9585/18880)\n",
      "Loss: 1.360 | Acc: 50.771% (9618/18944)\n",
      "Loss: 1.360 | Acc: 50.758% (9648/19008)\n",
      "Loss: 1.360 | Acc: 50.771% (9683/19072)\n",
      "Loss: 1.359 | Acc: 50.758% (9713/19136)\n",
      "Loss: 1.360 | Acc: 50.781% (9750/19200)\n",
      "Loss: 1.360 | Acc: 50.737% (9774/19264)\n",
      "Loss: 1.360 | Acc: 50.740% (9807/19328)\n",
      "Loss: 1.360 | Acc: 50.727% (9837/19392)\n",
      "Loss: 1.360 | Acc: 50.725% (9869/19456)\n",
      "Loss: 1.359 | Acc: 50.748% (9906/19520)\n",
      "Loss: 1.359 | Acc: 50.756% (9940/19584)\n",
      "Loss: 1.359 | Acc: 50.707% (9963/19648)\n",
      "Loss: 1.360 | Acc: 50.715% (9997/19712)\n",
      "Loss: 1.359 | Acc: 50.733% (10033/19776)\n",
      "Loss: 1.360 | Acc: 50.736% (10066/19840)\n",
      "Loss: 1.360 | Acc: 50.749% (10101/19904)\n",
      "Loss: 1.359 | Acc: 50.761% (10136/19968)\n",
      "Loss: 1.359 | Acc: 50.744% (10165/20032)\n",
      "Loss: 1.359 | Acc: 50.731% (10195/20096)\n",
      "Loss: 1.359 | Acc: 50.739% (10229/20160)\n",
      "Loss: 1.359 | Acc: 50.702% (10254/20224)\n",
      "Loss: 1.360 | Acc: 50.700% (10286/20288)\n",
      "Loss: 1.360 | Acc: 50.712% (10321/20352)\n",
      "Loss: 1.360 | Acc: 50.705% (10352/20416)\n",
      "Loss: 1.359 | Acc: 50.728% (10389/20480)\n",
      "Loss: 1.360 | Acc: 50.711% (10418/20544)\n",
      "Loss: 1.360 | Acc: 50.699% (10448/20608)\n",
      "Loss: 1.359 | Acc: 50.706% (10482/20672)\n",
      "Loss: 1.359 | Acc: 50.694% (10512/20736)\n",
      "Loss: 1.358 | Acc: 50.721% (10550/20800)\n",
      "Loss: 1.358 | Acc: 50.738% (10586/20864)\n",
      "Loss: 1.357 | Acc: 50.784% (10628/20928)\n",
      "Loss: 1.357 | Acc: 50.810% (10666/20992)\n",
      "Loss: 1.356 | Acc: 50.879% (10713/21056)\n",
      "Loss: 1.356 | Acc: 50.838% (10737/21120)\n",
      "Loss: 1.356 | Acc: 50.850% (10772/21184)\n",
      "Loss: 1.356 | Acc: 50.866% (10808/21248)\n",
      "Loss: 1.356 | Acc: 50.849% (10837/21312)\n",
      "Loss: 1.355 | Acc: 50.847% (10869/21376)\n",
      "Loss: 1.356 | Acc: 50.844% (10901/21440)\n",
      "Loss: 1.355 | Acc: 50.870% (10939/21504)\n",
      "Loss: 1.355 | Acc: 50.867% (10971/21568)\n",
      "Loss: 1.355 | Acc: 50.860% (11002/21632)\n",
      "Loss: 1.355 | Acc: 50.867% (11036/21696)\n",
      "Loss: 1.355 | Acc: 50.873% (11070/21760)\n",
      "Loss: 1.355 | Acc: 50.843% (11096/21824)\n",
      "Loss: 1.355 | Acc: 50.845% (11129/21888)\n",
      "Loss: 1.355 | Acc: 50.847% (11162/21952)\n",
      "Loss: 1.354 | Acc: 50.881% (11202/22016)\n",
      "Loss: 1.354 | Acc: 50.901% (11239/22080)\n",
      "Loss: 1.353 | Acc: 50.926% (11277/22144)\n",
      "Loss: 1.353 | Acc: 50.923% (11309/22208)\n",
      "Loss: 1.353 | Acc: 50.934% (11344/22272)\n",
      "Loss: 1.352 | Acc: 50.949% (11380/22336)\n",
      "Loss: 1.352 | Acc: 50.960% (11415/22400)\n",
      "Loss: 1.352 | Acc: 50.962% (11448/22464)\n",
      "Loss: 1.352 | Acc: 50.972% (11483/22528)\n",
      "Loss: 1.351 | Acc: 50.996% (11521/22592)\n",
      "Loss: 1.351 | Acc: 51.024% (11560/22656)\n",
      "Loss: 1.350 | Acc: 51.056% (11600/22720)\n",
      "Loss: 1.350 | Acc: 51.049% (11631/22784)\n",
      "Loss: 1.350 | Acc: 51.064% (11667/22848)\n",
      "Loss: 1.350 | Acc: 51.069% (11701/22912)\n",
      "Loss: 1.350 | Acc: 51.079% (11736/22976)\n",
      "Loss: 1.350 | Acc: 51.055% (11763/23040)\n",
      "Loss: 1.350 | Acc: 51.065% (11798/23104)\n",
      "Loss: 1.350 | Acc: 51.057% (11829/23168)\n",
      "Loss: 1.351 | Acc: 51.033% (11856/23232)\n",
      "Loss: 1.351 | Acc: 51.030% (11888/23296)\n",
      "Loss: 1.351 | Acc: 51.045% (11924/23360)\n",
      "Loss: 1.351 | Acc: 51.042% (11956/23424)\n",
      "Loss: 1.351 | Acc: 51.035% (11987/23488)\n",
      "Loss: 1.351 | Acc: 51.028% (12018/23552)\n",
      "Loss: 1.351 | Acc: 50.999% (12044/23616)\n",
      "Loss: 1.352 | Acc: 50.980% (12072/23680)\n",
      "Loss: 1.352 | Acc: 50.956% (12099/23744)\n",
      "Loss: 1.351 | Acc: 51.004% (12143/23808)\n",
      "Loss: 1.351 | Acc: 51.014% (12178/23872)\n",
      "Loss: 1.351 | Acc: 50.998% (12207/23936)\n",
      "Loss: 1.351 | Acc: 51.004% (12241/24000)\n",
      "Loss: 1.351 | Acc: 51.001% (12273/24064)\n",
      "Loss: 1.351 | Acc: 51.003% (12306/24128)\n",
      "Loss: 1.351 | Acc: 50.996% (12337/24192)\n",
      "Loss: 1.351 | Acc: 51.006% (12372/24256)\n",
      "Loss: 1.351 | Acc: 50.991% (12401/24320)\n",
      "Loss: 1.351 | Acc: 50.988% (12433/24384)\n",
      "Loss: 1.351 | Acc: 50.969% (12461/24448)\n",
      "Loss: 1.351 | Acc: 50.942% (12487/24512)\n",
      "Loss: 1.351 | Acc: 50.981% (12529/24576)\n",
      "Loss: 1.351 | Acc: 50.982% (12562/24640)\n",
      "Loss: 1.351 | Acc: 50.967% (12591/24704)\n",
      "Loss: 1.351 | Acc: 50.981% (12627/24768)\n",
      "Loss: 1.351 | Acc: 50.966% (12656/24832)\n",
      "Loss: 1.351 | Acc: 50.976% (12691/24896)\n",
      "Loss: 1.351 | Acc: 50.994% (12728/24960)\n",
      "Loss: 1.350 | Acc: 51.031% (12770/25024)\n",
      "Loss: 1.351 | Acc: 51.036% (12804/25088)\n",
      "Loss: 1.351 | Acc: 51.022% (12833/25152)\n",
      "Loss: 1.351 | Acc: 51.007% (12862/25216)\n",
      "Loss: 1.351 | Acc: 51.017% (12897/25280)\n",
      "Loss: 1.351 | Acc: 51.010% (12928/25344)\n",
      "Loss: 1.351 | Acc: 51.031% (12966/25408)\n",
      "Loss: 1.351 | Acc: 51.048% (13003/25472)\n",
      "Loss: 1.351 | Acc: 51.061% (13039/25536)\n",
      "Loss: 1.351 | Acc: 51.047% (13068/25600)\n",
      "Loss: 1.350 | Acc: 51.044% (13100/25664)\n",
      "Loss: 1.351 | Acc: 51.034% (13130/25728)\n",
      "Loss: 1.351 | Acc: 51.047% (13166/25792)\n",
      "Loss: 1.350 | Acc: 51.056% (13201/25856)\n",
      "Loss: 1.351 | Acc: 51.053% (13233/25920)\n",
      "Loss: 1.351 | Acc: 51.081% (13273/25984)\n",
      "Loss: 1.350 | Acc: 51.071% (13303/26048)\n",
      "Loss: 1.349 | Acc: 51.095% (13342/26112)\n",
      "Loss: 1.349 | Acc: 51.104% (13377/26176)\n",
      "Loss: 1.349 | Acc: 51.128% (13416/26240)\n",
      "Loss: 1.349 | Acc: 51.106% (13443/26304)\n",
      "Loss: 1.349 | Acc: 51.123% (13480/26368)\n",
      "Loss: 1.349 | Acc: 51.120% (13512/26432)\n",
      "Loss: 1.349 | Acc: 51.117% (13544/26496)\n",
      "Loss: 1.349 | Acc: 51.118% (13577/26560)\n",
      "Loss: 1.349 | Acc: 51.131% (13613/26624)\n",
      "Loss: 1.349 | Acc: 51.143% (13649/26688)\n",
      "Loss: 1.349 | Acc: 51.110% (13673/26752)\n",
      "Loss: 1.349 | Acc: 51.122% (13709/26816)\n",
      "Loss: 1.349 | Acc: 51.127% (13743/26880)\n",
      "Loss: 1.349 | Acc: 51.125% (13775/26944)\n",
      "Loss: 1.349 | Acc: 51.126% (13808/27008)\n",
      "Loss: 1.349 | Acc: 51.138% (13844/27072)\n",
      "Loss: 1.349 | Acc: 51.157% (13882/27136)\n",
      "Loss: 1.349 | Acc: 51.136% (13909/27200)\n",
      "Loss: 1.349 | Acc: 51.152% (13946/27264)\n",
      "Loss: 1.349 | Acc: 51.164% (13982/27328)\n",
      "Loss: 1.348 | Acc: 51.186% (14021/27392)\n",
      "Loss: 1.348 | Acc: 51.180% (14052/27456)\n",
      "Loss: 1.348 | Acc: 51.181% (14085/27520)\n",
      "Loss: 1.348 | Acc: 51.182% (14118/27584)\n",
      "Loss: 1.348 | Acc: 51.183% (14151/27648)\n",
      "Loss: 1.348 | Acc: 51.180% (14183/27712)\n",
      "Loss: 1.347 | Acc: 51.195% (14220/27776)\n",
      "Loss: 1.347 | Acc: 51.182% (14249/27840)\n",
      "Loss: 1.348 | Acc: 51.175% (14280/27904)\n",
      "Loss: 1.347 | Acc: 51.183% (14315/27968)\n",
      "Loss: 1.347 | Acc: 51.184% (14348/28032)\n",
      "Loss: 1.347 | Acc: 51.192% (14383/28096)\n",
      "Loss: 1.347 | Acc: 51.186% (14414/28160)\n",
      "Loss: 1.347 | Acc: 51.183% (14446/28224)\n",
      "Loss: 1.348 | Acc: 51.181% (14478/28288)\n",
      "Loss: 1.348 | Acc: 51.182% (14511/28352)\n",
      "Loss: 1.347 | Acc: 51.186% (14545/28416)\n",
      "Loss: 1.347 | Acc: 51.187% (14578/28480)\n",
      "Loss: 1.347 | Acc: 51.195% (14613/28544)\n",
      "Loss: 1.347 | Acc: 51.209% (14650/28608)\n",
      "Loss: 1.347 | Acc: 51.207% (14682/28672)\n",
      "Loss: 1.347 | Acc: 51.211% (14716/28736)\n",
      "Loss: 1.347 | Acc: 51.222% (14752/28800)\n",
      "Loss: 1.347 | Acc: 51.216% (14783/28864)\n",
      "Loss: 1.347 | Acc: 51.224% (14818/28928)\n",
      "Loss: 1.347 | Acc: 51.214% (14848/28992)\n",
      "Loss: 1.347 | Acc: 51.222% (14883/29056)\n",
      "Loss: 1.346 | Acc: 51.233% (14919/29120)\n",
      "Loss: 1.346 | Acc: 51.230% (14951/29184)\n",
      "Loss: 1.346 | Acc: 51.248% (14989/29248)\n",
      "Loss: 1.346 | Acc: 51.252% (15023/29312)\n",
      "Loss: 1.345 | Acc: 51.270% (15061/29376)\n",
      "Loss: 1.345 | Acc: 51.270% (15094/29440)\n",
      "Loss: 1.345 | Acc: 51.264% (15125/29504)\n",
      "Loss: 1.345 | Acc: 51.289% (15165/29568)\n",
      "Loss: 1.344 | Acc: 51.303% (15202/29632)\n",
      "Loss: 1.344 | Acc: 51.313% (15238/29696)\n",
      "Loss: 1.344 | Acc: 51.304% (15268/29760)\n",
      "Loss: 1.344 | Acc: 51.318% (15305/29824)\n",
      "Loss: 1.344 | Acc: 51.315% (15337/29888)\n",
      "Loss: 1.343 | Acc: 51.329% (15374/29952)\n",
      "Loss: 1.343 | Acc: 51.326% (15406/30016)\n",
      "Loss: 1.343 | Acc: 51.333% (15441/30080)\n",
      "Loss: 1.342 | Acc: 51.337% (15475/30144)\n",
      "Loss: 1.342 | Acc: 51.354% (15513/30208)\n",
      "Loss: 1.342 | Acc: 51.348% (15544/30272)\n",
      "Loss: 1.342 | Acc: 51.352% (15578/30336)\n",
      "Loss: 1.342 | Acc: 51.349% (15610/30400)\n",
      "Loss: 1.341 | Acc: 51.356% (15645/30464)\n",
      "Loss: 1.341 | Acc: 51.389% (15688/30528)\n",
      "Loss: 1.341 | Acc: 51.402% (15725/30592)\n",
      "Loss: 1.340 | Acc: 51.429% (15766/30656)\n",
      "Loss: 1.340 | Acc: 51.452% (15806/30720)\n",
      "Loss: 1.341 | Acc: 51.439% (15835/30784)\n",
      "Loss: 1.340 | Acc: 51.426% (15864/30848)\n",
      "Loss: 1.340 | Acc: 51.443% (15902/30912)\n",
      "Loss: 1.340 | Acc: 51.466% (15942/30976)\n",
      "Loss: 1.340 | Acc: 51.463% (15974/31040)\n",
      "Loss: 1.340 | Acc: 51.463% (16007/31104)\n",
      "Loss: 1.339 | Acc: 51.473% (16043/31168)\n",
      "Loss: 1.339 | Acc: 51.476% (16077/31232)\n",
      "Loss: 1.339 | Acc: 51.486% (16113/31296)\n",
      "Loss: 1.339 | Acc: 51.508% (16153/31360)\n",
      "Loss: 1.339 | Acc: 51.505% (16185/31424)\n",
      "Loss: 1.339 | Acc: 51.509% (16219/31488)\n",
      "Loss: 1.339 | Acc: 51.515% (16254/31552)\n",
      "Loss: 1.339 | Acc: 51.521% (16289/31616)\n",
      "Loss: 1.338 | Acc: 51.547% (16330/31680)\n",
      "Loss: 1.338 | Acc: 51.566% (16369/31744)\n",
      "Loss: 1.337 | Acc: 51.600% (16413/31808)\n",
      "Loss: 1.337 | Acc: 51.597% (16445/31872)\n",
      "Loss: 1.338 | Acc: 51.588% (16475/31936)\n",
      "Loss: 1.338 | Acc: 51.581% (16506/32000)\n",
      "Loss: 1.338 | Acc: 51.591% (16542/32064)\n",
      "Loss: 1.338 | Acc: 51.609% (16581/32128)\n",
      "Loss: 1.338 | Acc: 51.597% (16610/32192)\n",
      "Loss: 1.338 | Acc: 51.597% (16643/32256)\n",
      "Loss: 1.338 | Acc: 51.593% (16675/32320)\n",
      "Loss: 1.338 | Acc: 51.584% (16705/32384)\n",
      "Loss: 1.338 | Acc: 51.596% (16742/32448)\n",
      "Loss: 1.338 | Acc: 51.596% (16775/32512)\n",
      "Loss: 1.338 | Acc: 51.593% (16807/32576)\n",
      "Loss: 1.338 | Acc: 51.602% (16843/32640)\n",
      "Loss: 1.338 | Acc: 51.596% (16874/32704)\n",
      "Loss: 1.338 | Acc: 51.608% (16911/32768)\n",
      "Loss: 1.338 | Acc: 51.617% (16947/32832)\n",
      "Loss: 1.338 | Acc: 51.611% (16978/32896)\n",
      "Loss: 1.338 | Acc: 51.632% (17018/32960)\n",
      "Loss: 1.338 | Acc: 51.641% (17054/33024)\n",
      "Loss: 1.338 | Acc: 51.635% (17085/33088)\n",
      "Loss: 1.338 | Acc: 51.629% (17116/33152)\n",
      "Loss: 1.338 | Acc: 51.623% (17147/33216)\n",
      "Loss: 1.338 | Acc: 51.632% (17183/33280)\n",
      "Loss: 1.338 | Acc: 51.634% (17217/33344)\n",
      "Loss: 1.338 | Acc: 51.628% (17248/33408)\n",
      "Loss: 1.337 | Acc: 51.637% (17284/33472)\n",
      "Loss: 1.337 | Acc: 51.610% (17308/33536)\n",
      "Loss: 1.337 | Acc: 51.610% (17341/33600)\n",
      "Loss: 1.337 | Acc: 51.637% (17383/33664)\n",
      "Loss: 1.337 | Acc: 51.634% (17415/33728)\n",
      "Loss: 1.337 | Acc: 51.639% (17450/33792)\n",
      "Loss: 1.336 | Acc: 51.654% (17488/33856)\n",
      "Loss: 1.336 | Acc: 51.672% (17527/33920)\n",
      "Loss: 1.336 | Acc: 51.665% (17558/33984)\n",
      "Loss: 1.336 | Acc: 51.665% (17591/34048)\n",
      "Loss: 1.336 | Acc: 51.674% (17627/34112)\n",
      "Loss: 1.336 | Acc: 51.674% (17660/34176)\n",
      "Loss: 1.336 | Acc: 51.659% (17688/34240)\n",
      "Loss: 1.336 | Acc: 51.656% (17720/34304)\n",
      "Loss: 1.336 | Acc: 51.650% (17751/34368)\n",
      "Loss: 1.336 | Acc: 51.667% (17790/34432)\n",
      "Loss: 1.336 | Acc: 51.690% (17831/34496)\n",
      "Loss: 1.335 | Acc: 51.696% (17866/34560)\n",
      "Loss: 1.335 | Acc: 51.698% (17900/34624)\n",
      "Loss: 1.335 | Acc: 51.695% (17932/34688)\n",
      "Loss: 1.335 | Acc: 51.712% (17971/34752)\n",
      "Loss: 1.335 | Acc: 51.720% (18007/34816)\n",
      "Loss: 1.335 | Acc: 51.712% (18037/34880)\n",
      "Loss: 1.335 | Acc: 51.720% (18073/34944)\n",
      "Loss: 1.335 | Acc: 51.728% (18109/35008)\n",
      "Loss: 1.336 | Acc: 51.691% (18129/35072)\n",
      "Loss: 1.335 | Acc: 51.708% (18168/35136)\n",
      "Loss: 1.335 | Acc: 51.722% (18206/35200)\n",
      "Loss: 1.335 | Acc: 51.727% (18241/35264)\n",
      "Loss: 1.336 | Acc: 51.710% (18268/35328)\n",
      "Loss: 1.335 | Acc: 51.715% (18303/35392)\n",
      "Loss: 1.336 | Acc: 51.695% (18329/35456)\n",
      "Loss: 1.336 | Acc: 51.692% (18361/35520)\n",
      "Loss: 1.336 | Acc: 51.695% (18395/35584)\n",
      "Loss: 1.336 | Acc: 51.700% (18430/35648)\n",
      "Loss: 1.335 | Acc: 51.722% (18471/35712)\n",
      "Loss: 1.335 | Acc: 51.713% (18501/35776)\n",
      "Loss: 1.335 | Acc: 51.719% (18536/35840)\n",
      "Loss: 1.335 | Acc: 51.718% (18569/35904)\n",
      "Loss: 1.335 | Acc: 51.718% (18602/35968)\n",
      "Loss: 1.335 | Acc: 51.712% (18633/36032)\n",
      "Loss: 1.334 | Acc: 51.726% (18671/36096)\n",
      "Loss: 1.334 | Acc: 51.723% (18703/36160)\n",
      "Loss: 1.334 | Acc: 51.736% (18741/36224)\n",
      "Loss: 1.334 | Acc: 51.731% (18772/36288)\n",
      "Loss: 1.335 | Acc: 51.728% (18804/36352)\n",
      "Loss: 1.334 | Acc: 51.727% (18837/36416)\n",
      "Loss: 1.335 | Acc: 51.727% (18870/36480)\n",
      "Loss: 1.335 | Acc: 51.727% (18903/36544)\n",
      "Loss: 1.335 | Acc: 51.718% (18933/36608)\n",
      "Loss: 1.334 | Acc: 51.734% (18972/36672)\n",
      "Loss: 1.334 | Acc: 51.734% (19005/36736)\n",
      "Loss: 1.334 | Acc: 51.739% (19040/36800)\n",
      "Loss: 1.334 | Acc: 51.752% (19078/36864)\n",
      "Loss: 1.334 | Acc: 51.771% (19118/36928)\n",
      "Loss: 1.334 | Acc: 51.771% (19151/36992)\n",
      "Loss: 1.334 | Acc: 51.773% (19185/37056)\n",
      "Loss: 1.334 | Acc: 51.786% (19223/37120)\n",
      "Loss: 1.334 | Acc: 51.778% (19253/37184)\n",
      "Loss: 1.334 | Acc: 51.772% (19284/37248)\n",
      "Loss: 1.334 | Acc: 51.774% (19318/37312)\n",
      "Loss: 1.334 | Acc: 51.779% (19353/37376)\n",
      "Loss: 1.333 | Acc: 51.790% (19390/37440)\n",
      "Loss: 1.333 | Acc: 51.792% (19424/37504)\n",
      "Loss: 1.333 | Acc: 51.799% (19460/37568)\n",
      "Loss: 1.333 | Acc: 51.791% (19490/37632)\n",
      "Loss: 1.333 | Acc: 51.801% (19527/37696)\n",
      "Loss: 1.333 | Acc: 51.806% (19562/37760)\n",
      "Loss: 1.333 | Acc: 51.811% (19597/37824)\n",
      "Loss: 1.333 | Acc: 51.805% (19628/37888)\n",
      "Loss: 1.333 | Acc: 51.792% (19656/37952)\n",
      "Loss: 1.333 | Acc: 51.783% (19686/38016)\n",
      "Loss: 1.333 | Acc: 51.770% (19714/38080)\n",
      "Loss: 1.333 | Acc: 51.775% (19749/38144)\n",
      "Loss: 1.333 | Acc: 51.798% (19791/38208)\n",
      "Loss: 1.333 | Acc: 51.803% (19826/38272)\n",
      "Loss: 1.333 | Acc: 51.810% (19862/38336)\n",
      "Loss: 1.333 | Acc: 51.812% (19896/38400)\n",
      "Loss: 1.333 | Acc: 51.794% (19922/38464)\n",
      "Loss: 1.333 | Acc: 51.794% (19955/38528)\n",
      "Loss: 1.333 | Acc: 51.793% (19988/38592)\n",
      "Loss: 1.332 | Acc: 51.813% (20029/38656)\n",
      "Loss: 1.332 | Acc: 51.805% (20059/38720)\n",
      "Loss: 1.332 | Acc: 51.810% (20094/38784)\n",
      "Loss: 1.332 | Acc: 51.804% (20125/38848)\n",
      "Loss: 1.332 | Acc: 51.807% (20159/38912)\n",
      "Loss: 1.332 | Acc: 51.827% (20200/38976)\n",
      "Loss: 1.332 | Acc: 51.821% (20231/39040)\n",
      "Loss: 1.332 | Acc: 51.818% (20263/39104)\n",
      "Loss: 1.332 | Acc: 51.818% (20296/39168)\n",
      "Loss: 1.333 | Acc: 51.807% (20325/39232)\n",
      "Loss: 1.333 | Acc: 51.797% (20354/39296)\n",
      "Loss: 1.333 | Acc: 51.773% (20378/39360)\n",
      "Loss: 1.333 | Acc: 51.768% (20409/39424)\n",
      "Loss: 1.333 | Acc: 51.760% (20439/39488)\n",
      "Loss: 1.333 | Acc: 51.757% (20471/39552)\n",
      "Loss: 1.333 | Acc: 51.749% (20501/39616)\n",
      "Loss: 1.334 | Acc: 51.744% (20532/39680)\n",
      "Loss: 1.333 | Acc: 51.761% (20572/39744)\n",
      "Loss: 1.333 | Acc: 51.779% (20612/39808)\n",
      "Loss: 1.333 | Acc: 51.783% (20647/39872)\n",
      "Loss: 1.333 | Acc: 51.788% (20682/39936)\n",
      "Loss: 1.333 | Acc: 51.797% (20719/40000)\n",
      "Loss: 1.332 | Acc: 51.795% (20751/40064)\n",
      "Loss: 1.332 | Acc: 51.799% (20786/40128)\n",
      "Loss: 1.332 | Acc: 51.809% (20823/40192)\n",
      "Loss: 1.332 | Acc: 51.833% (20866/40256)\n",
      "Loss: 1.331 | Acc: 51.853% (20907/40320)\n",
      "Loss: 1.331 | Acc: 51.855% (20941/40384)\n",
      "Loss: 1.331 | Acc: 51.864% (20978/40448)\n",
      "Loss: 1.331 | Acc: 51.866% (21012/40512)\n",
      "Loss: 1.331 | Acc: 51.863% (21044/40576)\n",
      "Loss: 1.331 | Acc: 51.858% (21075/40640)\n",
      "Loss: 1.331 | Acc: 51.838% (21100/40704)\n",
      "Loss: 1.331 | Acc: 51.827% (21129/40768)\n",
      "Loss: 1.331 | Acc: 51.842% (21168/40832)\n",
      "Loss: 1.331 | Acc: 51.846% (21203/40896)\n",
      "Loss: 1.331 | Acc: 51.841% (21234/40960)\n",
      "Loss: 1.331 | Acc: 51.823% (21260/41024)\n",
      "Loss: 1.331 | Acc: 51.820% (21292/41088)\n",
      "Loss: 1.331 | Acc: 51.820% (21325/41152)\n",
      "Loss: 1.331 | Acc: 51.817% (21357/41216)\n",
      "Loss: 1.331 | Acc: 51.827% (21394/41280)\n",
      "Loss: 1.331 | Acc: 51.829% (21428/41344)\n",
      "Loss: 1.331 | Acc: 51.811% (21454/41408)\n",
      "Loss: 1.331 | Acc: 51.835% (21497/41472)\n",
      "Loss: 1.331 | Acc: 51.839% (21532/41536)\n",
      "Loss: 1.330 | Acc: 51.856% (21572/41600)\n",
      "Loss: 1.331 | Acc: 51.848% (21602/41664)\n",
      "Loss: 1.330 | Acc: 51.852% (21637/41728)\n",
      "Loss: 1.330 | Acc: 51.864% (21675/41792)\n",
      "Loss: 1.330 | Acc: 51.866% (21709/41856)\n",
      "Loss: 1.330 | Acc: 51.875% (21746/41920)\n",
      "Loss: 1.330 | Acc: 51.894% (21787/41984)\n",
      "Loss: 1.330 | Acc: 51.907% (21826/42048)\n",
      "Loss: 1.330 | Acc: 51.900% (21856/42112)\n",
      "Loss: 1.330 | Acc: 51.897% (21888/42176)\n",
      "Loss: 1.330 | Acc: 51.901% (21923/42240)\n",
      "Loss: 1.330 | Acc: 51.898% (21955/42304)\n",
      "Loss: 1.330 | Acc: 51.900% (21989/42368)\n",
      "Loss: 1.330 | Acc: 51.909% (22026/42432)\n",
      "Loss: 1.329 | Acc: 51.911% (22060/42496)\n",
      "Loss: 1.329 | Acc: 51.906% (22091/42560)\n",
      "Loss: 1.329 | Acc: 51.914% (22128/42624)\n",
      "Loss: 1.329 | Acc: 51.919% (22163/42688)\n",
      "Loss: 1.329 | Acc: 51.918% (22196/42752)\n",
      "Loss: 1.329 | Acc: 51.918% (22229/42816)\n",
      "Loss: 1.329 | Acc: 51.898% (22254/42880)\n",
      "Loss: 1.330 | Acc: 51.900% (22288/42944)\n",
      "Loss: 1.329 | Acc: 51.914% (22327/43008)\n",
      "Loss: 1.329 | Acc: 51.927% (22366/43072)\n",
      "Loss: 1.329 | Acc: 51.917% (22395/43136)\n",
      "Loss: 1.329 | Acc: 51.910% (22425/43200)\n",
      "Loss: 1.329 | Acc: 51.921% (22463/43264)\n",
      "Loss: 1.329 | Acc: 51.916% (22494/43328)\n",
      "Loss: 1.329 | Acc: 51.936% (22536/43392)\n",
      "Loss: 1.329 | Acc: 51.938% (22570/43456)\n",
      "Loss: 1.329 | Acc: 51.958% (22612/43520)\n",
      "Loss: 1.329 | Acc: 51.950% (22642/43584)\n",
      "Loss: 1.329 | Acc: 51.959% (22679/43648)\n",
      "Loss: 1.329 | Acc: 51.949% (22708/43712)\n",
      "Loss: 1.329 | Acc: 51.946% (22740/43776)\n",
      "Loss: 1.329 | Acc: 51.955% (22777/43840)\n",
      "Loss: 1.329 | Acc: 51.972% (22818/43904)\n",
      "Loss: 1.329 | Acc: 51.970% (22850/43968)\n",
      "Loss: 1.329 | Acc: 51.971% (22884/44032)\n",
      "Loss: 1.329 | Acc: 51.982% (22922/44096)\n",
      "Loss: 1.328 | Acc: 52.004% (22965/44160)\n",
      "Loss: 1.328 | Acc: 51.999% (22996/44224)\n",
      "Loss: 1.329 | Acc: 51.989% (23025/44288)\n",
      "Loss: 1.328 | Acc: 52.016% (23070/44352)\n",
      "Loss: 1.328 | Acc: 52.031% (23110/44416)\n",
      "Loss: 1.328 | Acc: 52.046% (23150/44480)\n",
      "Loss: 1.328 | Acc: 52.065% (23192/44544)\n",
      "Loss: 1.328 | Acc: 52.069% (23227/44608)\n",
      "Loss: 1.327 | Acc: 52.080% (23265/44672)\n",
      "Loss: 1.328 | Acc: 52.072% (23295/44736)\n",
      "Loss: 1.327 | Acc: 52.078% (23331/44800)\n",
      "Loss: 1.327 | Acc: 52.071% (23361/44864)\n",
      "Loss: 1.327 | Acc: 52.081% (23399/44928)\n",
      "Loss: 1.327 | Acc: 52.074% (23429/44992)\n",
      "Loss: 1.327 | Acc: 52.075% (23463/45056)\n",
      "Loss: 1.327 | Acc: 52.079% (23498/45120)\n",
      "Loss: 1.327 | Acc: 52.072% (23528/45184)\n",
      "Loss: 1.327 | Acc: 52.082% (23566/45248)\n",
      "Loss: 1.327 | Acc: 52.088% (23602/45312)\n",
      "Loss: 1.327 | Acc: 52.091% (23637/45376)\n",
      "Loss: 1.327 | Acc: 52.088% (23669/45440)\n",
      "Loss: 1.327 | Acc: 52.083% (23700/45504)\n",
      "Loss: 1.327 | Acc: 52.087% (23735/45568)\n",
      "Loss: 1.327 | Acc: 52.084% (23767/45632)\n",
      "Loss: 1.327 | Acc: 52.083% (23800/45696)\n",
      "Loss: 1.327 | Acc: 52.069% (23827/45760)\n",
      "Loss: 1.327 | Acc: 52.064% (23858/45824)\n",
      "Loss: 1.327 | Acc: 52.068% (23893/45888)\n",
      "Loss: 1.327 | Acc: 52.063% (23924/45952)\n",
      "Loss: 1.327 | Acc: 52.067% (23959/46016)\n",
      "Loss: 1.327 | Acc: 52.072% (23995/46080)\n",
      "Loss: 1.327 | Acc: 52.078% (24031/46144)\n",
      "Loss: 1.327 | Acc: 52.065% (24058/46208)\n",
      "Loss: 1.327 | Acc: 52.062% (24090/46272)\n",
      "Loss: 1.327 | Acc: 52.057% (24121/46336)\n",
      "Loss: 1.327 | Acc: 52.056% (24154/46400)\n",
      "Loss: 1.327 | Acc: 52.058% (24188/46464)\n",
      "Loss: 1.327 | Acc: 52.044% (24215/46528)\n",
      "Loss: 1.327 | Acc: 52.048% (24250/46592)\n",
      "Loss: 1.327 | Acc: 52.058% (24288/46656)\n",
      "Loss: 1.328 | Acc: 52.044% (24315/46720)\n",
      "Loss: 1.328 | Acc: 52.052% (24352/46784)\n",
      "Loss: 1.328 | Acc: 52.047% (24383/46848)\n",
      "Loss: 1.328 | Acc: 52.049% (24417/46912)\n",
      "Loss: 1.328 | Acc: 52.048% (24450/46976)\n",
      "Loss: 1.328 | Acc: 52.056% (24487/47040)\n",
      "Loss: 1.327 | Acc: 52.068% (24526/47104)\n",
      "Loss: 1.327 | Acc: 52.073% (24562/47168)\n",
      "Loss: 1.327 | Acc: 52.077% (24597/47232)\n",
      "Loss: 1.327 | Acc: 52.076% (24630/47296)\n",
      "Loss: 1.327 | Acc: 52.088% (24669/47360)\n",
      "Loss: 1.327 | Acc: 52.083% (24700/47424)\n",
      "Loss: 1.327 | Acc: 52.081% (24732/47488)\n",
      "Loss: 1.327 | Acc: 52.088% (24769/47552)\n",
      "Loss: 1.327 | Acc: 52.079% (24798/47616)\n",
      "Loss: 1.327 | Acc: 52.091% (24837/47680)\n",
      "Loss: 1.327 | Acc: 52.092% (24871/47744)\n",
      "Loss: 1.327 | Acc: 52.111% (24913/47808)\n",
      "Loss: 1.327 | Acc: 52.099% (24941/47872)\n",
      "Loss: 1.327 | Acc: 52.103% (24976/47936)\n",
      "Loss: 1.327 | Acc: 52.098% (25007/48000)\n",
      "Loss: 1.327 | Acc: 52.101% (25042/48064)\n",
      "Loss: 1.328 | Acc: 52.101% (25075/48128)\n",
      "Loss: 1.328 | Acc: 52.104% (25110/48192)\n",
      "Loss: 1.328 | Acc: 52.101% (25142/48256)\n",
      "Loss: 1.328 | Acc: 52.103% (25176/48320)\n",
      "Loss: 1.328 | Acc: 52.102% (25209/48384)\n",
      "Loss: 1.328 | Acc: 52.103% (25243/48448)\n",
      "Loss: 1.327 | Acc: 52.115% (25282/48512)\n",
      "Loss: 1.328 | Acc: 52.110% (25313/48576)\n",
      "Loss: 1.327 | Acc: 52.120% (25351/48640)\n",
      "Loss: 1.328 | Acc: 52.113% (25381/48704)\n",
      "Loss: 1.327 | Acc: 52.116% (25416/48768)\n",
      "Loss: 1.328 | Acc: 52.107% (25445/48832)\n",
      "Loss: 1.328 | Acc: 52.088% (25469/48896)\n",
      "Loss: 1.328 | Acc: 52.081% (25499/48960)\n",
      "Loss: 1.328 | Acc: 52.084% (25521/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 52.083673469387755\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.359 | Acc: 50.000% (32/64)\n",
      "Loss: 1.272 | Acc: 53.906% (69/128)\n",
      "Loss: 1.303 | Acc: 52.604% (101/192)\n",
      "Loss: 1.336 | Acc: 51.172% (131/256)\n",
      "Loss: 1.299 | Acc: 52.500% (168/320)\n",
      "Loss: 1.297 | Acc: 52.865% (203/384)\n",
      "Loss: 1.316 | Acc: 52.009% (233/448)\n",
      "Loss: 1.299 | Acc: 52.734% (270/512)\n",
      "Loss: 1.276 | Acc: 53.993% (311/576)\n",
      "Loss: 1.263 | Acc: 54.062% (346/640)\n",
      "Loss: 1.269 | Acc: 53.977% (380/704)\n",
      "Loss: 1.277 | Acc: 53.776% (413/768)\n",
      "Loss: 1.278 | Acc: 54.087% (450/832)\n",
      "Loss: 1.268 | Acc: 54.688% (490/896)\n",
      "Loss: 1.257 | Acc: 54.583% (524/960)\n",
      "Loss: 1.247 | Acc: 55.078% (564/1024)\n",
      "Loss: 1.253 | Acc: 54.596% (594/1088)\n",
      "Loss: 1.250 | Acc: 54.601% (629/1152)\n",
      "Loss: 1.247 | Acc: 54.852% (667/1216)\n",
      "Loss: 1.256 | Acc: 54.609% (699/1280)\n",
      "Loss: 1.255 | Acc: 54.390% (731/1344)\n",
      "Loss: 1.251 | Acc: 54.616% (769/1408)\n",
      "Loss: 1.246 | Acc: 54.416% (801/1472)\n",
      "Loss: 1.249 | Acc: 54.427% (836/1536)\n",
      "Loss: 1.249 | Acc: 54.312% (869/1600)\n",
      "Loss: 1.251 | Acc: 54.447% (906/1664)\n",
      "Loss: 1.252 | Acc: 54.572% (943/1728)\n",
      "Loss: 1.257 | Acc: 54.074% (969/1792)\n",
      "Loss: 1.257 | Acc: 53.987% (1002/1856)\n",
      "Loss: 1.257 | Acc: 54.115% (1039/1920)\n",
      "Loss: 1.257 | Acc: 54.284% (1077/1984)\n",
      "Loss: 1.254 | Acc: 54.492% (1116/2048)\n",
      "Loss: 1.247 | Acc: 54.593% (1153/2112)\n",
      "Loss: 1.249 | Acc: 54.458% (1185/2176)\n",
      "Loss: 1.249 | Acc: 54.598% (1223/2240)\n",
      "Loss: 1.248 | Acc: 54.514% (1256/2304)\n",
      "Loss: 1.249 | Acc: 54.519% (1291/2368)\n",
      "Loss: 1.249 | Acc: 54.729% (1331/2432)\n",
      "Loss: 1.250 | Acc: 54.647% (1364/2496)\n",
      "Loss: 1.256 | Acc: 54.336% (1391/2560)\n",
      "Loss: 1.260 | Acc: 54.154% (1421/2624)\n",
      "Loss: 1.260 | Acc: 54.241% (1458/2688)\n",
      "Loss: 1.261 | Acc: 54.106% (1489/2752)\n",
      "Loss: 1.262 | Acc: 54.013% (1521/2816)\n",
      "Loss: 1.262 | Acc: 53.889% (1552/2880)\n",
      "Loss: 1.260 | Acc: 53.940% (1588/2944)\n",
      "Loss: 1.259 | Acc: 53.923% (1622/3008)\n",
      "Loss: 1.259 | Acc: 53.841% (1654/3072)\n",
      "Loss: 1.259 | Acc: 53.731% (1685/3136)\n",
      "Loss: 1.258 | Acc: 53.844% (1723/3200)\n",
      "Loss: 1.258 | Acc: 53.922% (1760/3264)\n",
      "Loss: 1.259 | Acc: 53.816% (1791/3328)\n",
      "Loss: 1.259 | Acc: 53.715% (1822/3392)\n",
      "Loss: 1.258 | Acc: 53.733% (1857/3456)\n",
      "Loss: 1.260 | Acc: 53.636% (1888/3520)\n",
      "Loss: 1.259 | Acc: 53.739% (1926/3584)\n",
      "Loss: 1.262 | Acc: 53.673% (1958/3648)\n",
      "Loss: 1.261 | Acc: 53.610% (1990/3712)\n",
      "Loss: 1.260 | Acc: 53.575% (2023/3776)\n",
      "Loss: 1.259 | Acc: 53.594% (2058/3840)\n",
      "Loss: 1.260 | Acc: 53.432% (2086/3904)\n",
      "Loss: 1.260 | Acc: 53.402% (2119/3968)\n",
      "Loss: 1.260 | Acc: 53.423% (2154/4032)\n",
      "Loss: 1.261 | Acc: 53.540% (2193/4096)\n",
      "Loss: 1.262 | Acc: 53.582% (2229/4160)\n",
      "Loss: 1.260 | Acc: 53.693% (2268/4224)\n",
      "Loss: 1.260 | Acc: 53.755% (2305/4288)\n",
      "Loss: 1.259 | Acc: 53.745% (2339/4352)\n",
      "Loss: 1.257 | Acc: 53.895% (2380/4416)\n",
      "Loss: 1.255 | Acc: 53.951% (2417/4480)\n",
      "Loss: 1.254 | Acc: 54.049% (2456/4544)\n",
      "Loss: 1.256 | Acc: 53.971% (2487/4608)\n",
      "Loss: 1.254 | Acc: 54.088% (2527/4672)\n",
      "Loss: 1.252 | Acc: 54.075% (2561/4736)\n",
      "Loss: 1.254 | Acc: 54.083% (2596/4800)\n",
      "Loss: 1.252 | Acc: 54.235% (2638/4864)\n",
      "Loss: 1.251 | Acc: 54.363% (2679/4928)\n",
      "Loss: 1.251 | Acc: 54.267% (2709/4992)\n",
      "Loss: 1.250 | Acc: 54.312% (2746/5056)\n",
      "Loss: 1.252 | Acc: 54.219% (2776/5120)\n",
      "Loss: 1.251 | Acc: 54.302% (2815/5184)\n",
      "Loss: 1.252 | Acc: 54.249% (2847/5248)\n",
      "Loss: 1.249 | Acc: 54.311% (2885/5312)\n",
      "Loss: 1.252 | Acc: 54.222% (2915/5376)\n",
      "Loss: 1.251 | Acc: 54.173% (2947/5440)\n",
      "Loss: 1.251 | Acc: 54.197% (2983/5504)\n",
      "Loss: 1.253 | Acc: 54.095% (3012/5568)\n",
      "Loss: 1.254 | Acc: 54.102% (3047/5632)\n",
      "Loss: 1.253 | Acc: 54.073% (3080/5696)\n",
      "Loss: 1.252 | Acc: 54.132% (3118/5760)\n",
      "Loss: 1.252 | Acc: 54.087% (3150/5824)\n",
      "Loss: 1.254 | Acc: 53.991% (3179/5888)\n",
      "Loss: 1.255 | Acc: 53.999% (3214/5952)\n",
      "Loss: 1.254 | Acc: 54.056% (3252/6016)\n",
      "Loss: 1.256 | Acc: 53.947% (3280/6080)\n",
      "Loss: 1.256 | Acc: 53.955% (3315/6144)\n",
      "Loss: 1.255 | Acc: 53.947% (3349/6208)\n",
      "Loss: 1.257 | Acc: 53.827% (3376/6272)\n",
      "Loss: 1.256 | Acc: 53.883% (3414/6336)\n",
      "Loss: 1.257 | Acc: 53.812% (3444/6400)\n",
      "Loss: 1.258 | Acc: 53.806% (3478/6464)\n",
      "Loss: 1.257 | Acc: 53.845% (3515/6528)\n",
      "Loss: 1.259 | Acc: 53.701% (3540/6592)\n",
      "Loss: 1.260 | Acc: 53.711% (3575/6656)\n",
      "Loss: 1.259 | Acc: 53.661% (3606/6720)\n",
      "Loss: 1.259 | Acc: 53.700% (3643/6784)\n",
      "Loss: 1.257 | Acc: 53.724% (3679/6848)\n",
      "Loss: 1.258 | Acc: 53.675% (3710/6912)\n",
      "Loss: 1.260 | Acc: 53.598% (3739/6976)\n",
      "Loss: 1.260 | Acc: 53.594% (3773/7040)\n",
      "Loss: 1.260 | Acc: 53.575% (3806/7104)\n",
      "Loss: 1.260 | Acc: 53.627% (3844/7168)\n",
      "Loss: 1.261 | Acc: 53.554% (3873/7232)\n",
      "Loss: 1.259 | Acc: 53.646% (3914/7296)\n",
      "Loss: 1.258 | Acc: 53.668% (3950/7360)\n",
      "Loss: 1.258 | Acc: 53.650% (3983/7424)\n",
      "Loss: 1.258 | Acc: 53.659% (4018/7488)\n",
      "Loss: 1.257 | Acc: 53.668% (4053/7552)\n",
      "Loss: 1.258 | Acc: 53.611% (4083/7616)\n",
      "Loss: 1.257 | Acc: 53.724% (4126/7680)\n",
      "Loss: 1.256 | Acc: 53.758% (4163/7744)\n",
      "Loss: 1.255 | Acc: 53.791% (4200/7808)\n",
      "Loss: 1.256 | Acc: 53.697% (4227/7872)\n",
      "Loss: 1.256 | Acc: 53.705% (4262/7936)\n",
      "Loss: 1.257 | Acc: 53.675% (4294/8000)\n",
      "Loss: 1.257 | Acc: 53.683% (4329/8064)\n",
      "Loss: 1.259 | Acc: 53.617% (4358/8128)\n",
      "Loss: 1.258 | Acc: 53.662% (4396/8192)\n",
      "Loss: 1.259 | Acc: 53.634% (4428/8256)\n",
      "Loss: 1.261 | Acc: 53.546% (4455/8320)\n",
      "Loss: 1.263 | Acc: 53.542% (4489/8384)\n",
      "Loss: 1.262 | Acc: 53.575% (4526/8448)\n",
      "Loss: 1.263 | Acc: 53.548% (4558/8512)\n",
      "Loss: 1.264 | Acc: 53.498% (4588/8576)\n",
      "Loss: 1.264 | Acc: 53.484% (4621/8640)\n",
      "Loss: 1.264 | Acc: 53.504% (4657/8704)\n",
      "Loss: 1.264 | Acc: 53.513% (4692/8768)\n",
      "Loss: 1.263 | Acc: 53.567% (4731/8832)\n",
      "Loss: 1.262 | Acc: 53.608% (4769/8896)\n",
      "Loss: 1.263 | Acc: 53.627% (4805/8960)\n",
      "Loss: 1.263 | Acc: 53.646% (4841/9024)\n",
      "Loss: 1.265 | Acc: 53.543% (4866/9088)\n",
      "Loss: 1.265 | Acc: 53.584% (4904/9152)\n",
      "Loss: 1.263 | Acc: 53.613% (4941/9216)\n",
      "Loss: 1.263 | Acc: 53.610% (4975/9280)\n",
      "Loss: 1.263 | Acc: 53.564% (5005/9344)\n",
      "Loss: 1.263 | Acc: 53.582% (5041/9408)\n",
      "Loss: 1.264 | Acc: 53.558% (5073/9472)\n",
      "Loss: 1.265 | Acc: 53.513% (5103/9536)\n",
      "Loss: 1.264 | Acc: 53.552% (5141/9600)\n",
      "Loss: 1.264 | Acc: 53.591% (5179/9664)\n",
      "Loss: 1.264 | Acc: 53.588% (5213/9728)\n",
      "Loss: 1.264 | Acc: 53.585% (5247/9792)\n",
      "Loss: 1.263 | Acc: 53.612% (5284/9856)\n",
      "Loss: 1.263 | Acc: 53.599% (5317/9920)\n",
      "Loss: 1.264 | Acc: 53.516% (5343/9984)\n",
      "Loss: 1.264 | Acc: 53.480% (5348/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 53.48\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.184 | Acc: 56.250% (36/64)\n",
      "Loss: 1.236 | Acc: 55.469% (71/128)\n",
      "Loss: 1.177 | Acc: 58.333% (112/192)\n",
      "Loss: 1.200 | Acc: 58.594% (150/256)\n",
      "Loss: 1.161 | Acc: 59.062% (189/320)\n",
      "Loss: 1.167 | Acc: 59.635% (229/384)\n",
      "Loss: 1.168 | Acc: 58.705% (263/448)\n",
      "Loss: 1.167 | Acc: 58.984% (302/512)\n",
      "Loss: 1.168 | Acc: 58.507% (337/576)\n",
      "Loss: 1.185 | Acc: 57.812% (370/640)\n",
      "Loss: 1.192 | Acc: 57.812% (407/704)\n",
      "Loss: 1.187 | Acc: 57.682% (443/768)\n",
      "Loss: 1.173 | Acc: 57.812% (481/832)\n",
      "Loss: 1.169 | Acc: 57.366% (514/896)\n",
      "Loss: 1.175 | Acc: 57.188% (549/960)\n",
      "Loss: 1.172 | Acc: 57.324% (587/1024)\n",
      "Loss: 1.165 | Acc: 57.445% (625/1088)\n",
      "Loss: 1.165 | Acc: 57.292% (660/1152)\n",
      "Loss: 1.163 | Acc: 57.237% (696/1216)\n",
      "Loss: 1.155 | Acc: 57.500% (736/1280)\n",
      "Loss: 1.160 | Acc: 57.440% (772/1344)\n",
      "Loss: 1.162 | Acc: 57.528% (810/1408)\n",
      "Loss: 1.159 | Acc: 57.880% (852/1472)\n",
      "Loss: 1.161 | Acc: 57.552% (884/1536)\n",
      "Loss: 1.162 | Acc: 57.438% (919/1600)\n",
      "Loss: 1.170 | Acc: 56.971% (948/1664)\n",
      "Loss: 1.166 | Acc: 57.060% (986/1728)\n",
      "Loss: 1.174 | Acc: 56.975% (1021/1792)\n",
      "Loss: 1.173 | Acc: 57.274% (1063/1856)\n",
      "Loss: 1.172 | Acc: 57.135% (1097/1920)\n",
      "Loss: 1.171 | Acc: 57.208% (1135/1984)\n",
      "Loss: 1.173 | Acc: 57.227% (1172/2048)\n",
      "Loss: 1.171 | Acc: 57.339% (1211/2112)\n",
      "Loss: 1.173 | Acc: 57.307% (1247/2176)\n",
      "Loss: 1.169 | Acc: 57.321% (1284/2240)\n",
      "Loss: 1.171 | Acc: 57.335% (1321/2304)\n",
      "Loss: 1.179 | Acc: 57.010% (1350/2368)\n",
      "Loss: 1.182 | Acc: 57.155% (1390/2432)\n",
      "Loss: 1.184 | Acc: 57.131% (1426/2496)\n",
      "Loss: 1.184 | Acc: 57.109% (1462/2560)\n",
      "Loss: 1.188 | Acc: 57.012% (1496/2624)\n",
      "Loss: 1.186 | Acc: 57.217% (1538/2688)\n",
      "Loss: 1.188 | Acc: 57.049% (1570/2752)\n",
      "Loss: 1.182 | Acc: 57.244% (1612/2816)\n",
      "Loss: 1.178 | Acc: 57.361% (1652/2880)\n",
      "Loss: 1.178 | Acc: 57.371% (1689/2944)\n",
      "Loss: 1.182 | Acc: 57.214% (1721/3008)\n",
      "Loss: 1.178 | Acc: 57.259% (1759/3072)\n",
      "Loss: 1.176 | Acc: 57.270% (1796/3136)\n",
      "Loss: 1.178 | Acc: 57.219% (1831/3200)\n",
      "Loss: 1.178 | Acc: 57.138% (1865/3264)\n",
      "Loss: 1.182 | Acc: 57.061% (1899/3328)\n",
      "Loss: 1.180 | Acc: 57.164% (1939/3392)\n",
      "Loss: 1.181 | Acc: 57.089% (1973/3456)\n",
      "Loss: 1.182 | Acc: 56.932% (2004/3520)\n",
      "Loss: 1.184 | Acc: 56.948% (2041/3584)\n",
      "Loss: 1.189 | Acc: 56.689% (2068/3648)\n",
      "Loss: 1.190 | Acc: 56.546% (2099/3712)\n",
      "Loss: 1.191 | Acc: 56.488% (2133/3776)\n",
      "Loss: 1.193 | Acc: 56.510% (2170/3840)\n",
      "Loss: 1.195 | Acc: 56.455% (2204/3904)\n",
      "Loss: 1.192 | Acc: 56.653% (2248/3968)\n",
      "Loss: 1.191 | Acc: 56.696% (2286/4032)\n",
      "Loss: 1.192 | Acc: 56.665% (2321/4096)\n",
      "Loss: 1.195 | Acc: 56.514% (2351/4160)\n",
      "Loss: 1.195 | Acc: 56.487% (2386/4224)\n",
      "Loss: 1.194 | Acc: 56.460% (2421/4288)\n",
      "Loss: 1.192 | Acc: 56.595% (2463/4352)\n",
      "Loss: 1.195 | Acc: 56.544% (2497/4416)\n",
      "Loss: 1.195 | Acc: 56.652% (2538/4480)\n",
      "Loss: 1.194 | Acc: 56.778% (2580/4544)\n",
      "Loss: 1.194 | Acc: 56.749% (2615/4608)\n",
      "Loss: 1.196 | Acc: 56.721% (2650/4672)\n",
      "Loss: 1.197 | Acc: 56.693% (2685/4736)\n",
      "Loss: 1.199 | Acc: 56.729% (2723/4800)\n",
      "Loss: 1.200 | Acc: 56.743% (2760/4864)\n",
      "Loss: 1.203 | Acc: 56.656% (2792/4928)\n",
      "Loss: 1.203 | Acc: 56.611% (2826/4992)\n",
      "Loss: 1.201 | Acc: 56.764% (2870/5056)\n",
      "Loss: 1.205 | Acc: 56.641% (2900/5120)\n",
      "Loss: 1.205 | Acc: 56.694% (2939/5184)\n",
      "Loss: 1.205 | Acc: 56.650% (2973/5248)\n",
      "Loss: 1.204 | Acc: 56.702% (3012/5312)\n",
      "Loss: 1.201 | Acc: 56.882% (3058/5376)\n",
      "Loss: 1.199 | Acc: 56.949% (3098/5440)\n",
      "Loss: 1.200 | Acc: 56.959% (3135/5504)\n",
      "Loss: 1.201 | Acc: 56.932% (3170/5568)\n",
      "Loss: 1.200 | Acc: 56.942% (3207/5632)\n",
      "Loss: 1.200 | Acc: 56.917% (3242/5696)\n",
      "Loss: 1.201 | Acc: 56.858% (3275/5760)\n",
      "Loss: 1.201 | Acc: 56.885% (3313/5824)\n",
      "Loss: 1.201 | Acc: 56.878% (3349/5888)\n",
      "Loss: 1.199 | Acc: 56.922% (3388/5952)\n",
      "Loss: 1.200 | Acc: 56.848% (3420/6016)\n",
      "Loss: 1.201 | Acc: 56.842% (3456/6080)\n",
      "Loss: 1.201 | Acc: 56.885% (3495/6144)\n",
      "Loss: 1.199 | Acc: 56.975% (3537/6208)\n",
      "Loss: 1.199 | Acc: 56.952% (3572/6272)\n",
      "Loss: 1.196 | Acc: 57.118% (3619/6336)\n",
      "Loss: 1.197 | Acc: 57.000% (3648/6400)\n",
      "Loss: 1.197 | Acc: 56.977% (3683/6464)\n",
      "Loss: 1.196 | Acc: 57.047% (3724/6528)\n",
      "Loss: 1.196 | Acc: 57.069% (3762/6592)\n",
      "Loss: 1.194 | Acc: 57.121% (3802/6656)\n",
      "Loss: 1.195 | Acc: 57.098% (3837/6720)\n",
      "Loss: 1.195 | Acc: 57.031% (3869/6784)\n",
      "Loss: 1.195 | Acc: 57.009% (3904/6848)\n",
      "Loss: 1.193 | Acc: 57.060% (3944/6912)\n",
      "Loss: 1.195 | Acc: 56.938% (3972/6976)\n",
      "Loss: 1.199 | Acc: 56.932% (4008/7040)\n",
      "Loss: 1.199 | Acc: 56.926% (4044/7104)\n",
      "Loss: 1.200 | Acc: 56.948% (4082/7168)\n",
      "Loss: 1.200 | Acc: 56.872% (4113/7232)\n",
      "Loss: 1.199 | Acc: 56.935% (4154/7296)\n",
      "Loss: 1.197 | Acc: 56.984% (4194/7360)\n",
      "Loss: 1.198 | Acc: 56.937% (4227/7424)\n",
      "Loss: 1.197 | Acc: 56.985% (4267/7488)\n",
      "Loss: 1.196 | Acc: 56.978% (4303/7552)\n",
      "Loss: 1.198 | Acc: 56.933% (4336/7616)\n",
      "Loss: 1.198 | Acc: 56.927% (4372/7680)\n",
      "Loss: 1.201 | Acc: 56.857% (4403/7744)\n",
      "Loss: 1.203 | Acc: 56.762% (4432/7808)\n",
      "Loss: 1.204 | Acc: 56.733% (4466/7872)\n",
      "Loss: 1.205 | Acc: 56.691% (4499/7936)\n",
      "Loss: 1.205 | Acc: 56.675% (4534/8000)\n",
      "Loss: 1.206 | Acc: 56.622% (4566/8064)\n",
      "Loss: 1.208 | Acc: 56.545% (4596/8128)\n",
      "Loss: 1.207 | Acc: 56.616% (4638/8192)\n",
      "Loss: 1.208 | Acc: 56.589% (4672/8256)\n",
      "Loss: 1.209 | Acc: 56.538% (4704/8320)\n",
      "Loss: 1.210 | Acc: 56.489% (4736/8384)\n",
      "Loss: 1.211 | Acc: 56.428% (4767/8448)\n",
      "Loss: 1.211 | Acc: 56.426% (4803/8512)\n",
      "Loss: 1.211 | Acc: 56.437% (4840/8576)\n",
      "Loss: 1.210 | Acc: 56.505% (4882/8640)\n",
      "Loss: 1.210 | Acc: 56.503% (4918/8704)\n",
      "Loss: 1.210 | Acc: 56.512% (4955/8768)\n",
      "Loss: 1.209 | Acc: 56.544% (4994/8832)\n",
      "Loss: 1.209 | Acc: 56.542% (5030/8896)\n",
      "Loss: 1.209 | Acc: 56.551% (5067/8960)\n",
      "Loss: 1.211 | Acc: 56.472% (5096/9024)\n",
      "Loss: 1.211 | Acc: 56.481% (5133/9088)\n",
      "Loss: 1.210 | Acc: 56.479% (5169/9152)\n",
      "Loss: 1.211 | Acc: 56.489% (5206/9216)\n",
      "Loss: 1.209 | Acc: 56.541% (5247/9280)\n",
      "Loss: 1.209 | Acc: 56.550% (5284/9344)\n",
      "Loss: 1.209 | Acc: 56.526% (5318/9408)\n",
      "Loss: 1.209 | Acc: 56.535% (5355/9472)\n",
      "Loss: 1.209 | Acc: 56.523% (5390/9536)\n",
      "Loss: 1.207 | Acc: 56.615% (5435/9600)\n",
      "Loss: 1.207 | Acc: 56.591% (5469/9664)\n",
      "Loss: 1.206 | Acc: 56.600% (5506/9728)\n",
      "Loss: 1.207 | Acc: 56.536% (5536/9792)\n",
      "Loss: 1.207 | Acc: 56.504% (5569/9856)\n",
      "Loss: 1.210 | Acc: 56.421% (5597/9920)\n",
      "Loss: 1.210 | Acc: 56.400% (5631/9984)\n",
      "Loss: 1.210 | Acc: 56.419% (5669/10048)\n",
      "Loss: 1.208 | Acc: 56.448% (5708/10112)\n",
      "Loss: 1.208 | Acc: 56.456% (5745/10176)\n",
      "Loss: 1.209 | Acc: 56.436% (5779/10240)\n",
      "Loss: 1.210 | Acc: 56.386% (5810/10304)\n",
      "Loss: 1.211 | Acc: 56.366% (5844/10368)\n",
      "Loss: 1.209 | Acc: 56.375% (5881/10432)\n",
      "Loss: 1.210 | Acc: 56.355% (5915/10496)\n",
      "Loss: 1.209 | Acc: 56.354% (5951/10560)\n",
      "Loss: 1.209 | Acc: 56.382% (5990/10624)\n",
      "Loss: 1.210 | Acc: 56.334% (6021/10688)\n",
      "Loss: 1.210 | Acc: 56.315% (6055/10752)\n",
      "Loss: 1.211 | Acc: 56.268% (6086/10816)\n",
      "Loss: 1.210 | Acc: 56.296% (6125/10880)\n",
      "Loss: 1.208 | Acc: 56.396% (6172/10944)\n",
      "Loss: 1.209 | Acc: 56.314% (6199/11008)\n",
      "Loss: 1.208 | Acc: 56.268% (6230/11072)\n",
      "Loss: 1.208 | Acc: 56.286% (6268/11136)\n",
      "Loss: 1.208 | Acc: 56.259% (6301/11200)\n",
      "Loss: 1.208 | Acc: 56.259% (6337/11264)\n",
      "Loss: 1.210 | Acc: 56.197% (6366/11328)\n",
      "Loss: 1.210 | Acc: 56.171% (6399/11392)\n",
      "Loss: 1.210 | Acc: 56.137% (6431/11456)\n",
      "Loss: 1.210 | Acc: 56.189% (6473/11520)\n",
      "Loss: 1.209 | Acc: 56.233% (6514/11584)\n",
      "Loss: 1.209 | Acc: 56.224% (6549/11648)\n",
      "Loss: 1.209 | Acc: 56.233% (6586/11712)\n",
      "Loss: 1.208 | Acc: 56.309% (6631/11776)\n",
      "Loss: 1.210 | Acc: 56.284% (6664/11840)\n",
      "Loss: 1.210 | Acc: 56.284% (6700/11904)\n",
      "Loss: 1.209 | Acc: 56.292% (6737/11968)\n",
      "Loss: 1.209 | Acc: 56.325% (6777/12032)\n",
      "Loss: 1.209 | Acc: 56.300% (6810/12096)\n",
      "Loss: 1.210 | Acc: 56.308% (6847/12160)\n",
      "Loss: 1.210 | Acc: 56.299% (6882/12224)\n",
      "Loss: 1.211 | Acc: 56.299% (6918/12288)\n",
      "Loss: 1.211 | Acc: 56.315% (6956/12352)\n",
      "Loss: 1.211 | Acc: 56.306% (6991/12416)\n",
      "Loss: 1.211 | Acc: 56.314% (7028/12480)\n",
      "Loss: 1.211 | Acc: 56.362% (7070/12544)\n",
      "Loss: 1.211 | Acc: 56.393% (7110/12608)\n",
      "Loss: 1.211 | Acc: 56.408% (7148/12672)\n",
      "Loss: 1.210 | Acc: 56.454% (7190/12736)\n",
      "Loss: 1.210 | Acc: 56.453% (7226/12800)\n",
      "Loss: 1.209 | Acc: 56.538% (7273/12864)\n",
      "Loss: 1.208 | Acc: 56.559% (7312/12928)\n",
      "Loss: 1.209 | Acc: 56.512% (7342/12992)\n",
      "Loss: 1.209 | Acc: 56.487% (7375/13056)\n",
      "Loss: 1.209 | Acc: 56.494% (7412/13120)\n",
      "Loss: 1.208 | Acc: 56.470% (7445/13184)\n",
      "Loss: 1.208 | Acc: 56.492% (7484/13248)\n",
      "Loss: 1.208 | Acc: 56.475% (7518/13312)\n",
      "Loss: 1.208 | Acc: 56.437% (7549/13376)\n",
      "Loss: 1.208 | Acc: 56.458% (7588/13440)\n",
      "Loss: 1.208 | Acc: 56.420% (7619/13504)\n",
      "Loss: 1.208 | Acc: 56.442% (7658/13568)\n",
      "Loss: 1.208 | Acc: 56.419% (7691/13632)\n",
      "Loss: 1.207 | Acc: 56.454% (7732/13696)\n",
      "Loss: 1.206 | Acc: 56.453% (7768/13760)\n",
      "Loss: 1.205 | Acc: 56.489% (7809/13824)\n",
      "Loss: 1.205 | Acc: 56.480% (7844/13888)\n",
      "Loss: 1.206 | Acc: 56.451% (7876/13952)\n",
      "Loss: 1.206 | Acc: 56.414% (7907/14016)\n",
      "Loss: 1.206 | Acc: 56.428% (7945/14080)\n",
      "Loss: 1.205 | Acc: 56.476% (7988/14144)\n",
      "Loss: 1.206 | Acc: 56.440% (8019/14208)\n",
      "Loss: 1.205 | Acc: 56.460% (8058/14272)\n",
      "Loss: 1.206 | Acc: 56.452% (8093/14336)\n",
      "Loss: 1.206 | Acc: 56.451% (8129/14400)\n",
      "Loss: 1.205 | Acc: 56.471% (8168/14464)\n",
      "Loss: 1.206 | Acc: 56.470% (8204/14528)\n",
      "Loss: 1.205 | Acc: 56.517% (8247/14592)\n",
      "Loss: 1.205 | Acc: 56.516% (8283/14656)\n",
      "Loss: 1.205 | Acc: 56.556% (8325/14720)\n",
      "Loss: 1.204 | Acc: 56.568% (8363/14784)\n",
      "Loss: 1.203 | Acc: 56.580% (8401/14848)\n",
      "Loss: 1.203 | Acc: 56.565% (8435/14912)\n",
      "Loss: 1.203 | Acc: 56.544% (8468/14976)\n",
      "Loss: 1.204 | Acc: 56.496% (8497/15040)\n",
      "Loss: 1.205 | Acc: 56.475% (8530/15104)\n",
      "Loss: 1.205 | Acc: 56.487% (8568/15168)\n",
      "Loss: 1.204 | Acc: 56.532% (8611/15232)\n",
      "Loss: 1.204 | Acc: 56.498% (8642/15296)\n",
      "Loss: 1.205 | Acc: 56.458% (8672/15360)\n",
      "Loss: 1.206 | Acc: 56.432% (8704/15424)\n",
      "Loss: 1.205 | Acc: 56.495% (8750/15488)\n",
      "Loss: 1.205 | Acc: 56.514% (8789/15552)\n",
      "Loss: 1.204 | Acc: 56.532% (8828/15616)\n",
      "Loss: 1.204 | Acc: 56.537% (8865/15680)\n",
      "Loss: 1.203 | Acc: 56.523% (8899/15744)\n",
      "Loss: 1.202 | Acc: 56.547% (8939/15808)\n",
      "Loss: 1.202 | Acc: 56.565% (8978/15872)\n",
      "Loss: 1.201 | Acc: 56.564% (9014/15936)\n",
      "Loss: 1.201 | Acc: 56.544% (9047/16000)\n",
      "Loss: 1.202 | Acc: 56.536% (9082/16064)\n",
      "Loss: 1.201 | Acc: 56.572% (9124/16128)\n",
      "Loss: 1.202 | Acc: 56.559% (9158/16192)\n",
      "Loss: 1.202 | Acc: 56.545% (9192/16256)\n",
      "Loss: 1.203 | Acc: 56.532% (9226/16320)\n",
      "Loss: 1.204 | Acc: 56.531% (9262/16384)\n",
      "Loss: 1.204 | Acc: 56.524% (9297/16448)\n",
      "Loss: 1.204 | Acc: 56.529% (9334/16512)\n",
      "Loss: 1.204 | Acc: 56.521% (9369/16576)\n",
      "Loss: 1.206 | Acc: 56.478% (9398/16640)\n",
      "Loss: 1.205 | Acc: 56.489% (9436/16704)\n",
      "Loss: 1.204 | Acc: 56.500% (9474/16768)\n",
      "Loss: 1.204 | Acc: 56.535% (9516/16832)\n",
      "Loss: 1.204 | Acc: 56.564% (9557/16896)\n",
      "Loss: 1.202 | Acc: 56.639% (9606/16960)\n",
      "Loss: 1.203 | Acc: 56.614% (9638/17024)\n",
      "Loss: 1.202 | Acc: 56.613% (9674/17088)\n",
      "Loss: 1.202 | Acc: 56.635% (9714/17152)\n",
      "Loss: 1.202 | Acc: 56.633% (9750/17216)\n",
      "Loss: 1.202 | Acc: 56.655% (9790/17280)\n",
      "Loss: 1.202 | Acc: 56.671% (9829/17344)\n",
      "Loss: 1.201 | Acc: 56.681% (9867/17408)\n",
      "Loss: 1.201 | Acc: 56.662% (9900/17472)\n",
      "Loss: 1.201 | Acc: 56.632% (9931/17536)\n",
      "Loss: 1.201 | Acc: 56.648% (9970/17600)\n",
      "Loss: 1.202 | Acc: 56.629% (10003/17664)\n",
      "Loss: 1.203 | Acc: 56.600% (10034/17728)\n",
      "Loss: 1.202 | Acc: 56.621% (10074/17792)\n",
      "Loss: 1.202 | Acc: 56.631% (10112/17856)\n",
      "Loss: 1.203 | Acc: 56.607% (10144/17920)\n",
      "Loss: 1.203 | Acc: 56.611% (10181/17984)\n",
      "Loss: 1.203 | Acc: 56.616% (10218/18048)\n",
      "Loss: 1.202 | Acc: 56.631% (10257/18112)\n",
      "Loss: 1.201 | Acc: 56.668% (10300/18176)\n",
      "Loss: 1.201 | Acc: 56.694% (10341/18240)\n",
      "Loss: 1.201 | Acc: 56.720% (10382/18304)\n",
      "Loss: 1.200 | Acc: 56.735% (10421/18368)\n",
      "Loss: 1.200 | Acc: 56.733% (10457/18432)\n",
      "Loss: 1.200 | Acc: 56.699% (10487/18496)\n",
      "Loss: 1.200 | Acc: 56.692% (10522/18560)\n",
      "Loss: 1.200 | Acc: 56.712% (10562/18624)\n",
      "Loss: 1.200 | Acc: 56.699% (10596/18688)\n",
      "Loss: 1.202 | Acc: 56.666% (10626/18752)\n",
      "Loss: 1.202 | Acc: 56.617% (10653/18816)\n",
      "Loss: 1.202 | Acc: 56.621% (10690/18880)\n",
      "Loss: 1.202 | Acc: 56.635% (10729/18944)\n",
      "Loss: 1.202 | Acc: 56.676% (10773/19008)\n",
      "Loss: 1.202 | Acc: 56.638% (10802/19072)\n",
      "Loss: 1.201 | Acc: 56.652% (10841/19136)\n",
      "Loss: 1.202 | Acc: 56.625% (10872/19200)\n",
      "Loss: 1.202 | Acc: 56.624% (10908/19264)\n",
      "Loss: 1.202 | Acc: 56.628% (10945/19328)\n",
      "Loss: 1.201 | Acc: 56.647% (10985/19392)\n",
      "Loss: 1.201 | Acc: 56.641% (11020/19456)\n",
      "Loss: 1.201 | Acc: 56.634% (11055/19520)\n",
      "Loss: 1.202 | Acc: 56.628% (11090/19584)\n",
      "Loss: 1.202 | Acc: 56.622% (11125/19648)\n",
      "Loss: 1.202 | Acc: 56.590% (11155/19712)\n",
      "Loss: 1.202 | Acc: 56.624% (11198/19776)\n",
      "Loss: 1.201 | Acc: 56.623% (11234/19840)\n",
      "Loss: 1.201 | Acc: 56.607% (11267/19904)\n",
      "Loss: 1.201 | Acc: 56.616% (11305/19968)\n",
      "Loss: 1.202 | Acc: 56.614% (11341/20032)\n",
      "Loss: 1.202 | Acc: 56.603% (11375/20096)\n",
      "Loss: 1.202 | Acc: 56.627% (11416/20160)\n",
      "Loss: 1.202 | Acc: 56.611% (11449/20224)\n",
      "Loss: 1.202 | Acc: 56.615% (11486/20288)\n",
      "Loss: 1.202 | Acc: 56.619% (11523/20352)\n",
      "Loss: 1.202 | Acc: 56.632% (11562/20416)\n",
      "Loss: 1.203 | Acc: 56.606% (11593/20480)\n",
      "Loss: 1.203 | Acc: 56.591% (11626/20544)\n",
      "Loss: 1.203 | Acc: 56.590% (11662/20608)\n",
      "Loss: 1.203 | Acc: 56.579% (11696/20672)\n",
      "Loss: 1.202 | Acc: 56.588% (11734/20736)\n",
      "Loss: 1.203 | Acc: 56.591% (11771/20800)\n",
      "Loss: 1.203 | Acc: 56.586% (11806/20864)\n",
      "Loss: 1.202 | Acc: 56.599% (11845/20928)\n",
      "Loss: 1.202 | Acc: 56.598% (11881/20992)\n",
      "Loss: 1.202 | Acc: 56.592% (11916/21056)\n",
      "Loss: 1.202 | Acc: 56.605% (11955/21120)\n",
      "Loss: 1.202 | Acc: 56.618% (11994/21184)\n",
      "Loss: 1.201 | Acc: 56.617% (12030/21248)\n",
      "Loss: 1.201 | Acc: 56.625% (12068/21312)\n",
      "Loss: 1.201 | Acc: 56.634% (12106/21376)\n",
      "Loss: 1.201 | Acc: 56.623% (12140/21440)\n",
      "Loss: 1.201 | Acc: 56.617% (12175/21504)\n",
      "Loss: 1.201 | Acc: 56.621% (12212/21568)\n",
      "Loss: 1.202 | Acc: 56.606% (12245/21632)\n",
      "Loss: 1.201 | Acc: 56.600% (12280/21696)\n",
      "Loss: 1.202 | Acc: 56.595% (12315/21760)\n",
      "Loss: 1.201 | Acc: 56.617% (12356/21824)\n",
      "Loss: 1.202 | Acc: 56.588% (12386/21888)\n",
      "Loss: 1.202 | Acc: 56.569% (12418/21952)\n",
      "Loss: 1.203 | Acc: 56.550% (12450/22016)\n",
      "Loss: 1.203 | Acc: 56.562% (12489/22080)\n",
      "Loss: 1.203 | Acc: 56.553% (12523/22144)\n",
      "Loss: 1.203 | Acc: 56.570% (12563/22208)\n",
      "Loss: 1.203 | Acc: 56.546% (12594/22272)\n",
      "Loss: 1.203 | Acc: 56.550% (12631/22336)\n",
      "Loss: 1.203 | Acc: 56.540% (12665/22400)\n",
      "Loss: 1.203 | Acc: 56.530% (12699/22464)\n",
      "Loss: 1.204 | Acc: 56.521% (12733/22528)\n",
      "Loss: 1.204 | Acc: 56.498% (12764/22592)\n",
      "Loss: 1.204 | Acc: 56.497% (12800/22656)\n",
      "Loss: 1.204 | Acc: 56.501% (12837/22720)\n",
      "Loss: 1.204 | Acc: 56.505% (12874/22784)\n",
      "Loss: 1.204 | Acc: 56.482% (12905/22848)\n",
      "Loss: 1.204 | Acc: 56.486% (12942/22912)\n",
      "Loss: 1.204 | Acc: 56.476% (12976/22976)\n",
      "Loss: 1.204 | Acc: 56.484% (13014/23040)\n",
      "Loss: 1.204 | Acc: 56.492% (13052/23104)\n",
      "Loss: 1.205 | Acc: 56.487% (13087/23168)\n",
      "Loss: 1.205 | Acc: 56.478% (13121/23232)\n",
      "Loss: 1.205 | Acc: 56.482% (13158/23296)\n",
      "Loss: 1.205 | Acc: 56.473% (13192/23360)\n",
      "Loss: 1.206 | Acc: 56.489% (13232/23424)\n",
      "Loss: 1.205 | Acc: 56.510% (13273/23488)\n",
      "Loss: 1.205 | Acc: 56.505% (13308/23552)\n",
      "Loss: 1.206 | Acc: 56.496% (13342/23616)\n",
      "Loss: 1.206 | Acc: 56.491% (13377/23680)\n",
      "Loss: 1.205 | Acc: 56.498% (13415/23744)\n",
      "Loss: 1.206 | Acc: 56.468% (13444/23808)\n",
      "Loss: 1.206 | Acc: 56.472% (13481/23872)\n",
      "Loss: 1.206 | Acc: 56.480% (13519/23936)\n",
      "Loss: 1.206 | Acc: 56.492% (13558/24000)\n",
      "Loss: 1.206 | Acc: 56.495% (13595/24064)\n",
      "Loss: 1.205 | Acc: 56.486% (13629/24128)\n",
      "Loss: 1.205 | Acc: 56.519% (13673/24192)\n",
      "Loss: 1.205 | Acc: 56.506% (13706/24256)\n",
      "Loss: 1.205 | Acc: 56.497% (13740/24320)\n",
      "Loss: 1.205 | Acc: 56.504% (13778/24384)\n",
      "Loss: 1.205 | Acc: 56.508% (13815/24448)\n",
      "Loss: 1.205 | Acc: 56.511% (13852/24512)\n",
      "Loss: 1.205 | Acc: 56.531% (13893/24576)\n",
      "Loss: 1.205 | Acc: 56.518% (13926/24640)\n",
      "Loss: 1.205 | Acc: 56.497% (13957/24704)\n",
      "Loss: 1.205 | Acc: 56.496% (13993/24768)\n",
      "Loss: 1.205 | Acc: 56.512% (14033/24832)\n",
      "Loss: 1.205 | Acc: 56.527% (14073/24896)\n",
      "Loss: 1.205 | Acc: 56.538% (14112/24960)\n",
      "Loss: 1.205 | Acc: 56.534% (14147/25024)\n",
      "Loss: 1.205 | Acc: 56.553% (14188/25088)\n",
      "Loss: 1.205 | Acc: 56.524% (14217/25152)\n",
      "Loss: 1.205 | Acc: 56.504% (14248/25216)\n",
      "Loss: 1.205 | Acc: 56.503% (14284/25280)\n",
      "Loss: 1.205 | Acc: 56.506% (14321/25344)\n",
      "Loss: 1.205 | Acc: 56.486% (14352/25408)\n",
      "Loss: 1.206 | Acc: 56.474% (14385/25472)\n",
      "Loss: 1.206 | Acc: 56.469% (14420/25536)\n",
      "Loss: 1.206 | Acc: 56.461% (14454/25600)\n",
      "Loss: 1.206 | Acc: 56.460% (14490/25664)\n",
      "Loss: 1.206 | Acc: 56.479% (14531/25728)\n",
      "Loss: 1.206 | Acc: 56.463% (14563/25792)\n",
      "Loss: 1.207 | Acc: 56.455% (14597/25856)\n",
      "Loss: 1.207 | Acc: 56.462% (14635/25920)\n",
      "Loss: 1.207 | Acc: 56.473% (14674/25984)\n",
      "Loss: 1.207 | Acc: 56.477% (14711/26048)\n",
      "Loss: 1.206 | Acc: 56.491% (14751/26112)\n",
      "Loss: 1.206 | Acc: 56.494% (14788/26176)\n",
      "Loss: 1.206 | Acc: 56.502% (14826/26240)\n",
      "Loss: 1.206 | Acc: 56.524% (14868/26304)\n",
      "Loss: 1.206 | Acc: 56.504% (14899/26368)\n",
      "Loss: 1.206 | Acc: 56.500% (14934/26432)\n",
      "Loss: 1.206 | Acc: 56.484% (14966/26496)\n",
      "Loss: 1.206 | Acc: 56.483% (15002/26560)\n",
      "Loss: 1.205 | Acc: 56.487% (15039/26624)\n",
      "Loss: 1.205 | Acc: 56.497% (15078/26688)\n",
      "Loss: 1.205 | Acc: 56.497% (15114/26752)\n",
      "Loss: 1.206 | Acc: 56.477% (15145/26816)\n",
      "Loss: 1.206 | Acc: 56.466% (15178/26880)\n",
      "Loss: 1.206 | Acc: 56.469% (15215/26944)\n",
      "Loss: 1.206 | Acc: 56.454% (15247/27008)\n",
      "Loss: 1.207 | Acc: 56.446% (15281/27072)\n",
      "Loss: 1.207 | Acc: 56.453% (15319/27136)\n",
      "Loss: 1.206 | Acc: 56.456% (15356/27200)\n",
      "Loss: 1.207 | Acc: 56.433% (15386/27264)\n",
      "Loss: 1.207 | Acc: 56.440% (15424/27328)\n",
      "Loss: 1.207 | Acc: 56.454% (15464/27392)\n",
      "Loss: 1.207 | Acc: 56.439% (15496/27456)\n",
      "Loss: 1.207 | Acc: 56.439% (15532/27520)\n",
      "Loss: 1.207 | Acc: 56.435% (15567/27584)\n",
      "Loss: 1.207 | Acc: 56.445% (15606/27648)\n",
      "Loss: 1.208 | Acc: 56.412% (15633/27712)\n",
      "Loss: 1.208 | Acc: 56.394% (15664/27776)\n",
      "Loss: 1.209 | Acc: 56.390% (15699/27840)\n",
      "Loss: 1.209 | Acc: 56.401% (15738/27904)\n",
      "Loss: 1.208 | Acc: 56.404% (15775/27968)\n",
      "Loss: 1.208 | Acc: 56.407% (15812/28032)\n",
      "Loss: 1.208 | Acc: 56.410% (15849/28096)\n",
      "Loss: 1.208 | Acc: 56.413% (15886/28160)\n",
      "Loss: 1.208 | Acc: 56.409% (15921/28224)\n",
      "Loss: 1.209 | Acc: 56.395% (15953/28288)\n",
      "Loss: 1.209 | Acc: 56.405% (15992/28352)\n",
      "Loss: 1.208 | Acc: 56.394% (16025/28416)\n",
      "Loss: 1.209 | Acc: 56.387% (16059/28480)\n",
      "Loss: 1.209 | Acc: 56.397% (16098/28544)\n",
      "Loss: 1.208 | Acc: 56.414% (16139/28608)\n",
      "Loss: 1.208 | Acc: 56.410% (16174/28672)\n",
      "Loss: 1.208 | Acc: 56.441% (16219/28736)\n",
      "Loss: 1.208 | Acc: 56.431% (16252/28800)\n",
      "Loss: 1.208 | Acc: 56.434% (16289/28864)\n",
      "Loss: 1.208 | Acc: 56.437% (16326/28928)\n",
      "Loss: 1.208 | Acc: 56.422% (16358/28992)\n",
      "Loss: 1.209 | Acc: 56.419% (16393/29056)\n",
      "Loss: 1.209 | Acc: 56.418% (16429/29120)\n",
      "Loss: 1.209 | Acc: 56.418% (16465/29184)\n",
      "Loss: 1.209 | Acc: 56.418% (16501/29248)\n",
      "Loss: 1.209 | Acc: 56.404% (16533/29312)\n",
      "Loss: 1.209 | Acc: 56.424% (16575/29376)\n",
      "Loss: 1.209 | Acc: 56.406% (16606/29440)\n",
      "Loss: 1.209 | Acc: 56.433% (16650/29504)\n",
      "Loss: 1.209 | Acc: 56.429% (16685/29568)\n",
      "Loss: 1.208 | Acc: 56.436% (16723/29632)\n",
      "Loss: 1.208 | Acc: 56.445% (16762/29696)\n",
      "Loss: 1.208 | Acc: 56.452% (16800/29760)\n",
      "Loss: 1.208 | Acc: 56.455% (16837/29824)\n",
      "Loss: 1.208 | Acc: 56.454% (16873/29888)\n",
      "Loss: 1.209 | Acc: 56.427% (16901/29952)\n",
      "Loss: 1.209 | Acc: 56.437% (16940/30016)\n",
      "Loss: 1.209 | Acc: 56.416% (16970/30080)\n",
      "Loss: 1.209 | Acc: 56.416% (17006/30144)\n",
      "Loss: 1.209 | Acc: 56.422% (17044/30208)\n",
      "Loss: 1.209 | Acc: 56.435% (17084/30272)\n",
      "Loss: 1.209 | Acc: 56.431% (17119/30336)\n",
      "Loss: 1.209 | Acc: 56.454% (17162/30400)\n",
      "Loss: 1.208 | Acc: 56.457% (17199/30464)\n",
      "Loss: 1.209 | Acc: 56.447% (17232/30528)\n",
      "Loss: 1.208 | Acc: 56.456% (17271/30592)\n",
      "Loss: 1.208 | Acc: 56.456% (17307/30656)\n",
      "Loss: 1.208 | Acc: 56.462% (17345/30720)\n",
      "Loss: 1.208 | Acc: 56.448% (17377/30784)\n",
      "Loss: 1.208 | Acc: 56.457% (17416/30848)\n",
      "Loss: 1.208 | Acc: 56.460% (17453/30912)\n",
      "Loss: 1.208 | Acc: 56.476% (17494/30976)\n",
      "Loss: 1.208 | Acc: 56.476% (17530/31040)\n",
      "Loss: 1.207 | Acc: 56.498% (17573/31104)\n",
      "Loss: 1.207 | Acc: 56.491% (17607/31168)\n",
      "Loss: 1.207 | Acc: 56.487% (17642/31232)\n",
      "Loss: 1.207 | Acc: 56.499% (17682/31296)\n",
      "Loss: 1.207 | Acc: 56.521% (17725/31360)\n",
      "Loss: 1.207 | Acc: 56.505% (17756/31424)\n",
      "Loss: 1.207 | Acc: 56.507% (17793/31488)\n",
      "Loss: 1.207 | Acc: 56.494% (17825/31552)\n",
      "Loss: 1.207 | Acc: 56.478% (17856/31616)\n",
      "Loss: 1.207 | Acc: 56.468% (17889/31680)\n",
      "Loss: 1.208 | Acc: 56.439% (17916/31744)\n",
      "Loss: 1.207 | Acc: 56.448% (17955/31808)\n",
      "Loss: 1.207 | Acc: 56.466% (17997/31872)\n",
      "Loss: 1.207 | Acc: 56.472% (18035/31936)\n",
      "Loss: 1.207 | Acc: 56.469% (18070/32000)\n",
      "Loss: 1.207 | Acc: 56.440% (18097/32064)\n",
      "Loss: 1.207 | Acc: 56.443% (18134/32128)\n",
      "Loss: 1.207 | Acc: 56.443% (18170/32192)\n",
      "Loss: 1.207 | Acc: 56.461% (18212/32256)\n",
      "Loss: 1.206 | Acc: 56.463% (18249/32320)\n",
      "Loss: 1.206 | Acc: 56.497% (18296/32384)\n",
      "Loss: 1.206 | Acc: 56.515% (18338/32448)\n",
      "Loss: 1.206 | Acc: 56.515% (18374/32512)\n",
      "Loss: 1.206 | Acc: 56.496% (18404/32576)\n",
      "Loss: 1.206 | Acc: 56.489% (18438/32640)\n",
      "Loss: 1.206 | Acc: 56.485% (18473/32704)\n",
      "Loss: 1.207 | Acc: 56.479% (18507/32768)\n",
      "Loss: 1.207 | Acc: 56.463% (18538/32832)\n",
      "Loss: 1.208 | Acc: 56.448% (18569/32896)\n",
      "Loss: 1.208 | Acc: 56.423% (18597/32960)\n",
      "Loss: 1.208 | Acc: 56.414% (18630/33024)\n",
      "Loss: 1.208 | Acc: 56.407% (18664/33088)\n",
      "Loss: 1.208 | Acc: 56.437% (18710/33152)\n",
      "Loss: 1.208 | Acc: 56.431% (18744/33216)\n",
      "Loss: 1.207 | Acc: 56.436% (18782/33280)\n",
      "Loss: 1.208 | Acc: 56.415% (18811/33344)\n",
      "Loss: 1.207 | Acc: 56.427% (18851/33408)\n",
      "Loss: 1.208 | Acc: 56.417% (18884/33472)\n",
      "Loss: 1.208 | Acc: 56.411% (18918/33536)\n",
      "Loss: 1.208 | Acc: 56.411% (18954/33600)\n",
      "Loss: 1.208 | Acc: 56.396% (18985/33664)\n",
      "Loss: 1.208 | Acc: 56.395% (19021/33728)\n",
      "Loss: 1.208 | Acc: 56.407% (19061/33792)\n",
      "Loss: 1.208 | Acc: 56.404% (19096/33856)\n",
      "Loss: 1.208 | Acc: 56.418% (19137/33920)\n",
      "Loss: 1.208 | Acc: 56.429% (19177/33984)\n",
      "Loss: 1.208 | Acc: 56.435% (19215/34048)\n",
      "Loss: 1.208 | Acc: 56.438% (19252/34112)\n",
      "Loss: 1.207 | Acc: 56.434% (19287/34176)\n",
      "Loss: 1.207 | Acc: 56.443% (19326/34240)\n",
      "Loss: 1.207 | Acc: 56.448% (19364/34304)\n",
      "Loss: 1.207 | Acc: 56.462% (19405/34368)\n",
      "Loss: 1.207 | Acc: 56.456% (19439/34432)\n",
      "Loss: 1.207 | Acc: 56.438% (19469/34496)\n",
      "Loss: 1.207 | Acc: 56.453% (19510/34560)\n",
      "Loss: 1.207 | Acc: 56.472% (19553/34624)\n",
      "Loss: 1.207 | Acc: 56.469% (19588/34688)\n",
      "Loss: 1.207 | Acc: 56.460% (19621/34752)\n",
      "Loss: 1.206 | Acc: 56.477% (19663/34816)\n",
      "Loss: 1.206 | Acc: 56.499% (19707/34880)\n",
      "Loss: 1.206 | Acc: 56.496% (19742/34944)\n",
      "Loss: 1.206 | Acc: 56.493% (19777/35008)\n",
      "Loss: 1.206 | Acc: 56.512% (19820/35072)\n",
      "Loss: 1.206 | Acc: 56.506% (19854/35136)\n",
      "Loss: 1.206 | Acc: 56.500% (19888/35200)\n",
      "Loss: 1.206 | Acc: 56.491% (19921/35264)\n",
      "Loss: 1.206 | Acc: 56.462% (19947/35328)\n",
      "Loss: 1.206 | Acc: 56.476% (19988/35392)\n",
      "Loss: 1.206 | Acc: 56.476% (20024/35456)\n",
      "Loss: 1.206 | Acc: 56.475% (20060/35520)\n",
      "Loss: 1.206 | Acc: 56.472% (20095/35584)\n",
      "Loss: 1.206 | Acc: 56.466% (20129/35648)\n",
      "Loss: 1.206 | Acc: 56.477% (20169/35712)\n",
      "Loss: 1.206 | Acc: 56.471% (20203/35776)\n",
      "Loss: 1.207 | Acc: 56.454% (20233/35840)\n",
      "Loss: 1.207 | Acc: 56.437% (20263/35904)\n",
      "Loss: 1.207 | Acc: 56.417% (20292/35968)\n",
      "Loss: 1.207 | Acc: 56.430% (20333/36032)\n",
      "Loss: 1.206 | Acc: 56.444% (20374/36096)\n",
      "Loss: 1.206 | Acc: 56.441% (20409/36160)\n",
      "Loss: 1.206 | Acc: 56.454% (20450/36224)\n",
      "Loss: 1.206 | Acc: 56.462% (20489/36288)\n",
      "Loss: 1.206 | Acc: 56.459% (20524/36352)\n",
      "Loss: 1.206 | Acc: 56.467% (20563/36416)\n",
      "Loss: 1.206 | Acc: 56.456% (20595/36480)\n",
      "Loss: 1.206 | Acc: 56.450% (20629/36544)\n",
      "Loss: 1.206 | Acc: 56.466% (20671/36608)\n",
      "Loss: 1.206 | Acc: 56.465% (20707/36672)\n",
      "Loss: 1.206 | Acc: 56.479% (20748/36736)\n",
      "Loss: 1.206 | Acc: 56.478% (20784/36800)\n",
      "Loss: 1.206 | Acc: 56.491% (20825/36864)\n",
      "Loss: 1.206 | Acc: 56.480% (20857/36928)\n",
      "Loss: 1.207 | Acc: 56.485% (20895/36992)\n",
      "Loss: 1.207 | Acc: 56.490% (20933/37056)\n",
      "Loss: 1.207 | Acc: 56.495% (20971/37120)\n",
      "Loss: 1.207 | Acc: 56.495% (21007/37184)\n",
      "Loss: 1.206 | Acc: 56.518% (21052/37248)\n",
      "Loss: 1.206 | Acc: 56.523% (21090/37312)\n",
      "Loss: 1.206 | Acc: 56.528% (21128/37376)\n",
      "Loss: 1.206 | Acc: 56.541% (21169/37440)\n",
      "Loss: 1.206 | Acc: 56.559% (21212/37504)\n",
      "Loss: 1.206 | Acc: 56.535% (21239/37568)\n",
      "Loss: 1.206 | Acc: 56.540% (21277/37632)\n",
      "Loss: 1.206 | Acc: 56.531% (21310/37696)\n",
      "Loss: 1.206 | Acc: 56.523% (21343/37760)\n",
      "Loss: 1.207 | Acc: 56.509% (21374/37824)\n",
      "Loss: 1.207 | Acc: 56.511% (21411/37888)\n",
      "Loss: 1.207 | Acc: 56.511% (21447/37952)\n",
      "Loss: 1.206 | Acc: 56.521% (21487/38016)\n",
      "Loss: 1.206 | Acc: 56.523% (21524/38080)\n",
      "Loss: 1.206 | Acc: 56.523% (21560/38144)\n",
      "Loss: 1.206 | Acc: 56.538% (21602/38208)\n",
      "Loss: 1.206 | Acc: 56.524% (21633/38272)\n",
      "Loss: 1.206 | Acc: 56.545% (21677/38336)\n",
      "Loss: 1.206 | Acc: 56.531% (21708/38400)\n",
      "Loss: 1.206 | Acc: 56.544% (21749/38464)\n",
      "Loss: 1.206 | Acc: 56.530% (21780/38528)\n",
      "Loss: 1.207 | Acc: 56.522% (21813/38592)\n",
      "Loss: 1.206 | Acc: 56.535% (21854/38656)\n",
      "Loss: 1.206 | Acc: 56.544% (21894/38720)\n",
      "Loss: 1.206 | Acc: 56.541% (21929/38784)\n",
      "Loss: 1.206 | Acc: 56.546% (21967/38848)\n",
      "Loss: 1.206 | Acc: 56.540% (22001/38912)\n",
      "Loss: 1.206 | Acc: 56.550% (22041/38976)\n",
      "Loss: 1.206 | Acc: 56.547% (22076/39040)\n",
      "Loss: 1.206 | Acc: 56.549% (22113/39104)\n",
      "Loss: 1.206 | Acc: 56.549% (22149/39168)\n",
      "Loss: 1.206 | Acc: 56.548% (22185/39232)\n",
      "Loss: 1.206 | Acc: 56.560% (22226/39296)\n",
      "Loss: 1.206 | Acc: 56.550% (22258/39360)\n",
      "Loss: 1.206 | Acc: 56.552% (22295/39424)\n",
      "Loss: 1.206 | Acc: 56.551% (22331/39488)\n",
      "Loss: 1.206 | Acc: 56.536% (22361/39552)\n",
      "Loss: 1.206 | Acc: 56.553% (22404/39616)\n",
      "Loss: 1.206 | Acc: 56.552% (22440/39680)\n",
      "Loss: 1.206 | Acc: 56.554% (22477/39744)\n",
      "Loss: 1.206 | Acc: 56.561% (22516/39808)\n",
      "Loss: 1.206 | Acc: 56.566% (22554/39872)\n",
      "Loss: 1.206 | Acc: 56.578% (22595/39936)\n",
      "Loss: 1.206 | Acc: 56.568% (22627/40000)\n",
      "Loss: 1.206 | Acc: 56.562% (22661/40064)\n",
      "Loss: 1.206 | Acc: 56.547% (22691/40128)\n",
      "Loss: 1.206 | Acc: 56.544% (22726/40192)\n",
      "Loss: 1.206 | Acc: 56.533% (22758/40256)\n",
      "Loss: 1.206 | Acc: 56.548% (22800/40320)\n",
      "Loss: 1.206 | Acc: 56.545% (22835/40384)\n",
      "Loss: 1.205 | Acc: 56.569% (22881/40448)\n",
      "Loss: 1.205 | Acc: 56.571% (22918/40512)\n",
      "Loss: 1.205 | Acc: 56.575% (22956/40576)\n",
      "Loss: 1.205 | Acc: 56.587% (22997/40640)\n",
      "Loss: 1.205 | Acc: 56.584% (23032/40704)\n",
      "Loss: 1.206 | Acc: 56.566% (23061/40768)\n",
      "Loss: 1.205 | Acc: 56.561% (23095/40832)\n",
      "Loss: 1.205 | Acc: 56.568% (23134/40896)\n",
      "Loss: 1.205 | Acc: 56.565% (23169/40960)\n",
      "Loss: 1.205 | Acc: 56.564% (23205/41024)\n",
      "Loss: 1.205 | Acc: 56.547% (23234/41088)\n",
      "Loss: 1.205 | Acc: 56.556% (23274/41152)\n",
      "Loss: 1.205 | Acc: 56.580% (23320/41216)\n",
      "Loss: 1.205 | Acc: 56.587% (23359/41280)\n",
      "Loss: 1.205 | Acc: 56.567% (23387/41344)\n",
      "Loss: 1.205 | Acc: 56.576% (23427/41408)\n",
      "Loss: 1.205 | Acc: 56.592% (23470/41472)\n",
      "Loss: 1.205 | Acc: 56.589% (23505/41536)\n",
      "Loss: 1.205 | Acc: 56.591% (23542/41600)\n",
      "Loss: 1.205 | Acc: 56.591% (23578/41664)\n",
      "Loss: 1.205 | Acc: 56.588% (23613/41728)\n",
      "Loss: 1.205 | Acc: 56.590% (23650/41792)\n",
      "Loss: 1.205 | Acc: 56.596% (23689/41856)\n",
      "Loss: 1.205 | Acc: 56.596% (23725/41920)\n",
      "Loss: 1.205 | Acc: 56.593% (23760/41984)\n",
      "Loss: 1.205 | Acc: 56.595% (23797/42048)\n",
      "Loss: 1.205 | Acc: 56.594% (23833/42112)\n",
      "Loss: 1.205 | Acc: 56.584% (23865/42176)\n",
      "Loss: 1.205 | Acc: 56.579% (23899/42240)\n",
      "Loss: 1.205 | Acc: 56.564% (23929/42304)\n",
      "Loss: 1.205 | Acc: 56.583% (23973/42368)\n",
      "Loss: 1.205 | Acc: 56.578% (24007/42432)\n",
      "Loss: 1.205 | Acc: 56.584% (24046/42496)\n",
      "Loss: 1.205 | Acc: 56.591% (24085/42560)\n",
      "Loss: 1.205 | Acc: 56.585% (24119/42624)\n",
      "Loss: 1.205 | Acc: 56.583% (24154/42688)\n",
      "Loss: 1.205 | Acc: 56.580% (24189/42752)\n",
      "Loss: 1.205 | Acc: 56.577% (24224/42816)\n",
      "Loss: 1.205 | Acc: 56.562% (24254/42880)\n",
      "Loss: 1.205 | Acc: 56.564% (24291/42944)\n",
      "Loss: 1.206 | Acc: 56.559% (24325/43008)\n",
      "Loss: 1.206 | Acc: 56.563% (24363/43072)\n",
      "Loss: 1.206 | Acc: 56.551% (24394/43136)\n",
      "Loss: 1.206 | Acc: 56.556% (24432/43200)\n",
      "Loss: 1.206 | Acc: 56.548% (24465/43264)\n",
      "Loss: 1.206 | Acc: 56.529% (24493/43328)\n",
      "Loss: 1.206 | Acc: 56.529% (24529/43392)\n",
      "Loss: 1.206 | Acc: 56.528% (24565/43456)\n",
      "Loss: 1.206 | Acc: 56.544% (24608/43520)\n",
      "Loss: 1.206 | Acc: 56.553% (24648/43584)\n",
      "Loss: 1.205 | Acc: 56.573% (24693/43648)\n",
      "Loss: 1.205 | Acc: 56.570% (24728/43712)\n",
      "Loss: 1.205 | Acc: 56.577% (24767/43776)\n",
      "Loss: 1.205 | Acc: 56.574% (24802/43840)\n",
      "Loss: 1.206 | Acc: 56.564% (24834/43904)\n",
      "Loss: 1.205 | Acc: 56.564% (24870/43968)\n",
      "Loss: 1.205 | Acc: 56.568% (24908/44032)\n",
      "Loss: 1.205 | Acc: 56.579% (24949/44096)\n",
      "Loss: 1.205 | Acc: 56.592% (24991/44160)\n",
      "Loss: 1.205 | Acc: 56.607% (25034/44224)\n",
      "Loss: 1.205 | Acc: 56.616% (25074/44288)\n",
      "Loss: 1.204 | Acc: 56.629% (25116/44352)\n",
      "Loss: 1.204 | Acc: 56.649% (25161/44416)\n",
      "Loss: 1.204 | Acc: 56.652% (25199/44480)\n",
      "Loss: 1.204 | Acc: 56.652% (25235/44544)\n",
      "Loss: 1.204 | Acc: 56.660% (25275/44608)\n",
      "Loss: 1.204 | Acc: 56.651% (25307/44672)\n",
      "Loss: 1.204 | Acc: 56.652% (25344/44736)\n",
      "Loss: 1.204 | Acc: 56.663% (25385/44800)\n",
      "Loss: 1.204 | Acc: 56.674% (25426/44864)\n",
      "Loss: 1.204 | Acc: 56.680% (25465/44928)\n",
      "Loss: 1.204 | Acc: 56.672% (25498/44992)\n",
      "Loss: 1.204 | Acc: 56.652% (25525/45056)\n",
      "Loss: 1.204 | Acc: 56.653% (25562/45120)\n",
      "Loss: 1.204 | Acc: 56.662% (25602/45184)\n",
      "Loss: 1.204 | Acc: 56.665% (25640/45248)\n",
      "Loss: 1.204 | Acc: 56.660% (25674/45312)\n",
      "Loss: 1.204 | Acc: 56.675% (25717/45376)\n",
      "Loss: 1.204 | Acc: 56.664% (25748/45440)\n",
      "Loss: 1.204 | Acc: 56.659% (25782/45504)\n",
      "Loss: 1.205 | Acc: 56.641% (25810/45568)\n",
      "Loss: 1.205 | Acc: 56.642% (25847/45632)\n",
      "Loss: 1.205 | Acc: 56.644% (25884/45696)\n",
      "Loss: 1.204 | Acc: 56.648% (25922/45760)\n",
      "Loss: 1.204 | Acc: 56.665% (25966/45824)\n",
      "Loss: 1.204 | Acc: 56.681% (26010/45888)\n",
      "Loss: 1.203 | Acc: 56.690% (26050/45952)\n",
      "Loss: 1.203 | Acc: 56.706% (26094/46016)\n",
      "Loss: 1.203 | Acc: 56.704% (26129/46080)\n",
      "Loss: 1.203 | Acc: 56.714% (26170/46144)\n",
      "Loss: 1.203 | Acc: 56.707% (26203/46208)\n",
      "Loss: 1.203 | Acc: 56.712% (26242/46272)\n",
      "Loss: 1.203 | Acc: 56.714% (26279/46336)\n",
      "Loss: 1.203 | Acc: 56.711% (26314/46400)\n",
      "Loss: 1.202 | Acc: 56.728% (26358/46464)\n",
      "Loss: 1.202 | Acc: 56.742% (26401/46528)\n",
      "Loss: 1.202 | Acc: 56.746% (26439/46592)\n",
      "Loss: 1.202 | Acc: 56.745% (26475/46656)\n",
      "Loss: 1.202 | Acc: 56.755% (26516/46720)\n",
      "Loss: 1.202 | Acc: 56.739% (26545/46784)\n",
      "Loss: 1.203 | Acc: 56.739% (26581/46848)\n",
      "Loss: 1.202 | Acc: 56.742% (26619/46912)\n",
      "Loss: 1.202 | Acc: 56.748% (26658/46976)\n",
      "Loss: 1.202 | Acc: 56.739% (26690/47040)\n",
      "Loss: 1.203 | Acc: 56.732% (26723/47104)\n",
      "Loss: 1.203 | Acc: 56.733% (26760/47168)\n",
      "Loss: 1.203 | Acc: 56.735% (26797/47232)\n",
      "Loss: 1.203 | Acc: 56.724% (26828/47296)\n",
      "Loss: 1.203 | Acc: 56.719% (26862/47360)\n",
      "Loss: 1.203 | Acc: 56.716% (26897/47424)\n",
      "Loss: 1.203 | Acc: 56.720% (26935/47488)\n",
      "Loss: 1.203 | Acc: 56.729% (26976/47552)\n",
      "Loss: 1.203 | Acc: 56.723% (27009/47616)\n",
      "Loss: 1.203 | Acc: 56.728% (27048/47680)\n",
      "Loss: 1.203 | Acc: 56.740% (27090/47744)\n",
      "Loss: 1.203 | Acc: 56.756% (27134/47808)\n",
      "Loss: 1.203 | Acc: 56.737% (27161/47872)\n",
      "Loss: 1.203 | Acc: 56.732% (27195/47936)\n",
      "Loss: 1.203 | Acc: 56.729% (27230/48000)\n",
      "Loss: 1.203 | Acc: 56.741% (27272/48064)\n",
      "Loss: 1.203 | Acc: 56.726% (27301/48128)\n",
      "Loss: 1.204 | Acc: 56.707% (27328/48192)\n",
      "Loss: 1.204 | Acc: 56.712% (27367/48256)\n",
      "Loss: 1.204 | Acc: 56.707% (27401/48320)\n",
      "Loss: 1.204 | Acc: 56.707% (27437/48384)\n",
      "Loss: 1.204 | Acc: 56.710% (27475/48448)\n",
      "Loss: 1.203 | Acc: 56.724% (27518/48512)\n",
      "Loss: 1.204 | Acc: 56.717% (27551/48576)\n",
      "Loss: 1.204 | Acc: 56.708% (27583/48640)\n",
      "Loss: 1.204 | Acc: 56.704% (27617/48704)\n",
      "Loss: 1.204 | Acc: 56.707% (27655/48768)\n",
      "Loss: 1.204 | Acc: 56.711% (27693/48832)\n",
      "Loss: 1.204 | Acc: 56.710% (27729/48896)\n",
      "Loss: 1.203 | Acc: 56.724% (27772/48960)\n",
      "Loss: 1.203 | Acc: 56.716% (27791/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 56.71632653061224\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.282 | Acc: 53.125% (34/64)\n",
      "Loss: 1.177 | Acc: 54.688% (70/128)\n",
      "Loss: 1.254 | Acc: 53.125% (102/192)\n",
      "Loss: 1.337 | Acc: 49.609% (127/256)\n",
      "Loss: 1.306 | Acc: 50.312% (161/320)\n",
      "Loss: 1.291 | Acc: 51.823% (199/384)\n",
      "Loss: 1.296 | Acc: 52.455% (235/448)\n",
      "Loss: 1.310 | Acc: 52.148% (267/512)\n",
      "Loss: 1.271 | Acc: 53.646% (309/576)\n",
      "Loss: 1.269 | Acc: 53.750% (344/640)\n",
      "Loss: 1.298 | Acc: 53.267% (375/704)\n",
      "Loss: 1.299 | Acc: 52.865% (406/768)\n",
      "Loss: 1.314 | Acc: 53.245% (443/832)\n",
      "Loss: 1.303 | Acc: 53.795% (482/896)\n",
      "Loss: 1.291 | Acc: 54.167% (520/960)\n",
      "Loss: 1.273 | Acc: 55.273% (566/1024)\n",
      "Loss: 1.270 | Acc: 55.423% (603/1088)\n",
      "Loss: 1.260 | Acc: 55.556% (640/1152)\n",
      "Loss: 1.253 | Acc: 56.086% (682/1216)\n",
      "Loss: 1.269 | Acc: 55.547% (711/1280)\n",
      "Loss: 1.268 | Acc: 55.208% (742/1344)\n",
      "Loss: 1.267 | Acc: 55.114% (776/1408)\n",
      "Loss: 1.260 | Acc: 55.027% (810/1472)\n",
      "Loss: 1.265 | Acc: 54.818% (842/1536)\n",
      "Loss: 1.271 | Acc: 54.688% (875/1600)\n",
      "Loss: 1.276 | Acc: 54.688% (910/1664)\n",
      "Loss: 1.276 | Acc: 54.919% (949/1728)\n",
      "Loss: 1.280 | Acc: 54.967% (985/1792)\n",
      "Loss: 1.283 | Acc: 54.364% (1009/1856)\n",
      "Loss: 1.282 | Acc: 54.479% (1046/1920)\n",
      "Loss: 1.281 | Acc: 54.385% (1079/1984)\n",
      "Loss: 1.288 | Acc: 54.150% (1109/2048)\n",
      "Loss: 1.277 | Acc: 54.403% (1149/2112)\n",
      "Loss: 1.276 | Acc: 54.366% (1183/2176)\n",
      "Loss: 1.270 | Acc: 54.554% (1222/2240)\n",
      "Loss: 1.264 | Acc: 54.731% (1261/2304)\n",
      "Loss: 1.266 | Acc: 54.814% (1298/2368)\n",
      "Loss: 1.265 | Acc: 54.688% (1330/2432)\n",
      "Loss: 1.263 | Acc: 54.888% (1370/2496)\n",
      "Loss: 1.271 | Acc: 54.727% (1401/2560)\n",
      "Loss: 1.272 | Acc: 54.459% (1429/2624)\n",
      "Loss: 1.269 | Acc: 54.688% (1470/2688)\n",
      "Loss: 1.271 | Acc: 54.688% (1505/2752)\n",
      "Loss: 1.272 | Acc: 54.759% (1542/2816)\n",
      "Loss: 1.268 | Acc: 54.861% (1580/2880)\n",
      "Loss: 1.264 | Acc: 55.095% (1622/2944)\n",
      "Loss: 1.260 | Acc: 55.286% (1663/3008)\n",
      "Loss: 1.262 | Acc: 55.273% (1698/3072)\n",
      "Loss: 1.261 | Acc: 55.134% (1729/3136)\n",
      "Loss: 1.257 | Acc: 55.219% (1767/3200)\n",
      "Loss: 1.257 | Acc: 55.239% (1803/3264)\n",
      "Loss: 1.259 | Acc: 55.168% (1836/3328)\n",
      "Loss: 1.258 | Acc: 55.130% (1870/3392)\n",
      "Loss: 1.257 | Acc: 55.208% (1908/3456)\n",
      "Loss: 1.255 | Acc: 55.085% (1939/3520)\n",
      "Loss: 1.254 | Acc: 55.078% (1974/3584)\n",
      "Loss: 1.254 | Acc: 54.934% (2004/3648)\n",
      "Loss: 1.252 | Acc: 55.038% (2043/3712)\n",
      "Loss: 1.254 | Acc: 55.032% (2078/3776)\n",
      "Loss: 1.252 | Acc: 55.182% (2119/3840)\n",
      "Loss: 1.253 | Acc: 55.149% (2153/3904)\n",
      "Loss: 1.254 | Acc: 55.040% (2184/3968)\n",
      "Loss: 1.255 | Acc: 54.985% (2217/4032)\n",
      "Loss: 1.256 | Acc: 55.054% (2255/4096)\n",
      "Loss: 1.258 | Acc: 55.024% (2289/4160)\n",
      "Loss: 1.256 | Acc: 55.232% (2333/4224)\n",
      "Loss: 1.257 | Acc: 55.107% (2363/4288)\n",
      "Loss: 1.257 | Acc: 55.101% (2398/4352)\n",
      "Loss: 1.255 | Acc: 55.163% (2436/4416)\n",
      "Loss: 1.252 | Acc: 55.246% (2475/4480)\n",
      "Loss: 1.252 | Acc: 55.238% (2510/4544)\n",
      "Loss: 1.255 | Acc: 55.208% (2544/4608)\n",
      "Loss: 1.254 | Acc: 55.308% (2584/4672)\n",
      "Loss: 1.252 | Acc: 55.321% (2620/4736)\n",
      "Loss: 1.252 | Acc: 55.333% (2656/4800)\n",
      "Loss: 1.251 | Acc: 55.387% (2694/4864)\n",
      "Loss: 1.250 | Acc: 55.337% (2727/4928)\n",
      "Loss: 1.251 | Acc: 55.268% (2759/4992)\n",
      "Loss: 1.249 | Acc: 55.360% (2799/5056)\n",
      "Loss: 1.253 | Acc: 55.234% (2828/5120)\n",
      "Loss: 1.251 | Acc: 55.343% (2869/5184)\n",
      "Loss: 1.251 | Acc: 55.354% (2905/5248)\n",
      "Loss: 1.248 | Acc: 55.516% (2949/5312)\n",
      "Loss: 1.249 | Acc: 55.450% (2981/5376)\n",
      "Loss: 1.248 | Acc: 55.368% (3012/5440)\n",
      "Loss: 1.250 | Acc: 55.233% (3040/5504)\n",
      "Loss: 1.253 | Acc: 55.101% (3068/5568)\n",
      "Loss: 1.254 | Acc: 55.043% (3100/5632)\n",
      "Loss: 1.256 | Acc: 54.916% (3128/5696)\n",
      "Loss: 1.254 | Acc: 54.965% (3166/5760)\n",
      "Loss: 1.252 | Acc: 54.979% (3202/5824)\n",
      "Loss: 1.253 | Acc: 54.959% (3236/5888)\n",
      "Loss: 1.254 | Acc: 54.956% (3271/5952)\n",
      "Loss: 1.253 | Acc: 54.970% (3307/6016)\n",
      "Loss: 1.255 | Acc: 54.852% (3335/6080)\n",
      "Loss: 1.255 | Acc: 54.883% (3372/6144)\n",
      "Loss: 1.255 | Acc: 54.897% (3408/6208)\n",
      "Loss: 1.257 | Acc: 54.831% (3439/6272)\n",
      "Loss: 1.256 | Acc: 54.908% (3479/6336)\n",
      "Loss: 1.257 | Acc: 54.875% (3512/6400)\n",
      "Loss: 1.258 | Acc: 54.920% (3550/6464)\n",
      "Loss: 1.256 | Acc: 55.009% (3591/6528)\n",
      "Loss: 1.259 | Acc: 55.021% (3627/6592)\n",
      "Loss: 1.257 | Acc: 55.063% (3665/6656)\n",
      "Loss: 1.257 | Acc: 55.045% (3699/6720)\n",
      "Loss: 1.258 | Acc: 54.997% (3731/6784)\n",
      "Loss: 1.257 | Acc: 55.009% (3767/6848)\n",
      "Loss: 1.259 | Acc: 54.905% (3795/6912)\n",
      "Loss: 1.262 | Acc: 54.802% (3823/6976)\n",
      "Loss: 1.263 | Acc: 54.759% (3855/7040)\n",
      "Loss: 1.261 | Acc: 54.800% (3893/7104)\n",
      "Loss: 1.262 | Acc: 54.813% (3929/7168)\n",
      "Loss: 1.262 | Acc: 54.784% (3962/7232)\n",
      "Loss: 1.262 | Acc: 54.797% (3998/7296)\n",
      "Loss: 1.259 | Acc: 54.891% (4040/7360)\n",
      "Loss: 1.259 | Acc: 54.863% (4073/7424)\n",
      "Loss: 1.257 | Acc: 54.888% (4110/7488)\n",
      "Loss: 1.256 | Acc: 54.952% (4150/7552)\n",
      "Loss: 1.258 | Acc: 54.845% (4177/7616)\n",
      "Loss: 1.257 | Acc: 54.870% (4214/7680)\n",
      "Loss: 1.255 | Acc: 54.997% (4259/7744)\n",
      "Loss: 1.255 | Acc: 54.969% (4292/7808)\n",
      "Loss: 1.255 | Acc: 54.942% (4325/7872)\n",
      "Loss: 1.254 | Acc: 54.952% (4361/7936)\n",
      "Loss: 1.255 | Acc: 54.938% (4395/8000)\n",
      "Loss: 1.255 | Acc: 54.960% (4432/8064)\n",
      "Loss: 1.256 | Acc: 54.897% (4462/8128)\n",
      "Loss: 1.255 | Acc: 54.980% (4504/8192)\n",
      "Loss: 1.256 | Acc: 54.906% (4533/8256)\n",
      "Loss: 1.259 | Acc: 54.796% (4559/8320)\n",
      "Loss: 1.259 | Acc: 54.807% (4595/8384)\n",
      "Loss: 1.259 | Acc: 54.770% (4627/8448)\n",
      "Loss: 1.260 | Acc: 54.781% (4663/8512)\n",
      "Loss: 1.260 | Acc: 54.769% (4697/8576)\n",
      "Loss: 1.260 | Acc: 54.745% (4730/8640)\n",
      "Loss: 1.260 | Acc: 54.733% (4764/8704)\n",
      "Loss: 1.260 | Acc: 54.710% (4797/8768)\n",
      "Loss: 1.260 | Acc: 54.721% (4833/8832)\n",
      "Loss: 1.259 | Acc: 54.744% (4870/8896)\n",
      "Loss: 1.259 | Acc: 54.766% (4907/8960)\n",
      "Loss: 1.259 | Acc: 54.832% (4948/9024)\n",
      "Loss: 1.260 | Acc: 54.754% (4976/9088)\n",
      "Loss: 1.260 | Acc: 54.720% (5008/9152)\n",
      "Loss: 1.258 | Acc: 54.829% (5053/9216)\n",
      "Loss: 1.259 | Acc: 54.763% (5082/9280)\n",
      "Loss: 1.260 | Acc: 54.688% (5110/9344)\n",
      "Loss: 1.260 | Acc: 54.677% (5144/9408)\n",
      "Loss: 1.261 | Acc: 54.677% (5179/9472)\n",
      "Loss: 1.260 | Acc: 54.729% (5219/9536)\n",
      "Loss: 1.259 | Acc: 54.792% (5260/9600)\n",
      "Loss: 1.259 | Acc: 54.791% (5295/9664)\n",
      "Loss: 1.258 | Acc: 54.801% (5331/9728)\n",
      "Loss: 1.259 | Acc: 54.820% (5368/9792)\n",
      "Loss: 1.259 | Acc: 54.830% (5404/9856)\n",
      "Loss: 1.259 | Acc: 54.788% (5435/9920)\n",
      "Loss: 1.260 | Acc: 54.748% (5466/9984)\n",
      "Loss: 1.261 | Acc: 54.720% (5472/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 54.72\n",
      "\n",
      "Final train set accuracy is 56.71632653061224\n",
      "Final test set accuracy is 54.72\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "network = ViT(hidden_dims=hidden_dims, input_dims=input_dims, output_dims=output_dims,\n",
    "              num_trans_layers=num_trans_layers, num_heads=num_heads, image_k=image_k,\n",
    "              patch_k=patch_k)\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-dutch",
   "metadata": {
    "id": "noted-dutch"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Vision_Transformer_ViT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
